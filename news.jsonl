{"id": "2505.10561v1", "url": "http://arxiv.org/abs/2505.10561v1", "title": "T2A-Feedback: Improving Basic Capabilities of Text-to-Audio Generation via Fine-grained AI Feedback", "summary": "Text-to-audio (T2A) generation has achieved remarkable progress in generating\na variety of audio outputs from language prompts. However, current\nstate-of-the-art T2A models still struggle to satisfy human preferences for\nprompt-following and acoustic quality when generating complex multi-event\naudio. To improve the performance of the model in these high-level\napplications, we propose to enhance the basic capabilities of the model with AI\nfeedback learning. First, we introduce fine-grained AI audio scoring pipelines\nto: 1) verify whether each event in the text prompt is present in the audio\n(Event Occurrence Score), 2) detect deviations in event sequences from the\nlanguage description (Event Sequence Score), and 3) assess the overall acoustic\nand harmonic quality of the generated audio (Acoustic&Harmonic Quality). We\nevaluate these three automatic scoring pipelines and find that they correlate\nsignificantly better with human preferences than other evaluation metrics. This\nhighlights their value as both feedback signals and evaluation metrics.\nUtilizing our robust scoring pipelines, we construct a large audio preference\ndataset, T2A-FeedBack, which contains 41k prompts and 249k audios, each\naccompanied by detailed scores. Moreover, we introduce T2A-EpicBench, a\nbenchmark that focuses on long captions, multi-events, and story-telling\nscenarios, aiming to evaluate the advanced capabilities of T2A models. Finally,\nwe demonstrate how T2A-FeedBack can enhance current state-of-the-art audio\nmodel. With simple preference tuning, the audio generation model exhibits\nsignificant improvements in both simple (AudioCaps test set) and complex\n(T2A-EpicBench) scenarios.", "authors": ["Zehan Wang", "Ke Lei", "Chen Zhu", "Jiawei Huang", "Sashuai Zhou", "Luping Liu", "Xize Cheng", "Shengpeng Ji", "Zhenhui Ye", "Tao Jin", "Zhou Zhao"], "published_date": "2025-05-15", "title_zh": "T2A-Feedback：透過細粒度AI回饋提升文字轉語音生成之基礎能力", "summary_zh": "現今的文字轉語音模型在生成複雜多事件音訊時，難以完全符合人類對提示遵循度和音訊品質的期望。本研究提出利用AI回饋學習來提升模型基礎能力。首先，建立了精細的AI音訊評分流程，包含事件發生評分、事件序列評分，以及音訊和諧品質評分。這些評分流程能更準確地反映人類偏好。接著，利用這些評分流程，構建了包含41k個提示和249k個音訊的大型音訊偏好數據集T2A-FeedBack，並推出專注於長描述、多事件和故事敘述場景的評測基準T2A-EpicBench。實驗證明，透過簡單的偏好調整，T2A-FeedBack能有效提升現有音訊生成模型在簡單和複雜場景下的表現。", "audio": "audios/2505.10561v1.mp3", "timestamp": "2025-05-18T23:05:41.112280"}
{"id": "2505.10556v1", "url": "http://arxiv.org/abs/2505.10556v1", "title": "An AI-driven framework for the prediction of personalised health response to air pollution", "summary": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "published_date": "2025-05-15", "title_zh": "一個AI驅動的框架，用於預測個人化健康對空氣污染的反應", "summary_zh": "本研究提出一個新的AI框架，結合穿戴裝置收集的生理數據和即時環境暴露數據，來預測個人對空氣污染的健康反應。透過雲端架構訓練AI模型，精準重建時間序列健康訊號，並捕捉非線性污染反應。研究使用個人智慧手錶的數據進行遷移學習，提升模型泛化能力，展現此方法在真實世界使用者數據中的適應性。", "audio": "audios/2505.10556v1.mp3", "timestamp": "2025-05-18T23:05:46.038004"}
{"id": "2505.10527v1", "url": "http://arxiv.org/abs/2505.10527v1", "title": "WorldPM: Scaling Human Preference Modeling", "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "published_date": "2025-05-15", "title_zh": "WorldPM：擴展人類偏好建模", "summary_zh": "受到語言模型擴展定律的啟發，我們發現類似的定律也存在於偏好建模中。我們提出 WorldPM (World Preference Modeling) 以強調這種擴展潛力，它統一了人類偏好的表達。我們收集了來自公共論壇的偏好數據，涵蓋不同的用戶群體，並使用1500萬規模的數據和從15億到720億參數的模型進行了廣泛的訓練。結果顯示，對抗性指標（識別欺騙性特徵的能力）隨著訓練數據和模型大小的增加而持續提升；客觀性指標（有明確答案的客觀知識）在大語言模型中展現出湧現行為，突顯了 WorldPM 的可擴展性；主觀性指標則未顯示出擴展趨勢。實驗驗證了 WorldPM 作為偏好微調基礎的有效性。在七個基準測試的 20 個子任務中，WorldPM 顯著提高了跨不同規模人類偏好數據集（7K、100K 和 800K 樣本）的泛化性能，在許多關鍵子任務上的性能提升超過 5%。將 WorldPM 整合到我們的內部 RLHF 流程中，我們觀察到內部和公共評估集的顯著改進，內部評估的增益顯著提高了 4% 到 8%。", "audio": "audios/2505.10527v1.mp3", "timestamp": "2025-05-18T23:05:53.910110"}
{"id": "2505.10525v1", "url": "http://arxiv.org/abs/2505.10525v1", "title": "Sobolev and quasiconformal distortion of intermediate dimension with applications to conformal dimension", "summary": "We study the distortion of intermediate dimension under supercritical Sobolev\nmappings and also under quasiconformal or quasisymmetric homeomorphisms. In\nparticular, we extend to the setting of intermediate dimensions both the\nGehring--V\\\"ais\\\"al\\\"a theorem on dilatation-dependent quasiconformal\ndistortion of dimension and Kovalev's theorem on the nonexistence of metric\nspaces with conformal dimension strictly between zero and one. Applications\ninclude new contributions to the quasiconformal classification of Euclidean\nsets and a new sufficient condition for the vanishing of conformal box-counting\ndimension. We illustrate our conclusions with specific consequences for\nBedford--McMullen carpets, samples of Mandelbrot percolation, and product sets\ncontaining a polynomially convergent sequence factor.", "authors": ["Jonathan M. Fraser", "Jeremy T. Tyson"], "published_date": "2025-05-15", "title_zh": "中間維度的Sobolev及擬共形扭曲，及其於共形維度的應用", "summary_zh": "本研究探討超臨界Sobolev映射及擬共形/擬對稱同胚變換下，中間維度的扭曲現象。我們將Gehring-V\"ais\"al\"a關於膨脹係數與擬共形維度扭曲的定理，以及Kovalev關於不存在共形維度介於0與1之間的度量空間的定理，推廣到中間維度的情境。研究成果應用於歐幾里得集合的擬共形分類，並提出新的充分條件判斷共形盒計數維度是否消失。我們以Bedford-McMullen地毯、Mandelbrot滲流樣本，以及包含多項式收斂序列因子的乘積集合為例，闡述了我們的結論。", "audio": "audios/2505.10525v1.mp3", "timestamp": "2025-05-18T23:05:58.394013"}
{"id": "2505.10496v1", "url": "http://arxiv.org/abs/2505.10496v1", "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "published_date": "2025-05-15", "title_zh": "CheXGenBench：合成胸腔X光片逼真度、隱私與效用之統一評估基準", "summary_zh": "CheXGenBench是一個綜合性的評估框架，用來衡量合成胸腔X光片生成模型的品質。它同時考量逼真度、潛在隱私風險以及在臨床上的實用性。 研究團隊發現現有的評估方法存在缺陷，導致結果不一致且缺乏參考價值。 因此，他們設計了一個標準化的評估基準，包含超過20個量化指標，並使用11種主流的文字轉圖像模型進行測試。研究結果揭示了現有評估方法的不足。 此外，研究團隊也釋出一個高品質的合成胸腔X光片資料集，SynthCheX-75K，包含7萬5千張圖像，以支持醫學AI領域的進一步研究。", "audio": "audios/2505.10496v1.mp3", "timestamp": "2025-05-18T23:06:03.275100"}
{"id": "2505.10490v1", "url": "http://arxiv.org/abs/2505.10490v1", "title": "Campus AI vs Commercial AI: A Late-Breaking Study on How LLM As-A-Service Customizations Shape Trust and Usage Patterns", "summary": "As the use of Large Language Models (LLMs) by students, lecturers and\nresearchers becomes more prevalent, universities - like other organizations -\nare pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer\naccessible pre-trained models, customizable to specific (business) needs. While\nmost studies prioritize data, model, or infrastructure adaptations (e.g., model\nfine-tuning), we focus on user-salient customizations, like interface changes\nand corporate branding, which we argue influence users' trust and usage\npatterns. This study serves as a functional prequel to a large-scale field\nstudy in which we examine how students and employees at a German university\nperceive and use their institution's customized LLMaaS compared to ChatGPT. The\ngoals of this prequel are to stimulate discussions on psychological effects of\nLLMaaS customizations and refine our research approach through feedback. Our\nforthcoming findings will deepen the understanding of trust dynamics in LLMs,\nproviding practical guidance for organizations considering LLMaaS deployment.", "authors": ["Leon Hannig", "Annika Bush", "Meltem Aksoy", "Steffen Becker", "Greta Ontrup"], "published_date": "2025-05-15", "title_zh": "校園AI vs. 商業AI：一項關於LLM即服務客製化如何形塑信任與使用模式的最新研究", "summary_zh": "隨著大學生、講師和研究人員廣泛使用大型語言模型（LLM），各大學正面臨制定完善AI策略的需求。本研究探討使用者可見的LLM客製化，例如介面修改和品牌形象，如何影響使用者對大學客製LLM的信任感和使用模式，並與ChatGPT進行比較。這項前期研究旨在引發關於LLM客製化心理效應的討論，並為後續的大規模實地研究提供方向，最終目標是深入了解LLM中的信任動態，為考慮部署LLMaaS的組織提供實用建議。", "audio": "audios/2505.10490v1.mp3", "timestamp": "2025-05-18T23:14:13.567508"}
{"id": "2505.10472v1", "url": "http://arxiv.org/abs/2505.10472v1", "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "published_date": "2025-05-15", "title_zh": "用於癌症溝通的大型語言模型：評估生成式人工智慧中的語言品質、安全性和可及性", "summary_zh": "關於乳癌和子宮頸癌的有效溝通仍然是個健康挑戰。本研究評估了大型語言模型（LLMs）生成準確、安全且易於理解的癌症資訊的能力。研究發現，通用LLMs在語言品質和感染力方面表現較好，而醫療LLMs則在溝通可及性方面更勝一籌。然而，醫療LLMs也更容易產生潛在危害、毒性和偏見。總體而言，研究強調了在設計模型時需要有針對性地改進，特別是在降低危害和偏見方面，從而創建更安全、有效且可信賴的AI健康資訊工具。", "audio": "audios/2505.10472v1.mp3", "timestamp": "2025-05-18T23:14:30.270685"}
{"id": "2505.10468v1", "url": "http://arxiv.org/abs/2505.10468v1", "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge", "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "published_date": "2025-05-15", "title_zh": "AI代理程式 vs. 具代理能力AI：概念分類、應用與挑戰", "summary_zh": "本研究區分了AI代理程式（AI Agents）和具代理能力AI（Agentic AI）兩個概念，並建立了結構化的分類體系。AI代理程式著重於利用大型語言模型（LLMs）和大型圖像模型（LIMs）進行狹窄、特定任務的自動化。而具代理能力AI則是一種範式轉移，強調多代理程式協作、動態任務分解、持久記憶和協調自主。研究比較了兩者的架構演進、運作機制、互動方式和自主程度，並探討了各自在客戶支援、研究自動化等領域的應用。同時，也分析了幻覺、脆弱性、突發行為和協調失敗等挑戰，並提出了針對性的解決方案。本研究旨在為開發穩健、可擴展且可解釋的AI代理程式和具代理能力AI系統提供清晰的指引。", "audio": "audios/2505.10468v1.mp3", "timestamp": "2025-05-18T15:29:49.604850"}
{"id": "2505.10454v1", "url": "http://arxiv.org/abs/2505.10454v1", "title": "Emotion-sensitive Explanation Model", "summary": "Explainable AI (XAI) research has traditionally focused on rational users,\naiming to improve understanding and reduce cognitive biases. However, emotional\nfactors play a critical role in how explanations are perceived and processed.\nPrior work shows that prior and task-generated emotions can negatively impact\nthe understanding of explanation. Building on these insights, we propose a\nthree-stage model for emotion-sensitive explanation grounding: (1) emotional or\nepistemic arousal, (2) understanding, and (3) agreement. This model provides a\nconceptual basis for developing XAI systems that dynamically adapt explanation\nstrategies to users emotional states, ultimately supporting more effective and\nuser-centered decision-making.", "authors": ["Christian Schütze", "Birte Richter", "Britta Wrede"], "published_date": "2025-05-15", "title_zh": "情緒敏感的解釋模型", "summary_zh": "傳統的可解釋AI(XAI)研究主要關注理性用戶，旨在提升理解和減少認知偏差。然而，情緒在解釋的感知和處理中扮演關鍵角色。本文提出一個三階段的情緒敏感解釋模型，包含情緒激發、理解和認同。此模型為開發能根據用戶情緒狀態動態調整解釋策略的XAI系統提供概念基礎，最終支持更有效且以用戶為中心的決策。", "audio": "audios/2505.10454v1.mp3", "timestamp": "2025-05-18T15:43:10.566009"}
{"id": "2505.10453v1", "url": "http://arxiv.org/abs/2505.10453v1", "title": "Vision language models have difficulty recognizing virtual objects", "summary": "Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.", "authors": ["Tyler Tran", "Sangeet Khemlani", "J. G. Trafton"], "published_date": "2025-05-15", "title_zh": "視覺語言模型難以識別虛擬物件", "summary_zh": "視覺語言模型（VLMs）結合了語言和視覺編碼器，可以處理多模態輸入，例如自動生成圖片描述。然而，它們對圖像中場景的視覺空間理解程度仍然是個問題。研究發現，當提示VLMs想像圖像中不存在的虛擬物件（例如：一棵樹下站著一個人，提示想像樹上卡著風箏），它們難以合理更新場景表徵並推理物件之間的空間關係。這表明目前的VLMs在處理虛擬物件方面的能力不足。", "audio": "audios/2505.10453v1.mp3", "timestamp": "2025-05-18T15:52:48.305855"}
{"id": "2505.10427v1", "url": "http://arxiv.org/abs/2505.10427v1", "title": "Influence of prior and task generated emotions on XAI explanation retention and understanding", "summary": "The explanation of AI results and how they are received by users is an\nincreasingly active research field. However, there is a surprising lack of\nknowledge about how social factors such as emotions affect the process of\nexplanation by a decision support system (DSS). While previous research has\nshown effects of emotions on DSS supported decision-making, it remains unknown\nin how far emotions affect cognitive processing during an explanation. In this\nstudy, we, therefore, investigated the influence of prior emotions and\ntask-related arousal on the retention and understanding of explained feature\nrelevance. To investigate the influence of prior emotions, we induced happiness\nand fear prior to the decision support interaction. Before emotion induction,\nuser characteristics to assess their risk type were collected via a\nquestionnaire. To identify emotional reactions to the explanations of the\nrelevance of different features, we observed heart rate variability (HRV),\nfacial expressions, and self-reported emotions of the explainee while observing\nand listening to the explanation and assessed their retention of the features\nas well as their influence on the outcome of the decision task. Results\nindicate that (1) task-unrelated prior emotions do not affected the ratantion\nbut may affect the understanding of the relevance of certain features in the\nsense of an emotion-induced confirmation bias, (2) certain features related to\npersonal attitudes yielded arousal in individual participants, (3) this arousal\naffected the understanding of these variables.", "authors": ["Birte Richter", "Christian Schütze", "Anna Aksonova", "Britta Wrede"], "published_date": "2025-05-15", "title_zh": "先前情緒與任務產生情緒對XAI解釋保留與理解的影響", "summary_zh": "理解AI決策的解釋越來越重要。本研究探討情緒如何影響使用者對AI解釋的理解與記憶。我們透過誘導快樂與恐懼情緒，以及監測心率、面部表情等生理反應，觀察情緒如何影響使用者理解AI解釋中的特徵重要性。研究發現，先前情緒可能影響使用者對特定特徵的理解，產生情緒誘導的確認偏誤；某些與個人態度相關的特徵會引起使用者的情緒反應，進而影響他們對這些特徵的理解。", "audio": "audios/2505.10427v1.mp3", "timestamp": "2025-05-18T16:06:13.256765"}
