{"id": "2505.10561v1", "url": "http://arxiv.org/abs/2505.10561v1", "title": "T2A-Feedback: Improving Basic Capabilities of Text-to-Audio Generation via Fine-grained AI Feedback", "summary": "Text-to-audio (T2A) generation has achieved remarkable progress in generating\na variety of audio outputs from language prompts. However, current\nstate-of-the-art T2A models still struggle to satisfy human preferences for\nprompt-following and acoustic quality when generating complex multi-event\naudio. To improve the performance of the model in these high-level\napplications, we propose to enhance the basic capabilities of the model with AI\nfeedback learning. First, we introduce fine-grained AI audio scoring pipelines\nto: 1) verify whether each event in the text prompt is present in the audio\n(Event Occurrence Score), 2) detect deviations in event sequences from the\nlanguage description (Event Sequence Score), and 3) assess the overall acoustic\nand harmonic quality of the generated audio (Acoustic&Harmonic Quality). We\nevaluate these three automatic scoring pipelines and find that they correlate\nsignificantly better with human preferences than other evaluation metrics. This\nhighlights their value as both feedback signals and evaluation metrics.\nUtilizing our robust scoring pipelines, we construct a large audio preference\ndataset, T2A-FeedBack, which contains 41k prompts and 249k audios, each\naccompanied by detailed scores. Moreover, we introduce T2A-EpicBench, a\nbenchmark that focuses on long captions, multi-events, and story-telling\nscenarios, aiming to evaluate the advanced capabilities of T2A models. Finally,\nwe demonstrate how T2A-FeedBack can enhance current state-of-the-art audio\nmodel. With simple preference tuning, the audio generation model exhibits\nsignificant improvements in both simple (AudioCaps test set) and complex\n(T2A-EpicBench) scenarios.", "authors": ["Zehan Wang", "Ke Lei", "Chen Zhu", "Jiawei Huang", "Sashuai Zhou", "Luping Liu", "Xize Cheng", "Shengpeng Ji", "Zhenhui Ye", "Tao Jin", "Zhou Zhao"], "published_date": "2025-05-15", "title_zh": "T2A-Feedback：透過細粒度AI回饋提升文字轉語音生成之基礎能力", "summary_zh": "現今的文字轉語音模型在生成複雜多事件音訊時，難以完全符合人類對提示遵循度和音訊品質的期望。本研究提出利用AI回饋學習來提升模型基礎能力。首先，建立了精細的AI音訊評分流程，包含事件發生評分、事件序列評分，以及音訊和諧品質評分。這些評分流程能更準確地反映人類偏好。接著，利用這些評分流程，構建了包含41k個提示和249k個音訊的大型音訊偏好數據集T2A-FeedBack，並推出專注於長描述、多事件和故事敘述場景的評測基準T2A-EpicBench。實驗證明，透過簡單的偏好調整，T2A-FeedBack能有效提升現有音訊生成模型在簡單和複雜場景下的表現。", "audio": "audios/2505.10561v1.mp3", "timestamp": "2025-05-18T23:05:41.112280"}
{"id": "2505.10556v1", "url": "http://arxiv.org/abs/2505.10556v1", "title": "An AI-driven framework for the prediction of personalised health response to air pollution", "summary": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "published_date": "2025-05-15", "title_zh": "一個AI驅動的框架，用於預測個人化健康對空氣污染的反應", "summary_zh": "本研究提出一個新的AI框架，結合穿戴裝置收集的生理數據和即時環境暴露數據，來預測個人對空氣污染的健康反應。透過雲端架構訓練AI模型，精準重建時間序列健康訊號，並捕捉非線性污染反應。研究使用個人智慧手錶的數據進行遷移學習，提升模型泛化能力，展現此方法在真實世界使用者數據中的適應性。", "audio": "audios/2505.10556v1.mp3", "timestamp": "2025-05-18T23:05:46.038004"}
{"id": "2505.10527v1", "url": "http://arxiv.org/abs/2505.10527v1", "title": "WorldPM: Scaling Human Preference Modeling", "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "published_date": "2025-05-15", "title_zh": "WorldPM：擴展人類偏好建模", "summary_zh": "受到語言模型擴展定律的啟發，我們發現類似的定律也存在於偏好建模中。我們提出 WorldPM (World Preference Modeling) 以強調這種擴展潛力，它統一了人類偏好的表達。我們收集了來自公共論壇的偏好數據，涵蓋不同的用戶群體，並使用1500萬規模的數據和從15億到720億參數的模型進行了廣泛的訓練。結果顯示，對抗性指標（識別欺騙性特徵的能力）隨著訓練數據和模型大小的增加而持續提升；客觀性指標（有明確答案的客觀知識）在大語言模型中展現出湧現行為，突顯了 WorldPM 的可擴展性；主觀性指標則未顯示出擴展趨勢。實驗驗證了 WorldPM 作為偏好微調基礎的有效性。在七個基準測試的 20 個子任務中，WorldPM 顯著提高了跨不同規模人類偏好數據集（7K、100K 和 800K 樣本）的泛化性能，在許多關鍵子任務上的性能提升超過 5%。將 WorldPM 整合到我們的內部 RLHF 流程中，我們觀察到內部和公共評估集的顯著改進，內部評估的增益顯著提高了 4% 到 8%。", "audio": "audios/2505.10527v1.mp3", "timestamp": "2025-05-18T23:05:53.910110"}
{"id": "2505.10525v1", "url": "http://arxiv.org/abs/2505.10525v1", "title": "Sobolev and quasiconformal distortion of intermediate dimension with applications to conformal dimension", "summary": "We study the distortion of intermediate dimension under supercritical Sobolev\nmappings and also under quasiconformal or quasisymmetric homeomorphisms. In\nparticular, we extend to the setting of intermediate dimensions both the\nGehring--V\\\"ais\\\"al\\\"a theorem on dilatation-dependent quasiconformal\ndistortion of dimension and Kovalev's theorem on the nonexistence of metric\nspaces with conformal dimension strictly between zero and one. Applications\ninclude new contributions to the quasiconformal classification of Euclidean\nsets and a new sufficient condition for the vanishing of conformal box-counting\ndimension. We illustrate our conclusions with specific consequences for\nBedford--McMullen carpets, samples of Mandelbrot percolation, and product sets\ncontaining a polynomially convergent sequence factor.", "authors": ["Jonathan M. Fraser", "Jeremy T. Tyson"], "published_date": "2025-05-15", "title_zh": "中間維度的Sobolev及擬共形扭曲，及其於共形維度的應用", "summary_zh": "本研究探討超臨界Sobolev映射及擬共形/擬對稱同胚變換下，中間維度的扭曲現象。我們將Gehring-V\"ais\"al\"a關於膨脹係數與擬共形維度扭曲的定理，以及Kovalev關於不存在共形維度介於0與1之間的度量空間的定理，推廣到中間維度的情境。研究成果應用於歐幾里得集合的擬共形分類，並提出新的充分條件判斷共形盒計數維度是否消失。我們以Bedford-McMullen地毯、Mandelbrot滲流樣本，以及包含多項式收斂序列因子的乘積集合為例，闡述了我們的結論。", "audio": "audios/2505.10525v1.mp3", "timestamp": "2025-05-18T23:05:58.394013"}
{"id": "2505.10496v1", "url": "http://arxiv.org/abs/2505.10496v1", "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "published_date": "2025-05-15", "title_zh": "CheXGenBench：合成胸腔X光片逼真度、隱私與效用之統一評估基準", "summary_zh": "CheXGenBench是一個綜合性的評估框架，用來衡量合成胸腔X光片生成模型的品質。它同時考量逼真度、潛在隱私風險以及在臨床上的實用性。 研究團隊發現現有的評估方法存在缺陷，導致結果不一致且缺乏參考價值。 因此，他們設計了一個標準化的評估基準，包含超過20個量化指標，並使用11種主流的文字轉圖像模型進行測試。研究結果揭示了現有評估方法的不足。 此外，研究團隊也釋出一個高品質的合成胸腔X光片資料集，SynthCheX-75K，包含7萬5千張圖像，以支持醫學AI領域的進一步研究。", "audio": "audios/2505.10496v1.mp3", "timestamp": "2025-05-18T23:06:03.275100"}
{"id": "2505.10490v1", "url": "http://arxiv.org/abs/2505.10490v1", "title": "Campus AI vs Commercial AI: A Late-Breaking Study on How LLM As-A-Service Customizations Shape Trust and Usage Patterns", "summary": "As the use of Large Language Models (LLMs) by students, lecturers and\nresearchers becomes more prevalent, universities - like other organizations -\nare pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer\naccessible pre-trained models, customizable to specific (business) needs. While\nmost studies prioritize data, model, or infrastructure adaptations (e.g., model\nfine-tuning), we focus on user-salient customizations, like interface changes\nand corporate branding, which we argue influence users' trust and usage\npatterns. This study serves as a functional prequel to a large-scale field\nstudy in which we examine how students and employees at a German university\nperceive and use their institution's customized LLMaaS compared to ChatGPT. The\ngoals of this prequel are to stimulate discussions on psychological effects of\nLLMaaS customizations and refine our research approach through feedback. Our\nforthcoming findings will deepen the understanding of trust dynamics in LLMs,\nproviding practical guidance for organizations considering LLMaaS deployment.", "authors": ["Leon Hannig", "Annika Bush", "Meltem Aksoy", "Steffen Becker", "Greta Ontrup"], "published_date": "2025-05-15", "title_zh": "校園AI vs. 商業AI：一項關於LLM即服務客製化如何形塑信任與使用模式的最新研究", "summary_zh": "隨著大學生、講師和研究人員廣泛使用大型語言模型（LLM），各大學正面臨制定完善AI策略的需求。本研究探討使用者可見的LLM客製化，例如介面修改和品牌形象，如何影響使用者對大學客製LLM的信任感和使用模式，並與ChatGPT進行比較。這項前期研究旨在引發關於LLM客製化心理效應的討論，並為後續的大規模實地研究提供方向，最終目標是深入了解LLM中的信任動態，為考慮部署LLMaaS的組織提供實用建議。", "audio": "audios/2505.10490v1.mp3", "timestamp": "2025-05-18T23:14:13.567508"}
{"id": "2505.10472v1", "url": "http://arxiv.org/abs/2505.10472v1", "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "published_date": "2025-05-15", "title_zh": "用於癌症溝通的大型語言模型：評估生成式人工智慧中的語言品質、安全性和可及性", "summary_zh": "關於乳癌和子宮頸癌的有效溝通仍然是個健康挑戰。本研究評估了大型語言模型（LLMs）生成準確、安全且易於理解的癌症資訊的能力。研究發現，通用LLMs在語言品質和感染力方面表現較好，而醫療LLMs則在溝通可及性方面更勝一籌。然而，醫療LLMs也更容易產生潛在危害、毒性和偏見。總體而言，研究強調了在設計模型時需要有針對性地改進，特別是在降低危害和偏見方面，從而創建更安全、有效且可信賴的AI健康資訊工具。", "audio": "audios/2505.10472v1.mp3", "timestamp": "2025-05-18T23:14:30.270685"}
