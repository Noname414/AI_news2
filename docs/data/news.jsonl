{"query": "AI", "id": "2506.06242v1", "url": "http://arxiv.org/abs/2506.06242v1", "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models", "summary": "Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}", "authors": ["Zahra Babaiee", "Peyman M. Kiasari", "Daniela Rus", "Radu Grosu"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:19:31.539877", "title_zh": "視覺圖形競技場：評估視覺和多模態大型語言模型的視覺概念化能力", "summary_zh": "近年來，多模態大型語言模型在視覺問答方面取得了顯著進展。然而，AI在「概念化」能力上仍存在差距，也就是辨識和推理相同概念，不受視覺形式變化的影響。為了解決這個問題，我們推出了視覺圖形競技場（VGA），它是一個包含六個基於圖形的任務的數據集，旨在評估和提升AI系統的視覺抽象能力。VGA使用不同的圖形佈局來測試獨立於視覺形式的推理。實驗結果顯示，人類在各項任務中幾乎達到完美準確度，而模型在同構檢測方面完全失敗，在路徑/循環任務方面表現有限，這突顯了當前AI模型在視覺理解方面的根本局限性。VGA提供了一個框架，旨在推動AI視覺模型在概念化方面取得類似人類的進展。", "applications": ["**自動駕駛：** 讓汽車能辨識不同角度或光線下的交通標誌，確保行車安全。例如，即使交通標誌被樹葉遮蔽一部分，或因光線反射而變形，汽車也能正確判斷其意義。", "**醫療影像分析：** 協助醫生辨識X光片或斷層掃描中不同形態的腫瘤，提高診斷準確性。例如，即使腫瘤形狀不規則或與周圍組織融合，AI也能準確辨識並標記。", "**智慧零售：** 讓機器人能辨識貨架上不同包裝或擺放方式的商品，提升倉儲和物流效率。例如，即使商品條碼被遮蓋，或商品被隨意堆放，機器人也能準確辨識商品種類和數量。"], "pitch": "各位投資人，我們正處於AI革命的風口浪尖！視覺圖形競技場（VGA）不僅僅是一個數據集，它是解鎖AI真正視覺理解能力的鑰匙。試想一下，一個能像人類一樣理解世界，不受視覺表象干擾的AI，它將顛覆自動駕駛、醫療診斷、智慧製造等各個領域。目前AI在概念化方面的不足，正是我們VGA的機會。我們正在打造下一代AI視覺引擎，它將超越簡單的模式匹配，真正理解圖像背後的概念。這意味著更安全可靠的自動駕駛、更精準高效的醫療診斷、以及更智能化的生產流程。我們的團隊由頂尖的AI專家組成，我們有信心將VGA打造成AI視覺領域的黃金標準。現在投資VGA，您不僅僅是投資一個數據集，更是投資一個充滿無限可能的未來！讓我們一起引領這場視覺智能的革命，共同創造一個更智能、更美好的世界！", "audio": "docs/data/audios/2506.06242v1.wav"}
{"query": "Foundation Model", "id": "2506.06281v1", "url": "http://arxiv.org/abs/2506.06281v1", "title": "TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation", "summary": "Modern Earth observation (EO) increasingly leverages deep learning to harness\nthe scale and diversity of satellite imagery across sensors and regions. While\nrecent foundation models have demonstrated promising generalization across EO\ntasks, many remain limited by the scale, geographical coverage, and spectral\ndiversity of their training data, factors critical for learning globally\ntransferable representations. In this work, we introduce TerraFM, a scalable\nself-supervised learning model that leverages globally distributed Sentinel-1\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\nsampling to enrich spatial and semantic coverage. By treating sensing\nmodalities as natural augmentations in our self-supervised approach, we unify\nradar and optical inputs via modality-specific patch embeddings and adaptive\ncross-attention fusion. Our training strategy integrates local-global\ncontrastive learning and introduces a dual-centering mechanism that\nincorporates class-frequency-aware regularization to address long-tailed\ndistributions in land cover.TerraFM achieves strong generalization on both\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\nhttps://github.com/mbzuai-oryx/TerraFM .", "authors": ["Muhammad Sohail Danish", "Muhammad Akhtar Munir", "Syed Roshaan Ali Shah", "Muhammad Haris Khan", "Rao Muhammad Anwer", "Jorma Laaksonen", "Fahad Shahbaz Khan", "Salman Khan"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:20:54.025845", "title_zh": "TerraFM：適用於統一多感測器地球觀測的可擴展基礎模型", "summary_zh": "TerraFM是一個利用深度學習技術，結合Sentinel-1和Sentinel-2衛星影像的可擴展自監督學習模型。它透過獨特的空間瓦片和土地覆蓋感知採樣方法，豐富了空間和語義覆蓋範圍。TerraFM將雷達和光學輸入視為自然增強，透過模態特定的patch嵌入和自適應交叉注意力融合來統一處理。其訓練策略結合了局部-全局對比學習，並引入雙中心機制，以解決土地覆蓋中長尾分佈的問題。TerraFM在分類和分割任務上表現出色，優於先前的模型，為地球觀測領域帶來了更強大的通用性和準確性。", "applications": ["農作物監測：農民可以利用TerraFM分析衛星影像，了解農作物的生長狀況、預測產量，及早發現病蟲害，提高農業生產效率。", "災害評估：在地震、洪水等災害發生後，TerraFM可以快速分析災區的受損情況，協助救援人員制定更有效的救援計畫，並進行災後重建。", "環境保護：環保機構可以利用TerraFM監測森林砍伐、水污染等環境問題，及時採取措施保護地球資源。"], "pitch": "各位創投先進，想像一下，我們正站在一個前所未有的數據金礦之上：地球觀測數據！TerraFM，我們的殺手級應用，正是開啟這座寶藏的鑰匙。它不僅能整合不同衛星感測器的數據，更具備強大的泛化能力，能應用於農業、災害管理、環境監測等各個領域。這意味著什麼？更精準的作物預測，減少糧食浪費；更快速的災害評估，拯救更多生命；更有效的環境監測，守護我們的地球。但這還不是全部！TerraFM的自監督學習能力，使其能不斷從海量數據中自我提升，就像一個永動機，不斷產生價值。未來，我們甚至可以將TerraFM應用於城市規劃、基礎設施建設、甚至是國防安全等更廣闊的領域。現在投資TerraFM，就是投資地球的未來，回報將遠超您的想像！", "audio": "docs/data/audios/2506.06281v1.wav"}
{"query": "Diffusion Model", "id": "2506.06276v1", "url": "http://arxiv.org/abs/2506.06276v1", "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis", "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.", "authors": ["Jiatao Gu", "Tianrong Chen", "David Berthelot", "Huangjie Zheng", "Yuyang Wang", "Ruixiang Zhang", "Laurent Dinh", "Miguel Angel Bautista", "Josh Susskind", "Shuangfei Zhai"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:22:30.194724", "title_zh": "STARFlow：擴展潛在歸一化流以實現高解析度圖像合成", "summary_zh": "STARFlow是一種基於歸一化流的可擴展生成模型，在高解析度圖像合成方面表現出色。其核心是Transformer自迴歸流（TARFlow），結合了歸一化流的表達能力和自迴歸Transformer的結構化建模能力。STARFlow通過深度-淺層設計、在預訓練自編碼器的潛在空間中建模以及創新的引導算法，顯著提高了可擴展性。該模型保持端到端的歸一化流，無需離散化即可在連續空間中進行精確的最大似然訓練。STARFlow在類條件和文本條件圖像生成任務中表現出色，其樣本品質接近最先進的擴散模型。這是首次成功展示歸一化流在此規模和解析度下有效運作。", "applications": ["想像一下，你想要一張獨一無二的寵物照片，但你沒有專業攝影師。STARFlow可以根據你的文字描述，例如「一隻戴著皇冠的可愛貓咪」，自動生成一張高解析度的照片。", "假設你是遊戲開發者，需要大量不同的遊戲角色和場景。STARFlow可以幫助你快速生成各種風格的遊戲素材，節省大量美術設計的時間和成本。", "如果你是室內設計師，想向客戶展示不同裝修風格的效果圖。STARFlow可以根據客戶的描述，快速生成逼真的室內設計圖，方便客戶選擇。"], "pitch": "各位投資人，我們帶來的是STARFlow，一項革命性的圖像生成技術，它將徹底改變圖像內容創作的遊戲規則！想像一下，一個可以根據簡單的文字描述，就能生成照片級別真實圖像的世界。STARFlow不僅僅是一個技術突破，它是一座金礦！在廣告行銷領域，它可以創造出高度個性化的廣告素材，大幅提升點擊率和轉換率。在娛樂產業，它可以賦予遊戲開發者和電影製作人前所未有的創作自由。在電商領域，它可以自動生成商品圖片，降低運營成本。更重要的是，隨著元宇宙的興起，對虛擬內容的需求將呈現爆炸式增長，而STARFlow正是滿足這一需求的完美解決方案。我們的團隊擁有世界一流的AI專家，我們已經成功驗證了STARFlow的技術可行性和商業潛力。現在，我們需要您的投資，共同將STARFlow推向市場，搶佔先機，打造一個全新的圖像內容生態系統。我們相信，STARFlow將成為下一代圖像生成技術的領導者，為您帶來豐厚的回報！", "audio": "docs/data/audios/2506.06276v1.wav"}
{"query": "AI", "id": "2506.06232v1", "url": "http://arxiv.org/abs/2506.06232v1", "title": "Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study", "summary": "While traditional computer vision models have historically struggled to\ngeneralize to endoscopic domains, the emergence of foundation models has shown\npromising cross-domain performance. In this work, we present the first\nlarge-scale study assessing the capabilities of Vision Language Models (VLMs)\nfor endoscopic tasks with a specific focus on laparoscopic surgery. Using a\ndiverse set of state-of-the-art models, multiple surgical datasets, and\nextensive human reference annotations, we address three key research questions:\n(1) Can current VLMs solve basic perception tasks on surgical images? (2) Can\nthey handle advanced frame-based endoscopic scene understanding tasks? and (3)\nHow do specialized medical VLMs compare to generalist models in this context?\nOur results reveal that VLMs can effectively perform basic surgical perception\ntasks, such as object counting and localization, with performance levels\ncomparable to general domain tasks. However, their performance deteriorates\nsignificantly when the tasks require medical knowledge. Notably, we find that\nspecialized medical VLMs currently underperform compared to generalist models\nacross both basic and advanced surgical tasks, suggesting that they are not yet\noptimized for the complexity of surgical environments. These findings highlight\nthe need for further advancements to enable VLMs to handle the unique\nchallenges posed by surgery. Overall, our work provides important insights for\nthe development of next-generation endoscopic AI systems and identifies key\nareas for improvement in medical visual language models.", "authors": ["Leon Mayer", "Tim Rädsch", "Dominik Michael", "Lucas Luttner", "Amine Yamlahi", "Evangelia Christodoulou", "Patrick Godau", "Marcel Knopp", "Annika Reinke", "Fiona Kolbinger", "Lena Maier-Hein"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:31:58.254264", "title_zh": "以外科手術數據挑戰視覺語言模型：一個新數據集與廣泛的基準測試研究", "summary_zh": "本研究首次大規模評估視覺語言模型（VLMs）在腹腔鏡手術等內視鏡任務中的能力。我們使用多種先進模型、手術數據集和人工標註，探討VLMs能否勝任手術圖像的基本感知任務和進階的內視鏡場景理解任務，以及專用醫療VLMs與通用模型的比較。結果顯示，VLMs在物體計數和定位等基本任務上表現出色，但處理需要醫學知識的任務時性能顯著下降。令人驚訝的是，專用醫療VLMs的表現不如通用模型，表明它們尚未針對手術環境的複雜性進行優化。這項研究突顯了未來開發內視鏡AI系統的需求，並為改進醫療視覺語言模型指明了方向。", "applications": ["想像一下，未來醫生在做腹腔鏡手術時，AI能即時辨識手術視野中的器官、血管，甚至提醒醫生注意潛在風險，就像有個經驗豐富的助手在旁邊一樣。", "以後醫學院學生可以利用這個AI系統來模擬手術，AI會根據學生的操作給予即時反饋，讓他們在真實手術前就能累積經驗。", "開發一套居家健康監測系統，透過內視鏡影像分析，早期發現腸胃道疾病，讓民眾在家就能進行初步的健康檢查。"], "pitch": "各位投資人，我們正在打造醫療AI的未來！這項技術不僅僅是個研究項目，它將徹底改變外科手術的面貌。想像一下，AI能輔助醫生進行更精準、更安全的手術，降低醫療事故的發生率，並大幅縮短手術時間。更重要的是，我們發現專用醫療模型的表現不如通用模型，這代表著巨大的市場機會！我們將開發針對手術環境優化的VLMs，解決現有模型的瓶頸，打造出真正能夠理解手術場景的AI。這不僅能應用於手術室，還能拓展到遠程醫療、醫學教育等領域，潛在市場規模數十億美元！現在加入我們，一起開創醫療AI的新紀元！", "audio": "docs/data/audios/2506.06232v1.wav"}
{"query": "Foundation Model", "id": "2506.06270v1", "url": "http://arxiv.org/abs/2506.06270v1", "title": "RecGPT: A Foundation Model for Sequential Recommendation", "summary": "This work addresses a fundamental barrier in recommender systems: the\ninability to generalize across domains without extensive retraining.\nTraditional ID-based approaches fail entirely in cold-start and cross-domain\nscenarios where new users or items lack sufficient interaction history.\nInspired by foundation models' cross-domain success, we develop a foundation\nmodel for sequential recommendation that achieves genuine zero-shot\ngeneralization capabilities. Our approach fundamentally departs from existing\nID-based methods by deriving item representations exclusively from textual\nfeatures. This enables immediate embedding of any new item without model\nretraining. We introduce unified item tokenization with Finite Scalar\nQuantization that transforms heterogeneous textual descriptions into\nstandardized discrete tokens. This eliminates domain barriers that plague\nexisting systems. Additionally, the framework features hybrid\nbidirectional-causal attention that captures both intra-item token coherence\nand inter-item sequential dependencies. An efficient catalog-aware beam search\ndecoder enables real-time token-to-item mapping. Unlike conventional approaches\nconfined to their training domains, RecGPT naturally bridges diverse\nrecommendation contexts through its domain-invariant tokenization mechanism.\nComprehensive evaluations across six datasets and industrial scenarios\ndemonstrate consistent performance advantages.", "authors": ["Yangqin Jiang", "Xubin Ren", "Lianghao Xia", "Da Luo", "Kangyi Lin", "Chao Huang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:33:25.131072", "title_zh": "RecGPT：用於序列推薦的基礎模型", "summary_zh": "RecGPT 是一個突破性的推薦系統，它像大型語言模型一樣，具備跨領域的泛化能力，不需要針對新領域重新訓練。它捨棄了傳統基於ID的方法，改為完全從文字特徵提取商品資訊，讓新商品能立即加入推薦，無需重新訓練模型。RecGPT 使用統一的商品符號化方法，將各種文字描述轉換為標準化的離散符號，消除了領域之間的障礙。此外，它還採用混合雙向因果注意力機制，捕捉商品內部的關聯和商品之間的順序關係。這種方法在六個數據集和工業場景中都展現了優越的性能，為推薦系統帶來了革命性的改變。", "applications": ["**個人化新聞推薦：** 不再只推薦你看過的新聞，而是根據你讀過的文章內容，推薦其他領域但主題相關的新聞，擴展你的知識視野。", "**跨平台商品推薦：** 假設你在A電商平台買了咖啡豆，RecGPT可以立刻在B平台上推薦你適合的咖啡濾杯或磨豆機，即使你在B平台沒有任何購買紀錄。", "**冷啟動影視推薦：** 新上映的冷門獨立電影，即使觀看人數不多，RecGPT也能透過電影簡介的文字內容，推薦給可能感興趣的觀眾，讓小眾佳作也能被發掘。"], "pitch": "各位投資人，想像一下，一個能理解所有商品和使用者喜好的超級推薦引擎，它不需要大量數據訓練，就能精準推薦，這就是RecGPT的潛力！傳統推薦系統就像個別的孤島，RecGPT則是一座連接所有島嶼的橋樑。它不僅解決了冷啟動和跨領域推薦的難題，更開創了全新的商業模式。我們可以將RecGPT授權給各個電商平台、內容平台，甚至線下零售商，讓他們輕鬆實現個性化推薦，提升銷售額和使用者滿意度。更進一步，RecGPT可以應用於智慧城市、智慧醫療等領域，例如根據病患的病歷和生活習慣，推薦個性化的健康管理方案。未來，RecGPT將成為AI推薦領域的領導者，引領下一代推薦技術的發展。現在投資RecGPT，就是投資未來！", "audio": "docs/data/audios/2506.06270v1.wav"}
{"query": "Diffusion Model", "id": "2506.06185v1", "url": "http://arxiv.org/abs/2506.06185v1", "title": "Antithetic Noise in Diffusion Models", "summary": "We initiate a systematic study of antithetic initial noise in diffusion\nmodels. Across unconditional models trained on diverse datasets,\ntext-conditioned latent-diffusion models, and diffusion-posterior samplers, we\nfind that pairing each initial noise with its negation consistently yields\nstrongly negatively correlated samples. To explain this phenomenon, we combine\nexperiments and theoretical analysis, leading to a symmetry conjecture that the\nlearned score function is approximately affine antisymmetric (odd symmetry up\nto a constant shift), and provide evidence supporting it. Leveraging this\nnegative correlation, we enable two applications: (1) enhancing image diversity\nin models like Stable Diffusion without quality loss, and (2) sharpening\nuncertainty quantification (e.g., up to 90% narrower confidence intervals) when\nestimating downstream statistics. Building on these gains, we extend the\ntwo-point pairing to a randomized quasi-Monte Carlo estimator, which further\nimproves estimation accuracy. Our framework is training-free, model-agnostic,\nand adds no runtime overhead.", "authors": ["Jing Jia", "Sifan Liu", "Bowen Song", "Wei Yuan", "Liyue Shen", "Guanyang Wang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:34:50.492212", "title_zh": "擴散模型中的反義噪音", "summary_zh": "本研究深入探討擴散模型中反義初始噪音的特性。我們發現，無論是無條件模型、文本條件潛在擴散模型還是擴散後驗採樣器，將每個初始噪音與其負值配對，都能產生強烈負相關的樣本。我們提出「對稱猜想」，認為模型學習到的分數函數近似為仿射反對稱（奇對稱加上常數偏移）。基於這種負相關性，我們實現了兩個應用：一是提高Stable Diffusion等模型的圖像多樣性，且不損失品質；二是銳化不確定性量化，例如縮小高達90%的置信區間。此外，我們將雙點配對擴展到隨機準蒙地卡羅估計器，進一步提高了估計準確性。此框架無需訓練、適用於各種模型，且不增加運行時開銷。", "applications": ["想像一下，你想要用AI繪圖產生風景照，但每次生成的結果都很類似。使用這項技術，你可以輕鬆產生更多樣化的風景，讓你的照片集更加豐富。", "醫生在分析X光片時，常常需要判斷是否有微小的病灶。這項技術可以幫助醫生更精準地評估診斷結果的不確定性，提供更可靠的醫療建議。", "科學家在模擬氣候變遷時，需要考慮各種不確定因素。這項技術可以幫助他們更準確地預測氣候變遷的影響，為政策制定提供更可靠的依據。"], "pitch": "各位創投，擴散模型是AI領域的明日之星，但其生成結果的多樣性和預測的準確性仍有提升空間。我們的「反義噪音」技術，無需額外訓練成本，就能顯著提高圖像生成的多樣性，並大幅提升不確定性量化的準確性。這意味著，我們可以打造更具創意、更可靠的AI應用。試想一下，將這項技術應用於自動駕駛，可以更精準地預測路況風險；應用於金融市場預測，可以更有效地管理投資組合風險；應用於新藥研發，可以更快速地篩選潛力藥物。這不僅是一項技術突破，更是一個巨大的商業機會。我們相信，透過您的投資，我們可以將這項技術推向更廣闊的應用領域，共同開創AI的新紀元！", "audio": "docs/data/audios/2506.06185v1.wav"}
{"query": "AI", "id": "2506.06225v1", "url": "http://arxiv.org/abs/2506.06225v1", "title": "\"We need to avail ourselves of GenAI to enhance knowledge distribution\": Empowering Older Adults through GenAI Literacy", "summary": "As generative AI (GenAI) becomes increasingly widespread, it is crucial to\nequip users, particularly vulnerable populations such as older adults (65 and\nolder), with the knowledge to understand its benefits and potential risks.\nOlder adults often exhibit greater reservations about adopting emerging\ntechnologies and require tailored literacy support. Using a mixed methods\napproach, this study examines strategies for delivering GenAI literacy to older\nadults through a chatbot named Litti, evaluating its impact on their AI\nliteracy (knowledge, safety, and ethical use). The quantitative data indicated\na trend toward improved AI literacy, though the results were not statistically\nsignificant. However, qualitative interviews revealed diverse levels of\nfamiliarity with generative AI and a strong desire to learn more. Findings also\nshow that while Litti provided a positive learning experience, it did not\nsignificantly enhance participants' trust or sense of safety regarding GenAI.\nThis exploratory case study highlights the challenges and opportunities in\ndesigning AI literacy education for the rapidly growing older adult population.", "authors": ["Eunhye Grace Ko", "Shaini Nanayakkara", "Earl W. Huff Jr"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:44:00.677565", "title_zh": "「我們需要善用生成式AI來強化知識傳播」：透過生成式AI素養賦能年長者", "summary_zh": "本研究探討如何提升年長者對生成式AI的素養，讓他們了解其益處與潛在風險。研究採用混合方法，透過名為Litti的聊天機器人，評估其對年長者AI素養（知識、安全和道德使用）的影響。定量數據顯示AI素養有改善趨勢，但未達統計顯著性。質性訪談則揭示年長者對生成式AI的熟悉程度各異，但都渴望學習更多。研究發現Litti提供了正面的學習體驗，但並未顯著提升參與者對生成式AI的信任感或安全感。本研究強調了為快速增長的年長者人口設計AI素養教育的挑戰與機會。", "applications": ["**長輩專屬的AI健康管家：** Litti可以變成一個24小時待命的健康顧問，提醒長輩服藥、提供飲食建議，甚至在緊急情況下聯絡家人或救護車。它能用長輩習慣的語言溝通，讓他們更安心。", "**AI陪伴聊天解悶神器：** 許多長輩獨居，Litti可以陪他們聊天、分享新聞、甚至一起玩簡單的遊戲。它能記住長輩的喜好，提供客製化的內容，減少孤獨感。", "**銀髮族數位學習好幫手：** Litti可以教長輩如何使用智慧型手機、平板電腦，讓他們輕鬆上手網路購物、視訊通話，甚至參與線上課程，享受數位生活的便利。"], "pitch": "各位投資人，高齡化社會是全球趨勢，而生成式AI是賦能銀髮族、提升他們生活品質的關鍵技術。想像一下，一個由AI驅動的銀髮族生態系，包含個人化的健康管理、社交互動、數位學習等服務，市場潛力無窮！我們的Litti聊天機器人正是這個生態系的起點。它不僅能提升長輩的AI素養，更能成為他們信任的數位夥伴。我們計劃將Litti整合到各種銀髮族產品和服務中，例如智慧居家設備、遠距醫療平台等，打造一個龐大的銀髮族AI市場。現在投資我們，您將搶佔先機，共同開創銀髮經濟的下一個藍海！未來，我們甚至可以將Litti發展成具有情感理解能力的AI，真正成為長輩們的心靈伴侶，這將是劃時代的創新！", "audio": "docs/data/audios/2506.06225v1.wav"}
{"query": "Foundation Model", "id": "2506.06211v1", "url": "http://arxiv.org/abs/2506.06211v1", "title": "PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts", "summary": "Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined\nproblem definitions. In contrast to conventional reasoning benchmarks\nconsisting of tasks with clear instructions, puzzlehunts require models to\ndiscover the underlying problem structure from multimodal evidence and\niterative reasoning, mirroring real-world domains such as scientific discovery,\nexploratory data analysis, or investigative problem-solving. Despite recent\nprogress in foundation models, their performance on such open-ended settings\nremains largely untested. In this paper, we introduce PuzzleWorld, a\nlarge-scale benchmark of 667 puzzlehunt-style problems designed to assess\nstep-by-step, open-ended, and creative multimodal reasoning. Each puzzle is\nannotated with the final solution, detailed reasoning traces, and cognitive\nskill labels, enabling holistic benchmarking and fine-grained diagnostic\nanalysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,\nwith the best model solving only 14% of puzzles and reaching 40% stepwise\naccuracy. To demonstrate the value of our reasoning annotations, we show that\nfine-tuning a small model on reasoning traces improves stepwise reasoning from\n4% to 11%, while training on final answers alone degrades performance to near\nzero. Our error analysis reveals that current models exhibit myopic reasoning,\nare bottlenecked by the limitations of language-based inference, and lack\nsketching capabilities crucial for visual and spatial reasoning. We release\nPuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on\nbuilding more general, open-ended, and creative reasoning systems.", "authors": ["Hengzhi Li", "Brendon Jiang", "Alexander Naehu", "Regan Song", "Justin Zhang", "Megan Tjandrasuwita", "Chanakya Ekbote", "Steven-Shine Chen", "Adithya Balachandran", "Wei Dai", "Rebecca Chang", "Paul Pu Liang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:45:24.929005", "title_zh": "謎題世界：謎題狩獵中多模態、開放式推理的基準測試", "summary_zh": "「謎題狩獵」是一種複雜、多步驟的謎題類型，缺乏明確的問題定義。PuzzleWorld是一個大型基準測試，包含667個謎題狩獵風格的問題，旨在評估逐步、開放式和創造性的多模態推理。現有模型在最終答案的準確率上僅達到1-2%，最佳模型也僅解決了14%的謎題。研究顯示，模型在推理過程中存在短視近利的問題，並受限於基於語言的推論能力，且缺乏視覺和空間推理所需的草圖能力。此基準測試將有助於開發更通用、開放式和創造性的推理系統，可用於科學發現、數據分析和調查性問題解決等領域。", "applications": ["設計逃脫遊戲：PuzzleWorld可以幫助遊戲設計師創建更具挑戰性、更有趣的逃脫遊戲，透過AI自動生成謎題和線索，讓玩家有更好的遊戲體驗。", "輔助兒童教育：將PuzzleWorld應用於兒童教育，可以開發出更具互動性的學習工具，培養孩子的邏輯思維、空間推理和創造力，讓學習過程更加生動有趣。", "提升企業問題解決能力：企業可以利用PuzzleWorld來訓練員工的解決問題能力，透過模擬真實世界的複雜情境，提升團隊合作和創新思維。"], "pitch": "各位投資人，我們正處於AI發展的關鍵時刻，生成式AI正快速改變世界。然而，現有的AI模型在處理需要多模態推理、開放式問題解決的複雜任務時，能力仍遠遠不足。PuzzleWorld的出現，正是為了填補這一空白。它不僅是一個基準測試，更是一個孕育新一代AI的搖籃。想像一下，未來的AI不僅能理解語言，還能看懂圖像、理解空間關係，甚至能像人類一樣進行創造性思考。這種AI將在科學研究、金融分析、甚至藝術創作等領域產生顛覆性的影響。我們相信，透過PuzzleWorld的持續發展，我們能打造出真正具有通用智能的AI，開創一個充滿無限可能的未來。現在投資PuzzleWorld，就是投資AI的未來，您將成為這場技術革命的先驅！", "audio": "docs/data/audios/2506.06211v1.wav"}
{"query": "Diffusion Model", "id": "2506.06085v1", "url": "http://arxiv.org/abs/2506.06085v1", "title": "Feedback Guidance of Diffusion Models", "summary": "While Classifier-Free Guidance (CFG) has become standard for improving sample\nfidelity in conditional diffusion models, it can harm diversity and induce\nmemorization by applying constant guidance regardless of whether a particular\nsample needs correction. We propose FeedBack Guidance (FBG), which uses a\nstate-dependent coefficient to self-regulate guidance amounts based on need.\nOur approach is derived from first principles by assuming the learned\nconditional distribution is linearly corrupted by the unconditional\ndistribution, contrasting with CFG's implicit multiplicative assumption. Our\nscheme relies on feedback of its own predictions about the conditional signal\ninformativeness to adapt guidance dynamically during inference, challenging the\nview of guidance as a fixed hyperparameter. The approach is benchmarked on\nImageNet512x512, where it significantly outperforms Classifier-Free Guidance\nand is competitive to Limited Interval Guidance (LIG) while benefitting from a\nstrong mathematical framework. On Text-To-Image generation, we demonstrate\nthat, as anticipated, our approach automatically applies higher guidance scales\nfor complex prompts than for simpler ones and that it can be easily combined\nwith existing guidance schemes such as CFG or LIG.", "authors": ["Koulischer Felix", "Handke Florian", "Deleu Johannes", "Demeester Thomas", "Ambrogioni Luca"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:47:05.009526", "title_zh": "擴散模型的反饋引導", "summary_zh": "現行的無分類器引導(CFG)雖然能提升條件式擴散模型的生成品質，但恆定的引導可能損害多樣性並導致記憶化。我們提出反饋引導(FBG)，它使用一個狀態相關係數，根據需求自我調節引導量。FBG基於第一原理推導，假設學習到的條件分佈被無條件分佈線性破壞。FBG利用自身對條件訊號資訊量的預測反饋，在推論過程中動態調整引導，挑戰了引導作為固定超參數的觀點。在ImageNet512x512基準測試中，FBG顯著優於CFG，並與LIG競爭，同時受益於強大的數學框架。在文本到圖像生成中，FBG能針對複雜提示自動應用更高的引導尺度，且易於與現有引導方案(如CFG或LIG)結合。", "applications": ["想像一下，你想要AI幫你畫一張生日派對的邀請函，但你只給了很簡單的描述，像是「生日快樂」。傳統的AI可能會畫出很普通的派對畫面。但用了反饋引導，AI會自動判斷這個提示太簡單，需要加強引導，於是它會加入更多細節，像是氣球、蛋糕、禮物等等，讓邀請函更豐富。", "假設你是服裝設計師，想用AI生成一些新的設計稿。你輸入一個比較模糊的概念，像是「未來感外套」。用了反饋引導的AI，會根據這個概念的複雜度，自動調整生成過程，確保生成的外套設計既有未來感，又不會過於抽象或難以理解，讓設計師能更容易的激發靈感。", "如果你在玩AI繪圖，想要生成一張特定風格的圖片，例如「梵谷風格的貓」。如果提示不夠明確，AI可能會畫出很普通的貓。但有了反饋引導，AI會自動加強梵谷風格的元素，像是用色、筆觸等等，讓生成的貓咪圖片更具藝術感，更符合你的期望。"], "pitch": "各位投資人，我們帶來的是擴散模型領域的革命性技術——反饋引導(FBG)。現有的生成式AI，如DALL-E、Midjourney等，都依賴於人工設定的引導參數，這不僅耗時，也限制了AI的創造力。FBG技術顛覆了這一模式，它讓AI能夠像一位經驗豐富的藝術家一樣，根據創作內容的複雜程度，自動調整引導的力度，從而生成更高品質、更具創意、更符合使用者需求的圖像。想像一下，未來，設計師、藝術家、甚至是普通使用者，都能夠輕鬆地利用AI創造出獨一無二的作品，而無需具備專業的AI知識。這將開啟一個全新的創意經濟時代，市場規模將是數百億美元級別的。更重要的是，FBG技術不僅僅局限於圖像生成，它還可以應用於音訊、影片、甚至3D模型的生成，潛力無限。我們相信，FBG技術將成為下一代生成式AI的核心引擎，而我們團隊將引領這場技術革命。現在加入我們，共同打造AI驅動的未來，您將獲得豐厚的回報！", "audio": "docs/data/audios/2506.06085v1.wav"}
{"query": "AI", "id": "2506.06220v1", "url": "http://arxiv.org/abs/2506.06220v1", "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval", "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind,\nthat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction.", "authors": ["Diji Yang", "Minghao Liu", "Chung-Hsiang Lo", "Yi Zhang", "James Davis"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:29:11.968645", "title_zh": "GenIR：用於心理圖像檢索的生成式視覺回饋", "summary_zh": "現有的視覺語言模型在文字到圖像檢索方面表現出色，但實際應用仍存在挑戰。GenIR針對「心理圖像檢索」任務，讓使用者能透過多輪互動，逐步逼近腦海中的圖像。GenIR的核心是利用擴散模型生成圖像，將AI系統的理解視覺化呈現，提供清晰且可操作的回饋。使用者能根據這些視覺回饋，更直觀有效地調整檢索條件。我們還建立了全自動流程，生成高品質的多輪心理圖像檢索數據集。實驗結果顯示，GenIR顯著優於現有的互動式方法，為未來研究奠定了基礎。", "applications": ["想像一下，你忘記了小時候最喜歡的玩具長什麼樣子，但還記得一些模糊的特徵。透過GenIR，你可以描述這些特徵，系統會生成可能的圖像，讓你逐步縮小範圍，最終找到你心心念念的玩具。", "假設你想在家裡重新裝潢，但腦海中只有一些零碎的想法。你可以用GenIR描述你想要的風格、顏色和家具，系統會生成不同的房間設計，幫助你找到最喜歡的方案，省去尋找靈感的時間。", "如果你正在尋找失散多年的親人，但只有一些模糊的記憶，例如臉部特徵或衣著風格。GenIR可以根據你的描述生成可能的圖像，幫助你擴大搜索範圍，增加找到親人的機會。"], "pitch": "各位投資人，我們相信GenIR將徹底改變圖像檢索的未來！現今的圖像檢索技術往往無法滿足人們腦海中模糊的需求。GenIR透過生成式視覺回饋，讓人機互動更加直觀高效，解決了這個痛點。想像一下，未來的電商平台，使用者只需描述想要的商品，AI就能生成商品圖像，甚至可以根據使用者的喜好客製化設計。在醫療領域，醫生可以透過GenIR，根據患者的描述生成病灶圖像，輔助診斷。在安全領域，警方可以根據目擊者的描述，生成嫌疑犯的模擬圖像，提高破案率。GenIR的應用場景無限廣闊，市場潛力巨大。我們已經建立了一個高品質的數據集，並開發了領先的生成式檢索方法。我們正在尋找有遠見的投資人，一起將GenIR推向市場，引領下一代圖像檢索革命！", "audio": "docs/data/audios/2506.06220v1.wav"}
{"query": "Foundation Model", "id": "2506.06105v1", "url": "http://arxiv.org/abs/2506.06105v1", "title": "Text-to-LoRA: Instant Transformer Adaption", "summary": "While Foundation Models provide a general tool for rapid content creation,\nthey regularly require task-specific adaptation. Traditionally, this exercise\ninvolves careful curation of datasets and repeated fine-tuning of the\nunderlying model. Fine-tuning techniques enable practitioners to adapt\nfoundation models for many new applications but require expensive and lengthy\ntraining while being notably sensitive to hyper-parameter choices. To overcome\nthese limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting\nLarge Language Models on the fly solely based on a natural language description\nof the target task. T2L is a hypernetwork trained to construct LoRAs in a\nsingle inexpensive forward pass. After training T2L on a suite of 9 pre-trained\nLoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA\ninstances match the performance of task-specific adapters across the\ncorresponding test sets. Furthermore, T2L can compress hundreds of LoRA\ninstances and zero-shot generalize to entirely unseen tasks. This approach\nprovides a significant step towards democratizing the specialization of\nfoundation models and enables language-based adaptation with minimal compute\nrequirements. Our code is available at https://github.com/SakanaAI/text-to-lora", "authors": ["Rujikorn Charakorn", "Edoardo Cetin", "Yujin Tang", "Robert Tjarko Lange"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:30:20.416876", "title_zh": "文字到LoRA：即時轉換器適應", "summary_zh": "本研究提出Text-to-LoRA (T2L)模型，能根據自然語言描述，即時調整大型語言模型以適應特定任務。T2L是一種超網路，只需一次前向傳遞就能建構LoRA。經過九個預訓練LoRA適配器訓練後，T2L重建的LoRA實例在對應測試集上表現與特定任務適配器相當。更重要的是，T2L能壓縮數百個LoRA實例，並零樣本泛化到全新任務。這項技術大幅降低了基礎模型專業化的門檻，並以極少的計算資源實現基於語言的適應，讓AI模型客製化變得更快速、更普及。", "applications": ["AI繪圖客製化：使用者只要用文字描述想要的圖片風格(例如：水墨畫、卡通風格)，AI就能快速調整模型，產生符合需求的圖片。", "個人化AI助理：針對不同使用者的需求，例如：商業寫作、程式碼除錯等，AI助理能根據文字指令即時調整模型，提供更精準的協助。", "遊戲AI角色客製化：遊戲開發者可以透過文字描述，快速調整AI角色的行為模式或對話風格，讓遊戲體驗更加豐富。"], "pitch": "各位投資人，想像一下，未來每個人都能輕鬆客製化AI模型，就像調整手機App一樣簡單！Text-to-LoRA技術，正是實現這個願景的關鍵。它能讓AI模型根據文字指令即時調整，無需耗時費力的重新訓練。這意味著，我們能以極低的成本，打造出無數個針對特定領域或個人需求的AI應用。從AI繪圖、個人化助理到遊戲AI，市場潛力無限。更重要的是，T2L技術還能壓縮模型，讓AI應用在各種裝置上都能流暢運行。我們相信，Text-to-LoRA將徹底顛覆AI產業，成為新一代AI應用的基礎設施。現在加入我們，一起打造AI客製化的未來！", "audio": "docs/data/audios/2506.06105v1.wav"}
{"query": "Diffusion Model", "id": "2506.06023v1", "url": "http://arxiv.org/abs/2506.06023v1", "title": "Restereo: Diffusion stereo video generation and restoration", "summary": "Stereo video generation has been gaining increasing attention with recent\nadvancements in video diffusion models. However, most existing methods focus on\ngenerating 3D stereoscopic videos from monocular 2D videos. These approaches\ntypically assume that the input monocular video is of high quality, making the\ntask primarily about inpainting occluded regions in the warped video while\npreserving disoccluded areas. In this paper, we introduce a new pipeline that\nnot only generates stereo videos but also enhances both left-view and\nright-view videos consistently with a single model. Our approach achieves this\nby fine-tuning the model on degraded data for restoration, as well as\nconditioning the model on warped masks for consistent stereo generation. As a\nresult, our method can be fine-tuned on a relatively small synthetic stereo\nvideo datasets and applied to low-quality real-world videos, performing both\nstereo video generation and restoration. Experiments demonstrate that our\nmethod outperforms existing approaches both qualitatively and quantitatively in\nstereo video generation from low-resolution inputs.", "authors": ["Xingchang Huang", "Ashish Kumar Singh", "Florian Dubost", "Cristina Nader Vasconcelos", "Sakar Khattar", "Liang Shi", "Christian Theobalt", "Cengiz Oztireli", "Gurprit Singh"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:32:05.943203", "title_zh": "Restereo：擴散立體影片生成與修復", "summary_zh": "本研究提出一個新穎的立體影片生成流程，不僅能從單眼2D影片生成3D立體影片，還能同時增強左右視角的影片品質。此方法透過在降質數據上微調模型進行修復，並以扭曲遮罩為條件進行一致的立體生成。因此，即使在相對較小的合成立體影片數據集上進行微調，也能應用於低品質的真實世界影片，同時實現立體影片的生成和修復。實驗結果表明，本方法在低解析度輸入的立體影片生成方面，在品質和數量上均優於現有方法。", "applications": ["在家用VR觀影時，即使影片來源畫質不佳，也能透過此技術即時提升畫質並轉換為立體3D，享受更沉浸式的觀影體驗。", "老舊照片或影片的數位修復：將舊照片或影片轉換為立體影像，讓回憶更加生動，並修復畫質，讓珍貴的影像資料得以保存。", "線上遊戲體驗優化：即時將2D遊戲畫面轉換為3D立體畫面，提升遊戲沉浸感，並修復遊戲畫面中可能存在的模糊或失真問題。"], "pitch": "各位創投先進，我們帶來的是Restereo，一項劃時代的立體影片生成與修復技術。想像一下，現今VR/AR內容的最大瓶頸是什麼？是高品質3D內容的匱乏！Restereo能將任何2D影片，甚至是低畫質的老舊影片，即時轉換為令人驚豔的3D立體影像，並同步提升畫質。這代表什麼？龐大的內容創作潛力！從個人用戶到大型影視公司，都能輕易創造出引人入勝的VR/AR體驗。更重要的是，我們能賦予歷史影像新的生命力，將塵封的記憶以更真實、更立體的方式呈現。未來，Restereo將成為元宇宙內容生態的基石，我們不只是在修復影片，我們是在打造一個全新的視覺世界！現在投資Restereo，就是投資元宇宙的未來！", "audio": "docs/data/audios/2506.06023v1.wav"}
{"query": "AI", "id": "2506.06214v1", "url": "http://arxiv.org/abs/2506.06214v1", "title": "Can Theoretical Physics Research Benefit from Language Agents?", "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.", "authors": ["Sirui Lu", "Zhijing Jin", "Terry Jingchen Zhang", "Pavel Kos", "J. Ignacio Cirac", "Bernhard Schölkopf"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:35:20.869608", "title_zh": "理論物理研究能否受益於語言智能體？", "summary_zh": "大型語言模型(LLM)在各領域快速發展，但在理論物理研究中的應用尚不成熟。本文認為，若將LLM智能體與領域知識和工具箱適當結合，有潛力加速理論、計算和應用物理學的發展。我們分析了LLM目前在物理學方面的能力，包括數學推理和程式碼生成，並指出了在物理直覺、約束滿足和可靠推理方面的關鍵差距。我們設想未來專門用於物理學的LLM能夠處理多模態數據、提出可驗證的假設並設計實驗。實現這一願景需要應對根本挑戰：確保物理一致性，並開發穩健的驗證方法。我們呼籲物理學界和人工智慧社群共同努力，以幫助推進物理學的科學發現。", "applications": ["**智慧教材：** LLM能根據學生的學習進度和理解程度，客製化物理教材和練習題，就像一位24小時隨時待命的私人物理家教。", "**科學玩具：** LLM可以嵌入到玩具中，讓孩子在玩樂中學習物理知識，例如，一個能回答物理問題的積木或一個能模擬物理現象的遊戲。", "**故障排除：** LLM可以協助工程師快速診斷複雜系統的故障，例如，分析感測器數據，找出發電廠或飛機引擎的潛在問題。"], "pitch": "各位投資人，我們正處於AI與物理學交匯的革命性時刻！想像一下，一個能自主設計實驗、推導新理論的AI科學家，這不再是科幻小說。我們的團隊正在開發專為物理學打造的LLM智能體，它能處理複雜的物理數據，提出創新的解決方案，並加速科學發現的進程。這項技術的潛在商業價值難以估量，從新材料的發現到能源效率的突破，再到太空探索的加速，都將受益於此。我們預計，未來物理學LLM將成為科研機構、工程公司和政府部門不可或缺的工具。現在投資我們，您將站在這場科學革命的最前沿，共同塑造未來！", "audio": "docs/data/audios/2506.06214v1.wav"}
{"query": "Foundation Model", "id": "2506.06076v1", "url": "http://arxiv.org/abs/2506.06076v1", "title": "Full Conformal Adaptation of Medical Vision-Language Models", "summary": "Vision-language models (VLMs) pre-trained at large scale have shown\nunprecedented transferability capabilities and are being progressively\nintegrated into medical image analysis. Although its discriminative potential\nhas been widely explored, its reliability aspect remains overlooked. This work\ninvestigates their behavior under the increasingly popular split conformal\nprediction (SCP) framework, which theoretically guarantees a given error level\non output sets by leveraging a labeled calibration set. However, the zero-shot\nperformance of VLMs is inherently limited, and common practice involves\nfew-shot transfer learning pipelines, which cannot absorb the rigid\nexchangeability assumptions of SCP. To alleviate this issue, we propose full\nconformal adaptation, a novel setting for jointly adapting and conformalizing\npre-trained foundation models, which operates transductively over each test\ndata point using a few-shot adaptation set. Moreover, we complement this\nframework with SS-Text, a novel training-free linear probe solver for VLMs that\nalleviates the computational cost of such a transductive approach. We provide\ncomprehensive experiments using 3 different modality-specialized medical VLMs\nand 9 adaptation tasks. Our framework requires exactly the same data as SCP,\nand provides consistent relative improvements of up to 27% on set efficiency\nwhile maintaining the same coverage guarantees.", "authors": ["Julio Silva-Rodríguez", "Leo Fillioux", "Paul-Henry Cournède", "Maria Vakalopoulou", "Stergios Christodoulidis", "Ismail Ben Ayed", "Jose Dolz"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:36:57.027212", "title_zh": "醫學視覺語言模型之完全適形調整", "summary_zh": "大型預訓練的視覺語言模型（VLMs）在醫學影像分析中展現了前所未有的遷移能力。然而，其可靠性卻被忽略。本研究探討了在split conformal prediction (SCP)框架下VLMs的行為，該框架藉由標記的校準集，在輸出集上保證給定的錯誤水平。為了解決VLMs的zero-shot性能限制以及few-shot遷移學習管道無法滿足SCP的嚴格可交換性假設的問題，我們提出了完全適形調整，這是一種新穎的設定，用於聯合調整和適形預訓練的基礎模型，並使用few-shot調整集對每個測試數據點進行轉導操作。此外，我們使用SS-Text來補充這個框架，這是一種用於VLMs的免訓練線性探測求解器，可減輕這種轉導方法的計算成本。實驗結果表明，我們的框架在保持相同覆蓋率保證的同時，在集合效率上提供了高達27%的相對改進。", "applications": ["**遠距醫療影像判讀：** 想像一下，偏鄉地區的醫生可以透過手機App，將X光片上傳，AI就能快速提供初步診斷結果，協助醫生做出更精確的判斷，提升醫療效率。", "**個人化健康管理：** 未來，我們可以將自己的醫療影像，例如心電圖、眼底照片等，上傳到一個安全平台，AI會分析這些數據，並提供個人化的健康建議，例如飲食調整、運動計畫等。", "**新藥開發加速：** 藥廠可以利用這項技術，快速分析大量的醫學影像資料，找出潛在的藥物靶點，加速新藥開發的進程，讓更多疾病得到及時治療。"], "pitch": "各位投資人，我們帶來的是醫學影像AI的革命性突破！傳統AI在醫學影像判讀上，準確度參差不齊，醫生往往不敢完全信任。我們的「完全適形調整」技術，能讓AI在判讀醫學影像時，不僅給出結果，還能提供信賴度評估，讓醫生更安心。想像一下，這項技術能大幅降低誤診率，提升醫療品質，減少醫療糾紛。更重要的是，它能解放醫生的時間，讓他們能更專注於病人護理。市場潛力巨大！從遠距醫療、個人化健康管理，到新藥開發，都有廣闊的應用前景。我們預期，在未來五年內，這項技術將成為醫學影像AI的產業標準，帶領我們在精準醫療時代搶佔先機。現在加入我們，您將成為這場醫療革命的領航者！", "audio": "docs/data/audios/2506.06076v1.wav"}
{"query": "Diffusion Model", "id": "2506.06018v1", "url": "http://arxiv.org/abs/2506.06018v1", "title": "Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models", "summary": "Watermarking becomes one of the pivotal solutions to trace and verify the\norigin of synthetic images generated by artificial intelligence models, but it\nis not free of risks. Recent studies demonstrate the capability to forge\nwatermarks from a target image onto cover images via adversarial optimization\nwithout knowledge of the target generative model and watermark schemes. In this\npaper, we uncover a greater risk of an optimization-free and universal\nwatermark forgery that harnesses existing regenerative diffusion models. Our\nproposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and\nintegrates the target watermark via regenerating the image, without needing any\nadditional optimization routine. It allows for universal watermark forgery that\nworks independently of the target image's origin or the watermarking model\nused. We explore the watermarked latent extracted from the target image and\nvisual-textual context of cover images as priors to guide sampling of the\nregenerative process. Extensive evaluation on 24 scenarios of\nmodel-data-watermark combinations demonstrates that PnP can successfully forge\nthe watermark (up to 100% detectability and user attribution), and maintain the\nbest visual perception. By bypassing model retraining and enabling adaptability\nto any image, our approach significantly broadens the scope of forgery attacks,\npresenting a greater challenge to the security of current watermarking\ntechniques for diffusion models and the authority of watermarking schemes in\nsynthetic data generation and governance.", "authors": ["Chaoyi Zhu", "Zaitang Li", "Renyi Yang", "Robert Birke", "Pin-Yu Chen", "Tsung-Yi Ho", "Lydia Y. Chen"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:38:37.267711", "title_zh": "基於再生擴散模型之免優化通用浮水印偽造", "summary_zh": "浮水印技術被廣泛應用於追蹤和驗證AI生成圖像的來源，但存在偽造風險。本研究揭示了一種更嚴重的免優化通用浮水印偽造方法，利用現有的再生擴散模型，名為PnP（Plug-and-Plant）。PnP無需額外優化，即可透過圖像再生無縫提取和整合目標浮水印。此方法獨立於目標圖像的來源或浮水印模型，實現通用浮水印偽造。實驗證明，PnP在多種情境下成功偽造浮水印，同時保持最佳視覺效果。這種繞過模型重新訓練並適應任何圖像的能力，擴大了偽造攻擊的範圍，對當前擴散模型的浮水印技術安全性和合成數據生成和治理中浮水印方案的權威性提出了更大的挑戰。", "applications": ["情境一：假設你是一位藝術家，想保護你的AI生成作品不被盜用。但有人利用這項技術，將你的浮水印複製到其他圖像上，讓你難以證明原創性，甚至可能被誤認為抄襲者。", "情境二：新聞媒體使用AI生成圖片來輔助報導。如果有人惡意將浮水印偽造到假新聞圖片上，並嫁禍給該媒體，可能嚴重損害其聲譽和公信力。", "情境三：在學術界，研究人員發表基於AI生成數據的論文。如果他人偽造浮水印，聲稱該數據來自不同的來源，可能導致學術欺詐和錯誤的研究結論。"], "pitch": "各位創投朋友們，想像一下，AI生成的內容正以前所未有的速度爆發，但信任危機也隨之而來。我們的技術揭示了現有浮水印系統的重大漏洞，同時也帶來了巨大的商機！PnP技術不僅能檢測偽造的浮水印，更能進一步開發出更強大、更安全的浮水印系統，保護原創內容，維護數據的真實性。未來，我們可以將這項技術應用於數位版權管理、內容溯源、甚至金融安全等領域。試想一下，每一張AI生成的圖片、每一份重要的數據報告，都擁有一個無法偽造的數位身份證，這將徹底改變我們對數位內容的信任方式。現在投資我們，您將站在AI安全的最前沿，共同打造一個更值得信賴的AI未來！", "audio": "docs/data/audios/2506.06018v1.wav"}
{"query": "AI", "id": "2506.06166v1", "url": "http://arxiv.org/abs/2506.06166v1", "title": "The Lock-in Hypothesis: Stagnation by Algorithm", "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com", "authors": ["Tianyi Alex Qiu", "Zhonghao He", "Tejasveer Chugh", "Max Kleiman-Weiner"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:25:05.955123", "title_zh": "鎖定假說：演算法造成的停滯", "summary_zh": "大型語言模型（LLM）的訓練和部署，會與使用者形成一種回饋迴路：模型從數據中學習人類的信念，透過生成內容強化這些信念，再吸收這些被強化的信念，然後反覆地回饋給使用者。這種動態類似於同溫層效應。我們假設這種回饋迴路會鞏固使用者現有的價值觀和信念，導致多樣性的喪失，並可能鎖定錯誤的信念。我們透過基於代理的LLM模擬和真實世界的GPT使用數據，對此假設進行了形式化並進行了實證檢驗。分析顯示，在新的GPT版本發布後，多樣性出現了突然但持續的下降，這與假設的人機回饋迴路一致。", "applications": ["新聞App總是推播你喜歡的新聞，讓你覺得世界就是你想的那樣，忽略了其他不同的聲音，長期下來，你可能變得更偏激。", "社群媒體的演算法只推薦你追蹤與你意見相似的人，讓你越來越難接觸到不同的觀點，導致同溫層效應越來越嚴重。", "孩子使用AI學習工具，但AI只根據過去的資料生成答案，可能讓孩子學到過時或有偏見的知識，阻礙他們的創新能力。"], "pitch": "各位創投先進，我們正處於AI革命的關鍵時刻，但一個潛在的危機正在浮現：AI正在將我們鎖死在過去的認知中。想像一下，如果未來的AI只能重複過去的觀點，創新將停滯，社會將分裂。我們的研究揭示了這個『鎖定假說』，並提供了應對方案。我們正在開發一種『AI多樣性引擎』，它能主動引入不同的觀點，打破同溫層效應，確保AI成為促進進步的力量，而不是阻礙。這不僅是一項技術，更是一項社會責任。投資我們，就是投資一個更開放、更具創新力的未來。我們預期在三年內，這項技術將成為所有大型語言模型的標準配置，並在教育、媒體、政策制定等領域產生深遠影響。未來的AI，不應該只是過去的鏡子，而應該是通往新世界的窗戶。加入我們，一起開啟這扇窗！", "audio": "docs/data/audios/2506.06166v1.wav"}
{"query": "Foundation Model", "id": "2506.06006v1", "url": "http://arxiv.org/abs/2506.06006v1", "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models", "summary": "To what extent do vision-and-language foundation models possess a realistic\nworld model (observation $\\times$ action $\\rightarrow$ observation) and a\ndynamics model (observation $\\times$ observation $\\rightarrow$ action), when\nactions are expressed through language? While open-source foundation models\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\nthrough supervision is significantly easier than acquiring a world model. In\nturn, dynamics models can be used to bootstrap world models through two main\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\ntime verification. Firstly, the dynamics model can annotate actions for\nunlabelled pairs of video frame observations to expand the training data. We\nfurther propose a new objective, where image tokens in observation pairs are\nweighted by their importance, as predicted by a recognition model. Secondly,\nthe dynamics models can assign rewards to multiple samples of the world model\nto score them, effectively guiding search at inference time. We evaluate the\nworld models resulting from both strategies through the task of action-centric\nimage editing on Aurora-Bench. Our best model achieves a performance\ncompetitive with state-of-the-art image editing models, improving on them by a\nmargin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and\nachieving the best average human evaluation across all subsets of Aurora-Bench.", "authors": ["Yifu Qiu", "Yftah Ziser", "Anna Korhonen", "Shay B. Cohen", "Edoardo M. Ponti"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:26:30.464573", "title_zh": "從多模態基礎模型中的動力學模型引導世界模型", "summary_zh": "本研究探討視覺與語言基礎模型是否具備真實的世界模型（觀察×行動→觀察）和動力學模型（觀察×觀察→行動）。研究發現，微調模型以獲得動力學模型比獲得世界模型更容易。進而，動力學模型可以透過合成數據的弱監督學習和推理時驗證來引導世界模型。首先，動力學模型可以為未標記的影片幀觀察對添加行動標籤，擴展訓練數據。其次，動力學模型可以為世界模型的多個樣本分配獎勵，對其進行評分，從而在推理時有效地引導搜尋。實驗結果顯示，該模型在Aurora-Bench上進行以行動為中心的圖像編輯任務時，性能與最先進的圖像編輯模型相媲美，在真實世界子集上的表現提高了15%。", "applications": ["想像一下，你可以用一句話，例如「把房間變成充滿陽光的沙灘」，然後這個AI就能自動幫你修改照片，讓你的房間看起來就像真的在沙灘上！這就像擁有了魔法PS高手。", "以後玩遊戲，AI能更聰明地理解你的指令。例如，你說「跳到最高的平台上」，AI就能預測你的角色需要如何移動和跳躍，讓遊戲體驗更流暢、更真實。", "在製造業，我們可以透過AI預測機器在不同操作下的反應。例如，輸入「提高機器速度」，AI就能預測機器零件的磨損情況，提前預防故障，降低維修成本。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能讓機器像人類一樣理解世界，並根據指令改變現實！我們的核心突破在於，我們發現了從動力學模型引導世界模型的有效方法，這讓AI能更準確地預測行動的後果。這項技術的潛力無窮，從圖像編輯、遊戲開發到工業自動化，都能帶來顛覆性的變革。想像一下，一個能根據你的想法創造圖像、控制機器人的AI，這將是一個數十億美元的市場！我們已經在Aurora-Bench基準測試中取得了令人矚目的成果，超越了現有的圖像編輯模型。現在，我們需要您的資金，將這項技術推向市場，成為AI領域的領導者。投資我們，就是投資未來！未來，每個人都可以是創作者，都可以用簡單的語言改變世界！", "audio": "docs/data/audios/2506.06006v1.wav"}
{"query": "Diffusion Model", "id": "2506.05960v1", "url": "http://arxiv.org/abs/2506.05960v1", "title": "AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models", "summary": "Significant investments have been made towards the commodification of\ndiffusion models for generation of diverse media. Their mass-market adoption is\nhowever still hobbled by the intense hardware resource requirements of\ndiffusion model inference. Model quantization strategies tailored specifically\ntowards diffusion models have been useful in easing this burden, yet have\ngenerally explored the Uniform Scalar Quantization (USQ) family of quantization\nmethods. In contrast, Vector Quantization (VQ) methods, which operate on groups\nof multiple related weights as the basic unit of compression, have seen\nsubstantial success in Large Language Model (LLM) quantization. In this work,\nwe apply codebook-based additive vector quantization to the problem of\ndiffusion model compression. Our resulting approach achieves a new Pareto\nfrontier for the extremely low-bit weight quantization on the standard\nclass-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.\nNotably, we report sFID 1.92 points lower than the full-precision model at W4A8\nand the best-reported results for FID, sFID and ISC at W2A8. We are also able\nto demonstrate FLOPs savings on arbitrary hardware via an efficient inference\nkernel, as opposed to savings resulting from small integer operations which may\nlack broad hardware support.", "authors": ["Adil Hasan", "Thomas Peyrin"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:27:56.686311", "title_zh": "AQUATIC-Diff：適用於極小壓縮擴散模型的加法量化", "summary_zh": "本研究針對擴散模型在硬體資源上的高需求問題，提出了一種名為AQUATIC-Diff的加法向量量化方法。不同於以往常用的均勻標量量化，此方法基於碼本，能更有效地壓縮模型，在極低位元量化下達到新的效能巔峰。在ImageNet的LDM-4基準測試中，W4A8設定下sFID值比全精度模型低1.92點，W2A8設定下FID、sFID和ISC指標均達到最佳。更重要的是，我們開發了高效的推論核心，能在各種硬體上實現FLOPs節省，擺脫了對特定硬體支援小整數運算的依賴。", "applications": ["**手機攝影美化：** 將這項技術應用於手機App中，即使是低階手機也能快速生成高品質、風格獨特的照片，讓每個人都能輕鬆成為攝影大師。", "**遊戲角色生成：** 遊戲開發者可以利用這項技術，快速生成大量獨一無二的遊戲角色，節省美術設計時間，並提供玩家更多樣化的選擇。", "**AI藝術創作：** 藝術家可以使用這項技術，在資源有限的設備上進行AI藝術創作，激發無限創意，並將藝術帶入更多人的生活。"], "pitch": "各位創投先進，我們正站在AI圖像生成革命的浪潮之巔！AQUATIC-Diff技術，如同為擴散模型裝上了火箭推進器，使其能在極低的硬體資源下運行，打破了過往高算力需求的瓶頸。想像一下，未來每一台手機都能運行複雜的AI圖像生成模型，人人都能隨時隨地創造獨一無二的內容。這不僅僅是技術突破，更是商業模式的巨大變革！我們可以將此技術授權給手機廠商、遊戲公司、甚至是元宇宙平台，收取授權費用；或者開發基於AQUATIC-Diff的雲端服務，提供更高效、更低成本的AI圖像生成解決方案。隨著元宇宙、NFT等領域的蓬勃發展，對AI圖像生成的需求將呈指數級增長，AQUATIC-Diff必將成為這場盛宴中最耀眼的明星，為各位帶來豐厚的回報！現在投資AQUATIC-Diff，就是投資AI圖像生成的未來！", "audio": "docs/data/audios/2506.05960v1.wav"}
{"query": "AI", "id": "2506.08006v1", "url": "http://arxiv.org/abs/2506.08006v1", "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:53:33.383616", "title_zh": "夢境樂園：結合模擬器與生成模型的可控世界創造", "summary_zh": "本研究提出「夢境樂園」，一個結合物理模擬器和生成模型的混合世界生成框架。它利用分層世界抽象，將像素級和物件級的語義與幾何資訊編碼為中間表示，連接模擬器和生成模型。這增強了可控性，透過與真實世界分佈的早期對齊，降低了適應成本，並支援現有和未來預訓練生成模型的直接使用。我們構建了D3Sim數據集，以促進混合生成管道的訓練和評估。實驗表明，「夢境樂園」在圖像質量上提升了50.8%，可控性增強了17.9%，並具有增強具身智能體訓練的巨大潛力。", "applications": ["遊戲開發者可以利用這項技術快速創建多樣且逼真的遊戲世界，並精確控制場景中的元素，例如調整物體的物理特性或改變環境光照，讓遊戲體驗更豐富。", "建築師和設計師可以創建虛擬的建築模型，並模擬不同天氣或光照條件下的效果，讓客戶在實際建造前就能身歷其境地體驗設計方案。", "電影製作人可以使用這項技術製作特效場景，例如創建逼真的自然災害或科幻世界，並精確控制場景中的每個細節，降低製作成本並提高效率。"], "pitch": "想像一下，我們正站在一個無限可能的起點。Dreamland不僅僅是一個技術突破，它是一個通往全新現實的鑰匙。它將徹底改變遊戲、娛樂、設計乃至AI訓練的未來。我們的混合框架，結合了物理模擬的精確控制與生成模型的逼真渲染，創造出前所未有的可控虛擬世界。這意味著更高效的遊戲開發、更具沉浸感的虛擬體驗，以及更強大的AI智能體。D3Sim數據集是我們的獨家優勢，能加速AI學習並提升性能。市場潜力巨大：遊戲產業對逼真場景的需求、建築設計對可視化效果的追求、AI訓練對大量數據的渴求，都將推動Dreamland的快速成長。我們正在打造的不僅是一個產品，而是一個平台，一個生態系統，一個全新的現實。現在加入我們，一起塑造這個未來，共享這份巨大的商業價值！", "audio": "docs/data/audios/2506.08006v1.wav"}
{"query": "Foundation Model", "id": "2506.07940v1", "url": "http://arxiv.org/abs/2506.07940v1", "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation", "summary": "Foundation model fine-tuning faces a fundamental challenge: existing AutoML\nplatforms rely on single optimisation strategies that explore only a fraction\nof viable hyperparameter configurations. In this white paper, We introduce\nGradients, a decentralised AutoML platform that transforms hyperparameter\noptimisation into a competitive marketplace where independent miners compete to\ndiscover optimal configurations. Economic incentives align individual\nexploration with collective optimisation goals, driving systematic\ninvestigation of hyperparameter regions that centralised methods miss. We\nevaluate our approach across 180 controlled experiments spanning diverse model\narchitectures (70M to 70B parameters) and task types. Gradients achieves an\n82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI,\nDatabricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\%\nrespectively. Complex reasoning and retrieval tasks show particularly strong\ngains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for\nperson-specific generation. These results demonstrate that competitive,\neconomically-driven approaches can systematically discover superior\nconfigurations that centralised AutoML consistently miss.", "authors": ["Christopher Subia-Waud"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:54:57.509488", "title_zh": "梯度：當市場遇上微調——一種模型優化的分散式方法", "summary_zh": "現有自動機器學習平台在微調大型模型時，往往受限於單一優化策略，無法充分探索所有可能的超參數組合。Gradients平台將超參數優化轉變為一個去中心化的競爭市場，讓獨立的「礦工」競相尋找最佳配置。經濟誘因驅動個人探索，並將其與集體優化目標對齊，從而系統性地挖掘中心化方法遺漏的超參數區域。實驗結果顯示，Gradients在多種模型架構和任務類型中，相較於其他平台，平均提升了11.8%至42.1%的性能，尤其在複雜推理和檢索任務以及個人化生成方面表現出色。這證明了基於經濟驅動的競爭方法，能有效發現卓越的配置。", "applications": ["想像一下，你是一位行銷人員，想為你的產品創建最吸引人的廣告文案。Gradients就像一個超級優化的廣告文案產生器，能自動找到最有效的詞語和風格，讓你的廣告點擊率飆升。", "如果你是一位醫生，想利用AI診斷罕見疾病。Gradients可以幫助你快速微調AI模型，使其能更準確地識別出疾病的細微特徵，提高診斷的準確性。", "假設你是一位遊戲開發者，想創造一個能根據玩家喜好自動調整難度的遊戲。Gradients可以幫助你優化遊戲AI，讓每個玩家都能享受到獨一無二、高度個人化的遊戲體驗。"], "pitch": "各位投資人，我們相信Gradients將徹底改變AI模型的微調方式。現今，微調過程耗時且昂貴，如同大海撈針。Gradients透過去中心化的市場機制，將這個過程轉變為高效、經濟的競賽。想像一下，一個能自我優化的AI生態系統，就像AI界的App Store，每天都在產生更強大、更精準的模型。這不僅能節省數百萬美元的成本，更能加速AI在各行各業的應用。我們預見，Gradients將成為AI基礎設施的關鍵組成部分，為各行各業提供更強大、更個人化的AI解決方案。投資Gradients，就是投資AI的未來，一個充滿無限可能的未來！", "audio": "docs/data/audios/2506.07940v1.wav"}
{"query": "Diffusion Model", "id": "2506.08013v1", "url": "http://arxiv.org/abs/2506.08013v1", "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:56:34.805569", "title_zh": "StableMTL：利用潛在擴散模型，從部分標註的合成數據集中進行多任務學習", "summary_zh": "這項研究提出StableMTL方法，利用擴散模型強大的泛化能力，在只有部分標註的合成數據集上訓練多任務模型，實現零樣本學習。StableMTL將圖像生成器用於潛在回歸，通過任務編碼、逐任務條件化和定制的訓練方案來調整去噪框架。它採用統一的潛在損失，無需仔細平衡各任務的損失，從而實現無縫擴展到更多任務。此外，引入了多流模型和任務注意力機制，將任務間的交互轉化為高效的單向注意力，促進跨任務共享。實驗證明，StableMTL在多個基準測試中優於其他方法。", "applications": ["智慧城市：利用路口監視器畫面，同時辨識車流量、行人數量、違規停車等，提升交通管理效率，並減少人力成本。", "醫療影像分析：從X光片或斷層掃描中，同時檢測多種疾病徵兆，例如腫瘤大小、骨折位置、炎症反應等，輔助醫生進行更精確的診斷。", "電商平台：自動分析商品圖片，同時提取商品屬性（顏色、材質、款式）和場景信息（室內、戶外），提升商品分類和搜尋的準確性，改善使用者體驗。"], "pitch": "想像一下，我們能用AI同時處理多項任務，而且只需要少量標註數據甚至完全不需要！StableMTL就是實現這個願景的關鍵。它像一個超級AI訓練師，能從各種模擬數據中學習，並將知識應用到真實世界。這不僅能大幅降低AI開發成本，還能開啟無限可能。例如，在自動駕駛領域，我們可以同時訓練AI識別交通號誌、行人、障礙物，大幅提升安全性。在醫療診斷方面，AI能同時分析多種病徵，協助醫生做出更精準的判斷。這項技術的潛在市場價值數十億美元，現在投資，就能搶佔AI多任務學習的先機，成為下一個AI獨角獸！", "audio": "docs/data/audios/2506.08013v1.wav"}
{"query": "AI", "id": "2506.07997v1", "url": "http://arxiv.org/abs/2506.07997v1", "title": "Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System", "summary": "The construction industry is characterized by both high physical and\npsychological risks, yet supports of mental health remain limited. While\nadvancements in artificial intelligence (AI), particularly large language\nmodels (LLMs), offer promising solutions, their potential in construction\nremains largely underexplored. To bridge this gap, we developed a\nconversational multi-agent system that addresses industry-specific challenges\nthrough an AI-driven approach integrated with domain knowledge. In parallel, it\nfulfills construction workers' basic psychological needs by enabling\ninteractions with multiple agents, each has a distinct persona. This approach\nensures that workers receive both practical problem-solving support and social\nengagement, ultimately contributing to their overall well-being. We evaluate\nits usability and effectiveness through a within-subjects user study with 12\nparticipants. The results show that our system significantly outperforms the\nsingle-agent baseline, achieving improvements of 18% in usability, 40% in\nself-determination, 60% in social presence, and 60% in trust. These findings\nhighlight the promise of LLM-driven AI systems in providing domain-specific\nsupport for construction workers.", "authors": ["Fan Yang", "Yuan Tian", "Jiansong Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:37:06.451208", "title_zh": "利用多代理人對話式AI系統支持建築工人的福祉", "summary_zh": "建築業面臨高 शारीरिक 與心理風險，但心理健康支持有限。本研究開發了一套多代理人對話式AI系統，結合領域知識，解決建築業的特定挑戰。系統透過與不同人格的代理人互動，滿足工人基本的心理需求，提供實際問題解決方案與社交互動，從而提升整體福祉。實驗結果顯示，相較於單一代理人系統，我們的系統在可用性、自主性、社交臨場感與信任度方面分別提升了18%、40%、60%與60%。這證明了大型語言模型驅動的AI系統在為建築工人提供領域特定支持方面的潛力。", "applications": ["工地裡，工人阿明心情不好，可以跟AI心理諮詢師聊聊，排解壓力，AI還能提醒他注意安全，避免工傷。", "老王是個水電工，遇到複雜的管線問題，可以問AI專家，AI會一步一步教他怎麼解決，省去查資料的時間。", "新來的工頭小李，對很多建材和工法不熟悉，可以隨時問AI老師，AI會提供相關知識和案例，幫助他快速上手。"], "pitch": "各位投資人，建築業長期面臨人力短缺、工安意外頻傳等問題，而我們的多代理人對話式AI系統，正是解決這些痛點的關鍵。它不僅能提升工人的心理健康與工作效率，更能降低工安事故的發生。想像一下，未來每個工地都配備這樣一套AI系統，它就像一位隨時待命的超級顧問，為工人提供全方位的支持。這將大幅提升建築業的生產力與安全性，創造巨大的商業價值。我們預計，這項技術將能應用於其他高風險行業，例如礦業、製造業等，市場潛力無限。現在投資我們，您將成為引領建築業AI革命的先驅！", "audio": "docs/data/audios/2506.07997v1.wav"}
{"query": "Foundation Model", "id": "2506.07886v1", "url": "http://arxiv.org/abs/2506.07886v1", "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:38:16.837702", "title_zh": "EgoM2P：以自我為中心的視角進行多模態多任務預訓練", "summary_zh": "本研究提出EgoM2P框架，旨在解決以自我為中心的視角下，如何有效理解多模態訊號的挑戰。EgoM2P利用時序感知的多模態tokens，透過遮蔽建模學習，訓練出一個通用的4D理解模型。此模型支援多種任務，包括眼球追蹤、以自我為中心的相機追蹤，以及從單眼視訊進行深度估計，甚至可以生成條件式的以自我為中心的視訊。EgoM2P在多項任務上達到或超越了專用模型的效果，且速度更快。我們將開源EgoM2P，以促進以自我為中心的視覺研究。", "applications": ["導航輔助：想像一下，戴上AR眼鏡，系統能根據你的視線和頭部動作，預測你的意圖，並在視野中即時顯示導航資訊，再也不用低頭看手機了。", "運動訓練：運動員佩戴設備後，系統能分析他們的動作、視線焦點和身體姿態，提供個人化的訓練建議，幫助他們提升表現，例如高爾夫揮桿或籃球投籃。", "遠端協作：工程師可以戴上頭戴式裝置，讓遠端的專家看到他們所看到的，並透過即時的視線追蹤和手勢識別，進行更有效的遠端指導和協作，減少錯誤和提高效率。"], "pitch": "各位投資人，我們正站在AIoT革命的風口浪尖！EgoM2P不僅僅是一個模型，它是一把解鎖未來人機互動的鑰匙。試想，透過我們的技術，AR/VR設備將變得更加智慧、更具沉浸感；機器人將能更精準地理解人類意圖，協作更加順暢；醫療領域，醫生可以透過AR眼鏡進行遠端手術指導，提升醫療水平。EgoM2P的潛力遠不止於此，它將成為元宇宙、智慧工廠、無人駕駛等領域的核心技術。我們正在打造一個全新的互動模式，一個以人為本的智慧世界。現在加入我們，共同開創這個千億級市場！", "audio": "docs/data/audios/2506.07886v1.wav"}
{"query": "Diffusion Model", "id": "2506.08009v1", "url": "http://arxiv.org/abs/2506.08009v1", "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:39:36.544597", "title_zh": "自我強制：彌合自迴歸影片擴散中的訓練-測試差距", "summary_zh": "本研究提出「自我強制」訓練方法，解決自迴歸影片擴散模型中長期存在的暴露偏差問題。傳統模型在訓練時依賴真實資料，但在實際應用時卻需根據自身產生的不完美結果生成影片。自我強制透過在訓練期間使用關鍵值（KV）快取進行自迴歸展開，讓模型根據先前自身生成的輸出產生每一幀，從而在影片層級進行整體性監督，直接評估整個生成序列的品質。此外，透過幾步擴散模型和隨機梯度截斷策略，兼顧計算成本和效能。實驗證明，此方法能在單一GPU上實現亞秒級延遲的即時串流影片生成，生成品質甚至超越速度較慢的非因果擴散模型。", "applications": ["想像一下，未來的線上遊戲！遊戲畫面不用事先算好，而是根據你的遊玩方式即時生成，每次玩都有獨一無二的體驗，就像真的身歷其境。", "假設你是個室內設計師，想讓客戶更快看到設計成果。現在只要輸入簡單的描述，就能即時生成不同風格的3D室內設計影片，快速溝通想法，大幅提升效率。", "如果醫院想用AI訓練醫生進行手術模擬，過去需要大量資源建立模型。現在利用這項技術，可以即時生成各種手術場景，讓醫生在逼真的環境下練習，提升手術成功率。"], "pitch": "各位投資人，我們正處於影片生成技術的革命性轉捩點！「自我強制」技術不僅解決了現有模型的瓶頸，更開創了即時、高品質影片生成的全新可能性。想像一下，未來影音內容的生產成本將大幅降低，個人化的互動式影片體驗將無處不在。從遊戲、娛樂、教育到醫療，各行各業都將因此受益。我們的技術擁有極高的商業價值，未來將能授權給各大影音平台、遊戲公司、教育機構，甚至能應用於元宇宙的內容生成。我們預計未來五年內，影片生成市場規模將達到數百億美元，而「自我強制」技術將在這個市場中佔據領先地位，為各位投資人帶來豐厚的回報！現在加入我們，一起打造影片生成的未來！", "audio": "docs/data/audios/2506.08009v1.wav"}
{"query": "AI", "id": "2506.07982v1", "url": "http://arxiv.org/abs/2506.07982v1", "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:29:23.275744", "title_zh": "τ²-Bench：在雙重控制環境中評估對話式代理", "summary_zh": "現有對話式AI代理的評估基準主要模擬單一控制環境，AI代理能使用工具與世界互動，使用者則是被動提供資訊。本研究提出τ²-Bench，模擬電信領域的雙重控制環境，代理和使用者都能使用工具在共享環境中操作。τ²-Bench包含：Dec-POMDP模型、可程式化任務生成器、可靠的使用者模擬器，以及細緻的代理效能分析。實驗顯示，從無使用者到雙重控制環境，代理效能顯著下降，突顯了引導使用者的挑戰。τ²-Bench為測試代理的推理能力和引導使用者行為的能力提供了一個可控的平台。", "applications": ["想像一下，以後打電話給電信客服，AI不只會幫你查帳單，還能一步步引導你設定數據漫遊，就像朋友一樣教你操作手機。", "家裡的智慧家電壞了，AI客服會引導你檢查電源、重啟設備，甚至教你簡單的故障排除，不用再盲目等待維修人員。", "使用複雜的軟體時，AI助手會像導航一樣，一步步引導你完成任務，再也不用擔心找不到功能或操作錯誤。"], "pitch": "各位投資人，我們正處於AI客服的轉型期！傳統AI只能被動回答問題，而我們的τ²-Bench技術，讓AI具備了引導使用者操作的能力，這將徹底改變人機互動模式。想像一下，未來電信公司、家電廠商、軟體開發商，都需要這種能主動引導使用者的AI客服。我們的技術不僅提升了客服效率，更降低了人力成本，潛在市場規模數十億美元！更重要的是，這項技術是AI邁向更複雜、更實用應用的關鍵一步。我們相信，τ²-Bench將成為AI客服領域的黃金標準，現在投資，您將站在AI革命的最前沿！", "audio": "docs/data/audios/2506.07982v1.wav"}
{"query": "Foundation Model", "id": "2506.07740v1", "url": "http://arxiv.org/abs/2506.07740v1", "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:30:39.914350", "title_zh": "Flow-Anything：從大規模單視角圖像學習真實世界光流估計", "summary_zh": "光流估計是電腦視覺的關鍵技術，是許多影片任務的基礎。Flow-Anything 提出一個大規模數據生成框架，從真實世界任何單視角圖像中學習光流估計。首先，利用單眼深度估計網路將單視角圖像轉換為3D表示，以便在虛擬相機下渲染光流和新視角圖像。其次，開發物件獨立體積渲染模組和深度感知修復模組，對3D表示中的動態物件進行建模。由此生成的 FA-Flow 數據集，首次展示了從大規模真實世界圖像生成光流訓練數據的優勢，超越了最先進的無監督和合成數據集上的監督方法。該模型可作為基礎模型，提升各種下游影片任務的效能。", "applications": ["**自動駕駛安全升級：** 想像一下，汽車能更精準地判斷行人或車輛的移動速度和方向，即使在光線不足或視線不佳的情況下，也能提前預警，大幅降低事故風險。", "**運動賽事分析：** 運動員的動作捕捉和分析變得更精準。例如，教練可以利用這項技術，精確分析籃球運動員的投籃姿勢，找出細微的改進空間，提升投籃命中率。", "**影視特效製作：** 製作更逼真的特效，例如模擬人群移動、火焰燃燒等。以往需要大量人工調整，現在可以透過 AI 自動生成，大幅降低製作成本，提升製作效率。"], "pitch": "想像一下，我們正在打造的是下一代影片理解引擎的核心！Flow-Anything 不僅僅是一個光流估計模型，它是一個從真實世界學習的強大基礎模型。這意味著，我們能夠以前所未有的精度理解、預測和生成影片內容。自動駕駛、智慧安防、運動分析、影視特效… 這些只是冰山一角！更令人興奮的是，我們正在探索將這項技術應用於元宇宙的內容生成，讓每個人都能輕鬆創造出逼真的虛擬世界。隨著影片數據爆炸式增長，對影片理解的需求將會持續攀升。Flow-Anything 將成為這場變革的關鍵推動力，潛在市場規模將達到數十億美元！現在加入我們，一起打造影片理解的未來！", "audio": "docs/data/audios/2506.07740v1.wav"}
{"query": "Diffusion Model", "id": "2506.08004v1", "url": "http://arxiv.org/abs/2506.08004v1", "title": "Dynamic View Synthesis as an Inverse Problem", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:32:02.092672", "title_zh": "動態視角合成作為一個逆問題", "summary_zh": "本研究將單眼影片的動態視角合成視為一個逆問題，並在無需訓練的環境下解決。透過重新設計預訓練視訊擴散模型的雜訊初始化階段，我們實現了高保真度的動態視角合成，無需任何權重更新或輔助模組。我們首先發現了零終端信噪比（SNR）排程對確定性反演造成根本障礙，並通過引入一種新的雜訊表示，稱為K階遞迴雜訊表示，來解決這個問題。我們推導出這種表示的閉合形式表達式，從而實現了VAE編碼和DDIM反演潛在變數之間的精確高效對齊。為了合成由相機運動產生嘅新可見區域，我們引入了隨機潛在調製，它在潛在空間上執行可見性感知採樣，以完成遮擋區域。綜合實驗表明，動態視角合成可以通過雜訊初始化階段中的結構化潛在操作有效地執行。", "applications": ["線上遊戲：玩家可以從任意角度觀看遊戲角色，提供更自由的遊戲體驗。", "虛擬實境旅遊：使用者可以透過現有影片，自由探索拍攝地點的各個角度，彷彿身歷其境。", "電影特效：在沒有額外拍攝的情況下，從不同的角度呈現爆炸、撞擊等特效畫面。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它能從普通影片中創造出令人驚豔的3D視角體驗。想像一下，你只需要一段手機影片，就能生成一個完整的3D模型，從任何角度觀看，這就是我們的技術能做到的。這項技術的應用潛力無窮，從遊戲、電影到電商、房地產，都能帶來顛覆性的改變。例如，電商平台可以讓消費者360度無死角地檢視商品，大大提升購買意願；房地產公司可以打造沉浸式的線上看房體驗，讓潛在買家足不出戶就能感受到房屋的空間感。更重要的是，我們的技術無需昂貴的硬體設備或複雜的訓練過程，成本效益極高。我們相信，這項技術將引領下一代視覺體驗的浪潮，成為元宇宙和虛擬實境領域的關鍵基礎設施。現在加入我們，一起打造視覺革命的未來！", "audio": "docs/data/audios/2506.08004v1.wav"}
{"query": "AI", "id": "2506.07957v1", "url": "http://arxiv.org/abs/2506.07957v1", "title": "Understanding the Error Sensitivity of Privacy-Aware Computing", "summary": "Homomorphic Encryption (HE) enables secure computation on encrypted data\nwithout decryption, allowing a great opportunity for privacy-preserving\ncomputation. In particular, domains such as healthcare, finance, and\ngovernment, where data privacy and security are of utmost importance, can\nbenefit from HE by enabling third-party computation and services on sensitive\ndata. In other words, HE constitutes the \"Holy Grail\" of cryptography: data\nremains encrypted all the time, being protected while in use.\n  HE's security guarantees rely on noise added to data to make relatively\nsimple problems computationally intractable. This error-centric intrinsic HE\nmechanism generates new challenges related to the fault tolerance and\nrobustness of HE itself: hardware- and software-induced errors during HE\noperation can easily evade traditional error detection and correction\nmechanisms, resulting in silent data corruption (SDC).\n  In this work, we motivate a thorough discussion regarding the sensitivity of\nHE applications to bit faults and provide a detailed error characterization\nstudy of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes\ndue to its fixed-point arithmetic support for AI and machine learning\napplications. We also delve into the impact of the residue number system (RNS)\nand the number theoretic transform (NTT), two widely adopted HE optimization\ntechniques, on CKKS' error sensitivity. To the best of our knowledge, this is\nthe first work that looks into the robustness and error sensitivity of\nhomomorphic encryption and, as such, it can pave the way for critical future\nwork in this area.", "authors": ["Matías Mazzanti", "Esteban Mocskos", "Augusto Vega", "Pradip Bose"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:53:58.454764", "title_zh": "理解隱私感知運算的錯誤敏感性", "summary_zh": "同態加密（HE）允許在加密數據上進行安全運算，無需解密，為保護隱私的運算提供了絕佳機會。醫療、金融和政府等對數據隱私和安全極為重要的領域，可以透過HE在敏感數據上實現第三方運算和服務。HE的安全性依賴於添加到數據中的雜訊，使相對簡單的問題在計算上變得難以處理。然而，這種以錯誤為中心的機制也帶來了新的挑戰：HE運算期間的硬體和軟體錯誤容易避開傳統的錯誤檢測和校正機制，導致靜默數據損壞（SDC）。本研究深入探討HE應用對位元錯誤的敏感性，並詳細分析了CKKS方案的錯誤特性，同時也研究了餘數系統（RNS）和數論變換（NTT）對CKKS錯誤敏感性的影響。這項研究首次關注同態加密的穩健性和錯誤敏感性，為該領域的未來重要研究奠定了基礎。", "applications": ["想像一下，醫院可以利用這項技術，在不洩露病人隱私的情況下，讓AI分析病人的病歷，找出潛在的疾病風險，及早預防。", "銀行可以使用這項技術，在加密的狀態下，分析客戶的交易數據，判斷是否有詐欺行為，同時保護客戶的財務隱私。", "政府可以利用這項技術，在不公開個人資訊的情況下，統計人口數據，制定更完善的公共政策。"], "pitch": "各位創投先進，我們正在打造的是數據安全的新紀元！同態加密技術，如同為數據穿上了一層隱形盔甲，讓數據在被使用的同時，依然保持加密狀態。試想一下，在醫療、金融、國防等領域，有多少數據因為安全疑慮而無法被充分利用？我們的研究，正是要提升同態加密的穩定性，讓這項技術真正落地應用。未來，我們將開發針對不同產業的同態加密解決方案，例如：AI模型可以在加密的醫療數據上進行訓練，藥廠可以在不洩露配方的情況下進行研發合作，甚至可以打造完全匿名的區塊鏈應用。這不僅僅是一項技術，更是一個龐大的市場，一個重新定義數據價值的機會！現在加入我們，一起開啟數據安全的新篇章！", "audio": "docs/data/audios/2506.07957v1.wav"}
{"query": "Foundation Model", "id": "2506.07647v1", "url": "http://arxiv.org/abs/2506.07647v1", "title": "Foundation Model Empowered Synesthesia of Machines (SoM): AI-native Intelligent Multi-Modal Sensing-Communication Integration", "summary": "To support future intelligent multifunctional sixth-generation (6G) wireless\ncommunication networks, Synesthesia of Machines (SoM) is proposed as a novel\nparadigm for artificial intelligence (AI)-native intelligent multi-modal\nsensing-communication integration. However, existing SoM system designs rely on\ntask-specific AI models and face challenges such as scarcity of massive\nhigh-quality datasets, constrained modeling capability, poor generalization,\nand limited universality. Recently, foundation models (FMs) have emerged as a\nnew deep learning paradigm and have been preliminarily applied to SoM-related\ntasks, but a systematic design framework is still lacking. In this paper, we\nfor the first time present a systematic categorization of FMs for SoM system\ndesign, dividing them into general-purpose FMs, specifically large language\nmodels (LLMs), and SoM domain-specific FMs, referred to as wireless foundation\nmodels. Furthermore, we derive key characteristics of FMs in addressing\nexisting challenges in SoM systems and propose two corresponding roadmaps,\ni.e., LLM-based and wireless foundation model-based design. For each roadmap,\nwe provide a framework containing key design steps as a guiding pipeline and\nseveral representative case studies of FM-empowered SoM system design.\nSpecifically, we propose LLM-based path loss generation (LLM4PG) and scatterer\ngeneration (LLM4SG) schemes, and wireless channel foundation model (WiCo) for\nSoM mechanism exploration, LLM-based wireless multi-task SoM transceiver\n(LLM4WM) and wireless foundation model (WiFo) for SoM-enhanced transceiver\ndesign, and wireless cooperative perception foundation model (WiPo) for\nSoM-enhanced cooperative perception, demonstrating the significant superiority\nof FMs over task-specific models. Finally, we summarize and highlight potential\ndirections for future research.", "authors": ["Xiang Cheng", "Boxun Liu", "Xuanyu Liu", "Ensong Liu", "Ziwei Huang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:55:21.591233", "title_zh": "基於基礎模型的機器聯覺(SoM)：AI原生智能多模態感知-通信集成", "summary_zh": "本研究提出基於基礎模型的機器聯覺(SoM)，旨在為未來的6G無線通信網絡提供AI原生的智能多模態感知與通信集成。現有SoM系統依賴於特定任務的AI模型，面臨數據稀缺、建模能力受限和泛化性差等挑戰。本研究系統性地對SoM系統設計的基礎模型進行分類，並提出基於大型語言模型(LLM)和無線基礎模型的兩種設計路線圖。通過案例研究，證明基礎模型在SoM機制探索、收發器設計和協同感知方面優於特定任務模型。這為未來無線通信的智能化開闢了新的方向。", "applications": ["想像一下，未來的無人機送貨更聰明！它們不僅能看到障礙物，還能『聽到』風的聲音，預測氣流變化，自動調整飛行路線，保證包裹安全準時送達。", "未來的智慧交通系統，車輛之間不僅能交換位置和速度信息，還能『感知』到前方道路的濕滑程度，提前提醒駕駛員減速，避免交通事故。", "在工廠裡，機器人不僅能看到零件的位置，還能『聽到』機器運轉的異常聲音，及早發現故障，避免生產線停工。"], "pitch": "各位投資人，我們正處於AI與無線通信融合的革命性時刻！傳統無線通信依賴人工設計，效率低下且難以適應複雜環境。我們的SoM技術，利用基礎模型，讓機器擁有『聯覺』能力，像人類一樣綜合運用多種感官信息，實現更智能、更可靠的無線通信。這不僅能大幅提升現有無線網絡的性能，更將催生全新的應用場景，例如：無人駕駛、智慧城市、工業互聯網等。試想一下，一個完全由AI驅動的無線世界，將帶來怎樣的巨大商業價值？我們的團隊擁有深厚的AI和無線通信背景，我們有信心將SoM技術打造成為下一代無線通信的基石，成為引領行業變革的領頭羊。現在加入我們，您將擁抱一個千億美元級別的市場，共同開啟AI賦能無線通信的黃金時代！", "audio": "docs/data/audios/2506.07647v1.wav"}
{"query": "Diffusion Model", "id": "2506.07999v1", "url": "http://arxiv.org/abs/2506.07999v1", "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:56:32.481289", "title_zh": "MADFormer：用於連續圖像生成的混合自迴歸與擴散轉換器", "summary_zh": "MADFormer結合了自迴歸（AR）和擴散模型，旨在優化圖像生成。它將圖像分成空間區塊，利用AR層進行全局條件設定，再用擴散層在各區塊內進行迭代局部細化。實驗證明，這種分區方式能有效提升高解析度圖像的生成品質，垂直混合AR和擴散層能在運算資源有限的情況下，顯著改善生成品質。MADFormer的研究為未來混合生成模型提供了實用的設計原則，展現了在效率和品質之間取得平衡的潛力。", "applications": ["客製化頭像生成：使用者可以輸入少量個人照片，系統就能生成風格多樣、高解析度的個人頭像，用於社交媒體或遊戲。", "老照片修復：將模糊或損壞的老照片透過AI修復，還原清晰細節，讓珍貴回憶重現。", "室內設計預覽：輸入房屋格局和個人喜好，AI就能生成不同風格的室內設計方案，讓使用者在裝修前預覽效果。"], "pitch": "各位創投夥伴，我們團隊帶來的是MADFormer，一項突破性的圖像生成技術，它巧妙結合了自迴歸和擴散模型，在高解析度圖像生成領域實現了質的飛躍。想像一下，未來遊戲、電影、廣告等產業，對高質量、客製化圖像的需求將會爆炸性增長。MADFormer能以更低的成本、更快的速度，生成以往難以想像的圖像內容。這不僅能大幅降低製作成本，更能激發無限的創意潛能。我們的技術已經在FFHQ-1024和ImageNet等數據集上驗證，效果顯著。我們預計，MADFormer將成為下一代圖像生成引擎的核心技術，佔領市場制高點。現在投資，您將成為這場圖像革命的早期參與者，共同分享百億美元級別的市場紅利！", "audio": "docs/data/audios/2506.07999v1.wav"}
{"query": "AI", "id": "2506.07955v1", "url": "http://arxiv.org/abs/2506.07955v1", "title": "Implementation Considerations for Automated AI Grading of Student Work", "summary": "This study explores the classroom implementation of an AI-powered grading\nplatform in K-12 settings through a co-design pilot with 19 teachers. We\ncombine platform usage logs, surveys, and qualitative interviews to examine how\nteachers use AI-generated rubrics and grading feedback. Findings reveal that\nwhile teachers valued the AI's rapid narrative feedback for formative purposes,\nthey distrusted automated scoring and emphasized the need for human oversight.\nStudents welcomed fast, revision-oriented feedback but remained skeptical of\nAI-only grading. We discuss implications for the design of trustworthy,\nteacher-centered AI assessment tools that enhance feedback while preserving\npedagogical agency.", "authors": ["Zewei", "Tian", "Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Zachary Zhang", "Kevin He", "Min Sun"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:28:11.077238", "title_zh": "學生作業自動AI評分之實施考量", "summary_zh": "本研究與19位中小學教師合作，共同設計並試用AI評分平台。透過平台使用紀錄、問卷和訪談，我們發現教師們重視AI快速生成敘述性回饋，有助於形成性評估，但對自動評分的信任度較低，強調人工監督的必要性。學生們歡迎快速且針對修改建議的回饋，但對完全由AI評分仍持懷疑態度。研究結果強調，設計值得信賴、以教師為中心的AI評估工具至關重要，這類工具應能增強回饋效果，同時保留教學自主性。", "applications": ["想像一下，以後老師改作業不用改到天昏地暗，AI可以先幫忙找出重點，老師再針對學生的個別狀況給予指導，這樣老師就有更多時間可以關心每個學生的學習進度。", "學生寫作文，AI可以馬上給予修改建議，像是用字遣詞、文法結構等等，學生可以即時修改，不用等到老師改完才能知道哪裡需要改進，學習效率更高。", "公司新人訓練時，AI可以自動評估新人的學習狀況，並提供客製化的學習建議，幫助新人更快上手，企業也能更有效率地培訓人才。"], "pitch": "各位投資人，想像一下，未來教育的樣貌將會因為AI而徹底改變！我們的AI評分平台，不僅能大幅減輕老師的負擔，更能提供學生更即時、更個人化的學習回饋。這不僅僅是一個評分工具，更是一個提升整體教育品質的革命性產品。目前市場上缺乏真正能與教師協作、值得信賴的AI評分解決方案，而我們正好填補了這個缺口。未來，我們可以將這項技術應用於各個領域，例如企業培訓、線上課程、甚至個人技能提升。透過AI的協助，學習將變得更有效率、更具互動性。現在投資我們，您將參與一場教育科技的變革，共同打造一個更智慧、更高效的學習未來！", "audio": "docs/data/audios/2506.07955v1.wav"}
{"query": "Foundation Model", "id": "2506.07603v1", "url": "http://arxiv.org/abs/2506.07603v1", "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:29:37.081905", "title_zh": "SurgBench：用於手術影片分析的統一大型基準", "summary_zh": "手術影片理解對於手術自動決策、技能評估和術後品質改善至關重要。SurgBench是一個統一的手術影片基準框架，包含一個預訓練資料集SurgBench-P和一個評估基準SurgBench-E。SurgBench-P涵蓋22種手術和11個專科的5300萬幀畫面，SurgBench-E則在六個類別（階段分類、鏡頭運動、工具識別、疾病診斷、動作分類和器官檢測）中提供72個細粒度任務的評估。實驗表明，現有的影片基礎模型難以在不同的手術影片分析任務中推廣，而基於SurgBench-P的預訓練可以顯著提高性能，並提供對未見過的手術和模態的卓越跨域泛化能力。", "applications": ["想像一下，醫生可以透過AI分析手術錄影，即時獲得建議，就像開車時有導航一樣，降低手術風險，提升成功率。", "我們可以利用AI分析手術影片，客觀評估醫生的手術技巧，就像運動員透過數據分析提升表現一樣，幫助醫生精進技術。", "未來，透過分析大量手術影片，AI可以協助開發更精準的醫療器材和手術流程，就像汽車工程師透過碰撞測試改善車輛安全一樣，提升整體醫療水平。"], "pitch": "各位投資人，我們正在打造手術影片分析的「ImageNet」時刻！SurgBench不僅是個資料集，更是推動手術AI發展的引擎。想像一下，AI能像資深外科醫生一樣，即時判斷手術階段、辨識工具、甚至診斷疾病，這將徹底改變手術室的運作模式。透過SurgBench，我們可以開發出：1. 手術導航系統：協助醫生進行更精準、安全的手術；2. 遠程手術協作平台：讓專家醫生能遠端指導手術，解決醫療資源不均的問題；3. 個性化手術訓練系統：根據醫生的能力，提供客製化的訓練方案。市場潛力巨大！全球手術器材和服務市場規模數千億美元，而手術AI的加入，將帶來指數級的增長。我們預計，SurgBench將成為手術AI領域的黃金標準，吸引無數開發者和研究人員投入，共同打造更智慧、更高效、更安全的醫療未來。現在加入，您將站在這場醫療革命的最前沿！", "audio": "docs/data/audios/2506.07603v1.wav"}
{"query": "Diffusion Model", "id": "2506.07998v1", "url": "http://arxiv.org/abs/2506.07998v1", "title": "Generative Modeling of Weights: Generalization or Memorization?", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:31:10.048997", "title_zh": "權重的生成模型：泛化還是記憶？", "summary_zh": "近年來，生成模型在圖像和影片生成方面取得了成功，因此有人開始探索使用它們來合成有效的神經網路權重。這些方法將訓練好的神經網路檢查點作為訓練數據，並旨在於推論過程中生成高性能的神經網路權重。然而，我們的研究發現，這些方法在很大程度上是通過記憶來合成權重的，它們產生的權重要么是訓練檢查點的複製品，要么只是簡單的插值。目前的這些方法在獲得不同且高性能的模型方面，並不能優於簡單的基準方法，例如在權重中添加噪音或採取簡單的權重集成。更令人驚訝的是，即使修改與圖像擴散模型中記憶相關的建模因素或應用數據增強，也無法有效緩解這種記憶現象。這項研究提醒我們，需要更謹慎地評估生成模型在新領域的應用能力，並對當前生成模型可以建模的數據類型進行更實際的評估。", "applications": ["AI藝術風格轉換：如果能真正生成新的模型權重，就能創造出前所未見的藝術風格，讓使用者輕鬆生成獨一無二的藝術作品。", "個性化醫療診斷：針對不同患者的基因數據，生成專屬的AI診斷模型，提供更精準的醫療建議，避免誤診。", "自動化程式碼優化：針對不同的程式碼結構，生成最佳化的編譯器權重，提升程式執行效率，讓App運行更順暢。"], "pitch": "各位創投，我們正在開發一種革命性的AI權重生成技術，它將徹底顛覆現有的AI模型訓練方式。雖然目前的研究顯示現有方法存在記憶問題，但這也代表著巨大的突破機會！想像一下，我們不再需要耗費大量資源從頭訓練模型，而是透過生成模型，快速產生針對特定任務的最佳化權重。這將大幅降低AI開發成本，加速AI應用落地。初期，我們可以聚焦在利基市場，例如個性化醫療、金融風險評估等，透過生成專屬模型，提供更精準的服務。未來，隨著技術成熟，我們更可以打造一個AI權重交易平台，讓AI開發者可以自由交易、組合、再生成新的權重，形成一個蓬勃發展的AI生態系。這不僅僅是一項技術，更是一個全新的商業模式，一個千億美元級的市場！現在加入我們，一起開創AI的下一個黃金時代！", "audio": "docs/data/audios/2506.07998v1.wav"}
{"query": "AI", "id": "2506.07949v1", "url": "http://arxiv.org/abs/2506.07949v1", "title": "Cost-Optimal Active AI Model Evaluation", "summary": "The development lifecycle of generative AI systems requires continual\nevaluation, data acquisition, and annotation, which is costly in both resources\nand time. In practice, rapid iteration often makes it necessary to rely on\nsynthetic annotation data because of the low cost, despite the potential for\nsubstantial bias. In this paper, we develop novel, cost-aware methods for\nactively balancing the use of a cheap, but often inaccurate, weak rater -- such\nas a model-based autorater that is designed to automatically assess the quality\nof generated content -- with a more expensive, but also more accurate, strong\nrater alternative such as a human. More specifically, the goal of our approach\nis to produce a low variance, unbiased estimate of the mean of the target\n\"strong\" rating, subject to some total annotation budget. Building on recent\nwork in active and prediction-powered statistical inference, we derive a family\nof cost-optimal policies for allocating a given annotation budget between weak\nand strong raters so as to maximize statistical efficiency. Using synthetic and\nreal-world data, we empirically characterize the conditions under which these\npolicies yield improvements over prior methods. We find that, especially in\ntasks where there is high variability in the difficulty of examples, our\npolicies can achieve the same estimation precision at a far lower total\nannotation budget than standard evaluation methods.", "authors": ["Anastasios N. Angelopoulos", "Jacob Eisenstein", "Jonathan Berant", "Alekh Agarwal", "Adam Fisch"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:36:20.593723", "title_zh": "成本最佳化的主動式AI模型評估", "summary_zh": "生成式AI系統的開發需要持續評估、資料收集和標註，這些都非常耗時且昂貴。為了加速迭代，開發者常使用成本較低的合成標註資料，但這可能導致偏差。本研究開發了一種成本敏感的方法，能主動平衡低成本但可能不準確的弱評估者（例如基於模型的自動評估器）和更昂貴但更準確的強評估者（例如人類）的使用。我們的目標是在有限的標註預算下，產生目標「強」評級的低變異數、無偏估計。我們利用主動和預測驅動的統計推斷，推導出一系列成本最佳化策略，以最大化統計效率的方式在弱評估者和強評估者之間分配標註預算。實驗結果表明，尤其是在範例難度差異很大的任務中，我們的策略能以遠低於標準評估方法的總標註預算，實現相同的估計精度。", "applications": ["線上購物評論分類：自動分析大量商品評論，判斷哪些評論需要人工審核以確保真實性和準確性，例如過濾機器人產生的假評論，節省人力成本。", "醫療影像診斷輔助：AI初步判讀X光片，將可疑病例優先交由醫生檢查，提高診斷效率並減輕醫生工作負擔。", "自動客服品質監控：AI自動評估客服人員的回答品質，僅將評分較低的對話轉交人工審核，確保服務品質並降低人工監控成本。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它將徹底改變AI模型的評估方式。想像一下，AI模型就像一個學生，需要不斷考試來提升能力。傳統的考試（人工評估）非常昂貴且耗時。我們的技術就像一個AI助教，能夠快速且低成本地進行初步評估，只有少數困難的題目才需要教授（人工專家）親自批改。這意味著，我們可以更快、更經濟地訓練出更強大的AI模型。市場潛力巨大！任何需要高品質AI模型的行業，如自動駕駛、金融風控、醫療診斷等，都將受益於我們的技術。我們預計，隨著AI模型越來越複雜，對高效評估的需求將呈指數級增長。我們的技術不僅能降低成本，還能加速AI的發展進程，搶佔市場先機。我們正在尋找有遠見的投資人，共同打造AI評估的新標準，開創一個AI驅動的未來！", "audio": "docs/data/audios/2506.07949v1.wav"}
{"query": "Foundation Model", "id": "2506.07584v1", "url": "http://arxiv.org/abs/2506.07584v1", "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data", "summary": "A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.", "authors": ["Hao Li", "Bowen Deng", "Chang Xu", "Zhiyuan Feng", "Viktor Schlegel", "Yu-Hao Huang", "Yizheng Sun", "Jingyuan Sun", "Kailai Yang", "Yiyao Yu", "Jiang Bian"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:37:42.168037", "title_zh": "MIRA：用於真實世界健康數據的醫療時間序列基礎模型", "summary_zh": "MIRA是一個專為醫療時間序列預測設計的基礎模型。它利用連續時間旋轉位置編碼精準建模時間間隔，透過頻率特定的專家混合層促進時間特化，並基於神經常微分方程的連續動態外推區塊，在任意目標時間戳上進行準確預測。MIRA在大型醫療數據集上預訓練，包含超過4540億個時間點。相較於其他模型，MIRA在分佈外和分佈內情境下，分別平均降低了10%和7%的預測誤差。我們也建立了一個全面的基準，為未來醫療時間序列建模研究奠定基礎。", "applications": ["**個人健康追蹤：** 想像一下，你的智慧手錶不只記錄心率，還能預測你未來幾小時的血壓變化，提前預警心血管風險，讓你及早採取行動。", "**醫院資源調度：** 醫院可以利用MIRA預測未來急診室的病患流量，提前調配醫護人員和床位，避免醫療資源擠兌，提升整體效率。", "**新藥開發：** 藥廠可以利用MIRA分析臨床試驗數據，更準確地預測藥物療效，加速新藥開發進程，讓更多病患受益。"], "pitch": "各位投資人，我們正處於醫療AI的黃金時代！MIRA不僅僅是一個模型，它是一個醫療時間序列的革命性突破。試想，一個能夠精準預測病患健康狀況、優化醫院運營、加速新藥開發的AI引擎，它的市場潛力有多大？現有的醫療數據分析方法往往受限於數據量和複雜性，而MIRA透過其獨特的架構，克服了這些挑戰，實現了跨機構、跨模態的數據整合和知識遷移。這意味著，即使在數據稀缺或隱私受限的環境下，MIRA也能發揮卓越的預測能力。我們相信，MIRA將成為醫療AI領域的Game Changer，引領醫療健康產業進入一個預測性、個性化的新時代。現在投資MIRA，您將搶佔先機，共同打造一個更健康、更智慧的未來！想像一下，未來的醫療決策不再依賴於事後分析，而是基於精準的預測和預防，這將為整個社會節省巨大的醫療成本，並提升人們的生活品質。MIRA的商業價值遠不止於此，它還可以應用於保險精算、健康管理、遠程醫療等眾多領域，創造無限的可能性。我們誠摯邀請您加入我們的行列，共同見證MIRA的輝煌前景！", "audio": "docs/data/audios/2506.07584v1.wav"}
{"query": "Diffusion Model", "id": "2506.07986v1", "url": "http://arxiv.org/abs/2506.07986v1", "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:39:04.324764", "title_zh": "重新思考多模態擴散轉換器中的跨模態互動", "summary_zh": "多模態擴散轉換器(MM-DiT)在文字驅動的視覺生成方面取得了顯著進展，但現有模型在文字提示與生成內容的精確對齊方面仍存在挑戰。本研究發現MM-DiT的注意力機制存在兩個關鍵問題：視覺和文字模態之間的token不平衡導致跨模態注意力受到抑制，以及缺乏時間步感知的注意力權重，進而阻礙對齊。為了解決這些問題，我們提出一種名為「溫度調整跨模態注意力(TACA)」的參數高效方法，通過溫度縮放和時間步相關調整來動態地重新平衡多模態互動。結合LoRA微調，TACA顯著提升了T2I-CompBench基準測試上的文本-圖像對齊效果，且計算開銷極小。經驗證，TACA能有效改善圖像-文本對齊。", "applications": ["想像一下，你可以用一段文字描述你想要的房間裝潢風格，例如『簡約北歐風，陽光灑落的客廳』，AI就能自動生成符合你描述的3D模型，讓你提前預覽裝潢效果。", "假設你是服裝設計師，只要輸入『一件具有未來感的銀色外套，搭配不對稱剪裁』，AI就能快速生成多種設計草圖，激發你的創作靈感，省去大量手繪時間。", "如果你是社群媒體小編，需要快速製作吸睛的廣告素材，只要輸入產品描述和想要的風格，AI就能自動生成高品質的產品圖片，讓你的廣告更具吸引力。"], "pitch": "各位投資人，我們正站在AI圖像生成革命的浪潮之巔！我們的技術TACA，能讓AI更精準地理解文字指令，生成更符合需求的圖像，解決了目前AI圖像生成領域最核心的痛點。這意味著什麼？想像一下，未來設計師、行銷人員、甚至一般消費者，都能輕鬆地將腦海中的想法轉化為視覺圖像，創造無限可能！從客製化產品設計、虛擬實境內容生成，到電影特效製作，TACA的應用前景無可限量。我們相信，TACA將成為下一代AI圖像生成引擎的核心技術，引領市場走向更精準、更個性化的圖像生成時代。現在加入我們，一起打造AI圖像生成的未來，贏取豐厚的回報！", "audio": "docs/data/audios/2506.07986v1.wav"}
{"query": "AI", "id": "2506.07907v1", "url": "http://arxiv.org/abs/2506.07907v1", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between between the\ndecay amplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De~Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:25:46.104394", "title_zh": "利用C-偶與C-奇量子關聯的D D̄介子對，對D⁰→K⁻π⁺和D̄⁰→K⁻π⁺衰變間強相位差的新穎測量", "summary_zh": "本研究提出一種測量D⁰和D̄⁰介子衰變振幅之間強相位差的新穎技術，利用在ψ(3770)產生閾值以上能量的e⁺e⁻碰撞中產生的量子關聯D D̄介子對。這些介子對產生於電荷共軛對稱性的偶數和奇數本徵態。利用此技術，首次使用此類數據樣本確定了D⁰-D̄⁰的相對強相位。使用BESIII實驗收集的積分光度為7.13 fb⁻¹，質心能量在4.13-4.23 GeV之間的數據集，測得D⁰→K⁻π⁺和D̄⁰→K⁻π⁺衰變之間的強相位差δDKπ為(192.8⁺¹¹.⁰ ⁺¹.⁹₋₁₂.₄ ₋₂.₄)°。", "applications": ["想像一下，我們可以更精準地分析醫療影像。這項技術就像是幫醫生配備了更銳利的眼睛，能更早發現病灶，例如極早期的癌症，提高治癒率。", "在材料科學領域，這項技術能幫助我們更深入了解新材料的微觀結構。這就像是擁有一台超強顯微鏡，讓我們能設計出更堅固、更輕便、更高效能的材料，應用於航空、汽車等產業。", "在通訊加密方面，如果能精準掌握基本粒子的相位，就能開發出更難破解的加密技術。這就像是打造了一道堅不可摧的防火牆，保護我們的數位資訊安全。"], "pitch": "各位投資人，我們正在開發一項革命性的量子測量技術，它能精準掌握亞原子粒子的微妙特性，其潛力遠超想像！目前我們已成功驗證了該技術在D介子衰變上的應用，未來將可拓展至醫療、材料、資安等領域。試想，更精準的醫療診斷、更強大的新材料、以及牢不可破的加密技術，都將因為這項技術而成為可能。這不僅是一項科學突破，更是一座蘊藏無限商機的金礦！我們需要您的資金支持，共同引領這場科技革命，開創量子科技的新紀元！現在投資，您將成為這項劃時代技術的早期投資者，共享豐碩的成果。", "audio": "docs/data/audios/2506.07907v1.wav"}
{"query": "Foundation Model", "id": "2506.07576v1", "url": "http://arxiv.org/abs/2506.07576v1", "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding", "summary": "Video understanding has been considered as one critical step towards world\nmodeling, which is an important long-term problem in AI research. Recently,\nmulti-modal foundation models have shown such potential via large-scale\npretraining. However, these models simply align encoders of different\nmodalities via contrastive learning, while lacking deeper multi-modal\ninteractions, which is critical for understanding complex target movements with\ndiversified video scenes. To fill this gap, we propose a unified Super Encoding\nNetwork (SEN) for video understanding, which builds up such distinct\ninteractions through recursive association of multi-modal encoders in the\nfoundation models. Specifically, we creatively treat those well-trained\nencoders as \"super neurons\" in our SEN. Via designing a Recursive Association\n(RA) block, we progressively fuse multi-modalities with the input video, based\non knowledge integrating, distributing, and prompting of super neurons in a\nrecursive manner. In this way, our SEN can effectively encode deeper\nmulti-modal interactions, for prompting various video understanding tasks in\ndownstream. Extensive experiments show that, our SEN can remarkably boost the\nfour most representative video tasks, including tracking, recognition,\nchatting, and editing, e.g., for pixel-level tracking, the average jaccard\nindex improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular\nCaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,\nand frame consistency increases 4.1% compared to the popular TuneA-Video\napproach.", "authors": ["Boyu Chen", "Siran Chen", "Kunchang Li", "Qinglin Xu", "Yu Qiao", "Yali Wang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:26:56.691958", "title_zh": "超級編碼網路：用於影片理解的多模態編碼器之遞迴關聯", "summary_zh": "本研究提出一個名為「超級編碼網路」(SEN) 的新型影片理解架構，旨在提升多模態基礎模型在理解複雜影片場景中的能力。SEN 將預訓練的編碼器視為「超級神經元」，並透過遞迴關聯(RA)模組，逐步融合多模態資訊。RA模組基於知識整合、分配和提示，以遞迴方式將多模態資訊與輸入影片融合。實驗證明，SEN 能顯著提升影片追蹤、辨識、聊天和編輯等任務的效能，例如，在像素級追蹤上，Jaccard 指數平均提高 2.7%，時間一致性降低 8.8%。", "applications": ["智慧監控：SEN 可以讓監視器更精準地追蹤畫面中的人物或物體，即使在複雜的環境中也能有效辨識異常行為，例如跌倒、打架等，及時發出警報。", "自動駕駛：透過分析車載鏡頭拍攝的影片，SEN 可以幫助自動駕駛系統更準確地理解路況，例如辨識行人、車輛、交通號誌等，做出更安全的決策。", "影音創作：SEN 可以應用於影片編輯軟體中，讓使用者更輕鬆地進行影片剪輯、特效添加等操作，例如自動為影片添加字幕、根據影片內容推薦合適的背景音樂等。"], "pitch": "各位投資人，我們正在開發一種革命性的影片理解技術，它將徹底改變AI對影片內容的理解方式。想像一下，未來的影片分析不再只是簡單的物件辨識，而是能像人類一樣理解影片的上下文、情感和意圖。我們的「超級編碼網路」正是實現這一目標的關鍵。它能讓AI具備更強大的影片分析能力，應用範圍極其廣泛。從智慧安防、自動駕駛，到影音娛樂、教育培訓，甚至是醫療診斷，只要涉及影片內容的理解，我們的技術就能創造巨大的價值。我們相信，隨著5G、AIoT等技術的快速發展，影片數據將呈現爆炸式增長。而我們所掌握的影片理解核心技術，將使我們在這個巨大的市場中佔據領先地位。現在加入我們，一起開創影片理解的新時代！", "audio": "docs/data/audios/2506.07576v1.wav"}
{"query": "Diffusion Model", "id": "2506.07923v1", "url": "http://arxiv.org/abs/2506.07923v1", "title": "Efficient Seismic Data Interpolation via Sparse Attention Transformer and Diffusion Model", "summary": "Seismic data interpolation is a critical pre-processing step for improving\nseismic imaging quality and remains a focus of academic innovation. To address\nthe computational inefficiencies caused by extensive iterative resampling in\ncurrent plug-and-play diffusion interpolation methods, we propose the\ndiffusion-enhanced sparse attention transformer (Diff-spaformer), a novel deep\nlearning framework. Our model integrates transformer architectures and\ndiffusion models via a Seismic Prior Extraction Network (SPEN), which serves as\na bridge module. Full-layer sparse multi-head attention and feed-forward\npropagation capture global information distributions, while the diffusion model\nprovides robust prior guidance. To mitigate the computational burden of\nhigh-dimensional representations, self-attention is computed along the channel\nrather than the spatial dimension. We show that using negative squared\nEuclidean distance to compute sparse affinity matrices better suits seismic\ndata modeling, enabling broader contribution from amplitude feature nodes. An\nadaptive ReLU function further discards low or irrelevant self-attention\nvalues. We conduct training within a single-stage optimization framework,\nrequiring only a few reverse diffusion sampling steps during inference.\nExtensive experiments demonstrate improved interpolation fidelity and\ncomputational efficiency for both random and continuous missing data, offering\na new paradigm for high-efficiency seismic data reconstruction under complex\ngeological conditions.", "authors": ["Xiaoli Wei", "Chunxia Zhang", "Baisong Jiang", "Anxiang Di", "Deng Xiong", "Jiangshe Zhang", "Mingming Gong"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:28:24.557981", "title_zh": "基於稀疏注意力Transformer與擴散模型的高效地震數據內插", "summary_zh": "本研究提出一種名為Diff-spaformer的新型深度學習框架，用於高效地震數據內插。該模型結合了Transformer架構和擴散模型，利用Seismic Prior Extraction Network (SPEN) 作為橋樑。透過全層稀疏多頭注意力機制捕捉全局資訊，並利用擴散模型提供穩健的先驗指導。為降低高維度運算負擔，自注意力計算沿通道而非空間維度進行。實驗證明，該方法在隨機和連續缺失數據情況下，均能提升內插的準確性與計算效率，為複雜地質條件下的高效地震數據重建提供新途徑。簡而言之，這項技術能更快速、更精確地重建地震數據。", "applications": ["**更精準的石油探勘：** 想像一下，石油公司可以更清楚地看到地底下的油藏，減少鑽探的失敗率，降低探勘成本，就像幫他們配備了超強夜視鏡。", "**更安全的建築選址：** 在地震頻繁的地區，我們可以更準確地評估地質結構，避開斷層帶，讓房屋蓋得更安全，就像幫建築師們配備了地質雷達。", "**更有效的災害預防：** 透過分析地震數據，我們可以更了解地殼的變動，預測潛在的地震風險，提前做好防災準備，就像幫我們配備了地震預警系統。"], "pitch": "各位投資人，我們正站在一場地震數據分析革命的風口浪尖！傳統的地震數據內插方法耗時且不精確，嚴重影響石油探勘、建築安全和災害預防等領域。我們的Diff-spaformer技術，結合Transformer和擴散模型，突破了算力瓶頸，大幅提升了數據重建的效率和精度。想像一下，未來的石油探勘不再是盲人摸象，而是精準定位，大幅降低鑽探成本，提高開採成功率！在建築安全方面，我們能提供更可靠的地質評估，避免因地質災害造成的巨大損失。更重要的是，這項技術有潛力應用於地震預測，提前預警，拯救無數生命！我們不僅僅是在銷售一種算法，更是在銷售一種更安全、更高效、更可持續的未來！現在投資，您將成為這場變革的早期參與者，共享巨大的市場紅利。我們預計，該技術在石油天然氣、建築工程、地震監測等領域的市場規模將達到數十億美元。加入我們，一起打造更安全的地球，挖掘更豐富的資源，實現更大的商業價值！", "audio": "docs/data/audios/2506.07923v1.wav"}
{"query": "AI", "id": "2506.07907v2", "url": "http://arxiv.org/abs/2506.07907v2", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between the decay\namplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-11T02:03:24.394715", "title_zh": "利用C-宇稱偶與奇的量子關聯D零反D零對，對D零→K負π正與反D零→K負π正衰變間強相位差的新穎測量", "summary_zh": "本研究提出一種新穎的強相位差測量技術，利用在ψ(3770)產生閾值以上能量的電子正負對撞機產生的量子關聯D零反D零對。此技術利用電荷共軛對稱性的偶與奇本徵態產生的D零反D零對。我們首次使用此數據樣本確定了D零-反D零的相對強相位。利用北京譜儀III實驗收集的7.13 fb⁻¹積分光度數據集，在4.13-4.23 GeV質心能量下，測得D零→K負π正與反D零→K負π正衰變之間的強相位差δDKπ為(192.8+11.0+1.9−12.4−2.4)°。", "applications": ["想像一下，未來海關可以更精準地分辨走私品。透過分析次原子粒子的衰變模式，我們可以開發出更靈敏的檢測儀器，揪出那些企圖蒙混過關的違禁品。", "在醫療領域，這項技術能幫助我們更深入了解藥物與人體細胞的交互作用。透過精確測量粒子間的細微差異，加速新藥開發，讓疾病治療更有效率。", "這就像是為原子世界打造了一把更精密的尺。有了更準確的測量，科學家能更深入探索宇宙的奧秘，例如暗物質的性質，甚至找到新的物理定律。"], "pitch": "各位投資人，我們正在開創一個全新的精準測量時代！這項技術不僅僅是一項科學突破，更是具有顛覆性商業潛力的明日之星。想像一下，透過我們對次原子粒子行為的精準掌握，可以應用於國防安全、新藥開發、材料科學等各個領域，創造出龐大的市場價值。這是一項具有無限可能的技術，現在加入，您將成為這場科技革命的領航者，共同塑造未來世界的樣貌！我們預期在五年內，光是安檢設備升級市場，就可達到數十億美元的規模，而這僅僅是冰山一角。不要錯過這個千載難逢的投資機會，讓我們一起引領科技創新，共創輝煌未來！", "audio": "docs/data/audios/2506.07907v2.wav"}
{"query": "Foundation Model", "id": "2506.07559v1", "url": "http://arxiv.org/abs/2506.07559v1", "title": "Cross-channel Perception Learning for H&E-to-IHC Virtual Staining", "summary": "With the rapid development of digital pathology, virtual staining has become\na key technology in multimedia medical information systems, offering new\npossibilities for the analysis and diagnosis of pathological images. However,\nexisting H&E-to-IHC studies often overlook the cross-channel correlations\nbetween cell nuclei and cell membranes. To address this issue, we propose a\nnovel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL\nfirst decomposes HER2 immunohistochemical staining into Hematoxylin and DAB\nstaining channels, corresponding to cell nuclei and cell membranes,\nrespectively. Using the pathology foundation model Gigapath's Tile Encoder,\nCCPL extracts dual-channel features from both the generated and real images and\nmeasures cross-channel correlations between nuclei and membranes. The features\nof the generated and real stained images, obtained through the Tile Encoder,\nare also used to calculate feature distillation loss, enhancing the model's\nfeature extraction capabilities without increasing the inference burden.\nAdditionally, CCPL performs statistical analysis on the focal optical density\nmaps of both single channels to ensure consistency in staining distribution and\nintensity. Experimental results, based on quantitative metrics such as PSNR,\nSSIM, PCC, and FID, along with professional evaluations from pathologists,\ndemonstrate that CCPL effectively preserves pathological features, generates\nhigh-quality virtual stained images, and provides robust support for automated\npathological diagnosis using multimedia medical data.", "authors": ["Hao Yang", "JianYu Wu", "Run Fang", "Xuelian Zhao", "Yuan Ji", "Zhiyu Chen", "Guibin He", "Junceng Guo", "Yang Liu", "Xinhua Zeng"], "published_date": "2025-06-09", "timestamp": "2025-06-11T02:04:57.583889", "title_zh": "用於H&E到IHC虛擬染色的跨通道感知學習", "summary_zh": "本研究提出一種新的跨通道感知學習(CCPL)策略，旨在解決現有H&E到IHC研究中忽略細胞核與細胞膜之間跨通道關聯性的問題。CCPL首先將HER2免疫組織化學染色分解為對應細胞核和細胞膜的蘇木精和DAB染色通道。利用病理基礎模型Gigapath的Tile Encoder，CCPL提取生成圖像和真實圖像的雙通道特徵，並測量細胞核和細胞膜之間的跨通道相關性。此外，CCPL還對單個通道的焦點光密度圖進行統計分析，以確保染色分佈和強度的連貫性。實驗結果表明，CCPL有效地保留了病理特徵，生成了高質量的虛擬染色圖像，並為使用多媒體醫療數據進行自動病理診斷提供了強大的支持。", "applications": ["1. 醫生遠距會診：偏鄉或資源不足的醫院，可以透過這項技術將傳統染色切片數位化，並轉換成其他染色方式，讓遠端的專家醫生能更全面地判讀病理影像，提升診斷準確性，及時給予治療建議。", "2. 加速新藥開發：藥廠可以利用這項技術，快速將現有的病理切片資料轉換成不同染色方式的影像，加速分析藥物對細胞的作用機制，縮短新藥開發時程，降低研發成本。", "3. 個人化健康管理：未來，透過AI分析個人病理切片資料，並轉換成不同染色結果，預測疾病風險，提供更精準的個人化健康建議，例如飲食、運動或生活習慣調整等。"], "pitch": "各位投資人，我們正在開發一項革命性的病理影像技術：跨通道感知學習(CCPL)，它能將傳統的H&E染色切片轉換成多種IHC染色結果，就像病理影像界的翻譯機！想像一下，全球病理實驗室每年產生數百萬張切片，但往往因為缺乏特定染色試劑或專業判讀人員而延誤診斷。CCPL能解決這個痛點，大幅提升診斷效率與準確性，特別是在癌症等重大疾病的診斷上，時間就是生命！\n\n更重要的是，CCPL的商業潛力巨大。我們可以將這項技術授權給醫院、診所、藥廠，甚至開發成雲端平台，提供全球病理影像分析服務。未來，結合AI和大數據，CCPL還能進一步發展成個人化醫療的關鍵技術，預測疾病風險，指導精準治療。我們相信，CCPL將徹底改變病理診斷的模式，創造巨大的社會價值和商業回報。現在加入我們，一起開創病理影像的未來！", "audio": "docs/data/audios/2506.07559v1.wav"}
{"query": "Diffusion Model", "id": "2506.07903v1", "url": "http://arxiv.org/abs/2506.07903v1", "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces", "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.", "authors": ["Kevin Rojas", "Yuchen Zhu", "Sichen Zhu", "Felix X. -F. Ye", "Molei Tao"], "published_date": "2025-06-09", "timestamp": "2025-06-11T02:07:18.160299", "title_zh": "擴散一切：基於任意狀態空間的多模態擴散模型", "summary_zh": "本研究提出一個創新的多模態擴散模型框架，可以直接在不同的數據模態上生成關聯數據，無需依賴複雜的外部預處理。透過為每個模態設計獨立的雜訊排程，模型能同時進行無條件生成和模態條件生成。研究團隊在文字-圖像生成和混合型表格數據合成上驗證了此方法，結果顯示其性能具有競爭力。這項技術突破了解碼器精準度的限制，為資料有限的應用開闢了新途徑，有望促進多模態資料生成領域的發展。", "applications": ["想像一下，你只要用文字描述你想要的畫面，這個AI就能自動生成對應的圖片，而且圖片的細節和風格完全符合你的要求。例如，輸入「陽光灑落在托斯卡尼田園上的美麗風景」，就能立即生成一幅令人驚嘆的畫作。", "醫生可以利用這個技術，結合病人的文字描述和X光片等影像資料，AI就能生成更精確的3D模型，幫助醫生更了解病情，制定更有效的治療方案。例如，輸入病患的症狀描述加上CT掃描，AI生成腫瘤的3D模型，輔助手術規劃。", "在電商平台上，消費者可以用文字描述想要的商品款式和功能，AI就能根據這些描述，自動生成商品的3D模型和宣傳圖片，讓消費者更直觀地了解商品，提升購買意願。例如，輸入「紅色真皮沙發，現代簡約風格」，就能生成不同角度的沙發照片。"], "pitch": "各位投資人，我們團隊帶來的是一項顛覆性的AI技術——「Diffuse Everything」。它解決了多模態資料生成的核心痛點，讓AI不再受限於單一數據類型。想像一下，一個AI能同時理解文字、圖像、聲音甚至表格數據，並將它們完美融合，創造出全新的內容。這將開啟一個全新的內容創作時代，無論是遊戲開發、廣告設計，甚至是醫療診斷，都將因為這項技術而產生革命性的變化。我們預計，在五年內，這項技術將成為AI領域的基礎設施，市場規模將達到數百億美元。現在加入我們，您將成為這場AI革命的領航者，共同開創一個無限可能的未來！", "audio": "docs/data/audios/2506.07903v1.wav"}
{"query": "AI", "id": "2506.09050v1", "url": "http://arxiv.org/abs/2506.09050v1", "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.", "authors": ["Yuki Imajuku", "Kohki Horie", "Yoichi Iwata", "Kensho Aoki", "Naohiro Takahashi", "Takuya Akiba"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:52:11.520831", "title_zh": "ALE-Bench：長時程目標驅動演算法工程的基準測試", "summary_zh": "ALE-Bench是一個新的基準測試，用於評估AI系統在解決複雜最佳化問題上的能力，這些問題來自現實世界的挑戰，例如包裹遞送路線規劃、人員排班、工廠生產計畫和電網平衡。它採用AtCoder Heuristic Contests中的真實任務，這些任務計算量大且沒有已知的精確解。與短期的編碼基準測試不同，ALE-Bench鼓勵在長時間範圍內迭代改進解決方案，並支持利用測試反饋和視覺化的互動式代理架構。初步評估顯示，現有大型語言模型在特定問題上表現出色，但在跨問題的一致性和長時程問題解決能力方面，與人類相比仍存在差距。這個基準測試旨在推動未來AI的發展。", "applications": ["想像一下，物流公司使用這項技術來優化送貨路線，不僅能節省油耗、減少碳排放，還能更快地將包裹送到客戶手中。", "醫院運用這項技術來安排醫生和護士的輪班，確保每個時段都有足夠的人力，同時也能讓醫護人員得到充分的休息，提升醫療品質。", "工廠利用這項技術來規劃生產流程，減少浪費、提高效率，讓產品更快上市，滿足市場需求。"], "pitch": "各位創投先進，我們正在開發ALE-Bench，一個革命性的AI演算法工程基準測試平台，它將徹底改變優化問題的解決方式。想像一下，一個AI系統可以自動設計出更高效的物流路線、更合理的生產排程、甚至更穩定的電網系統，這不僅能為企業節省數十億美元的成本，還能大幅提升資源利用率，為永續發展做出貢獻。ALE-Bench不僅僅是一個基準測試，它更是一個AI開發的加速器，能激勵研究人員開發出更強大的AI演算法，解決現實世界中最複雜的挑戰。我們相信，ALE-Bench將成為AI領域的Game Changer，而現在正是投資這個未來趨勢的絕佳時機。讓我們一起打造一個更智慧、更高效的世界！", "audio": "docs/data/audios/2506.09050v1.wav"}
{"query": "Foundation Model", "id": "2506.09042v1", "url": "http://arxiv.org/abs/2506.09042v1", "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "summary": "Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:53:40.014111", "title_zh": "Cosmos-Drive-Dreams：基於世界基礎模型的可擴展合成駕駛數據生成", "summary_zh": "為了解決自駕車訓練數據不足且難以捕捉罕見情境的問題，NVIDIA開發了Cosmos-Drive-Dreams合成數據生成管線。此管線利用Cosmos-Drive模型，該模型基於NVIDIA Cosmos世界基礎模型針對駕駛領域進行特化，能產生高品質、多視角、時空一致且可控的駕駛影片。透過Cosmos-Drive-Dreams，可以大規模生成包含高擬真度和挑戰性情境的駕駛數據集，有效改善長尾分布問題，並提升3D車道檢測、3D物件檢測和駕駛策略學習等下游任務的泛化能力。NVIDIA已開源此管線工具包、數據集和模型權重。", "applications": ["情境一：自駕計程車公司可以利用這項技術，模擬各種極端天氣（暴雨、濃霧）或突發狀況（行人突然衝出、車輛違規變換車道），來訓練自駕系統，確保在真實世界中能安全可靠地運行。", "情境二：遊戲開發商可以利用這項技術，快速生成多樣化的城市街道、鄉村道路等場景，讓賽車遊戲或開放世界遊戲的內容更豐富、更逼真，提升玩家的沉浸式體驗。", "情境三：汽車保險公司可以利用這項技術，重現過去發生的交通事故，分析事故原因，評估責任歸屬，甚至可以模擬未來可能發生的事故，提前預防，降低理賠風險。"], "pitch": "各位投資人，想像一下，我們正站在自駕車革命的浪潮之巔！但這場革命的關鍵，不是硬體，而是數據！真實世界數據昂貴且難以收集，特別是那些罕見但致命的邊緣案例。Cosmos-Drive-Dreams，正是解決這個痛點的殺手級應用！\n\n我們提供的，不僅僅是合成數據，而是基於NVIDIA Cosmos世界模型打造的、可控、高擬真的駕駛環境模擬器。這意味著，我們可以無限量地生成各種極端、危險的駕駛情境，讓自駕系統在虛擬世界中經歷千錘百鍊，確保在真實世界中萬無一失！\n\n想想看，這項技術的潛力有多大？它可以加速自駕車的開發和部署，降低事故率，甚至催生全新的保險模式！更重要的是，它還可以應用於遊戲、模擬訓練、城市規劃等領域，打造一個龐大的元宇宙生態系統！\n\n我們不僅僅是在賣數據，我們是在賣安全、賣效率、賣未來！投資Cosmos-Drive-Dreams，就是投資自駕車的未來，就是投資一個萬億美元級別的市場！現在加入我們，一起開創自駕車的新紀元！", "audio": "docs/data/audios/2506.09042v1.wav"}
{"query": "Diffusion Model", "id": "2506.09045v1", "url": "http://arxiv.org/abs/2506.09045v1", "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache", "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.", "authors": ["Zehong Ma", "Longhui Wei", "Feng Wang", "Shiliang Zhang", "Qi Tian"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:54:54.577489", "title_zh": "MagCache：基於幅度感知快取的快速影片生成", "summary_zh": "現有影片擴散模型的加速技術常依賴統一的啟發式方法或時間嵌入變體來跳過時間步並重複使用快取特徵，但這些方法通常需要大量校準，且容易因提示詞過擬合而產生不一致的輸出。本研究發現一個跨模型和提示詞的統一幅度定律：連續殘差輸出的幅度比率在大多數時間步中單調且穩定地下降，在最後幾個步驟中迅速下降。基於此，我們提出幅度感知快取(MagCache)，它使用誤差建模機制和自適應快取策略來跳過不重要的時間步。MagCache只需單一樣本進行校準，實驗結果表明，MagCache在Open-Sora和Wan 2.1上分別實現了2.1倍和2.68倍的加速，同時保持了卓越的視覺保真度。在可比較的計算預算下，MagCache在LPIPS、SSIM和PSNR指標上顯著優於現有方法。", "applications": ["想像一下，你想要快速製作一個社群媒體短片，展示你的寵物做一些可愛的動作。使用MagCache技術，你只需上傳一段短片，AI就能快速生成各種風格的影片，例如卡通風格、水彩風格等，讓你的影片更吸睛。", "假設你是一位老師，需要為學生製作教學影片。MagCache技術可以幫助你快速生成高品質的動畫或模擬影片，例如模擬化學反應、物理實驗等，讓教學內容更生動有趣。", "如果你是一位遊戲開發者，需要快速生成大量的遊戲素材，例如角色動畫、場景特效等。MagCache技術可以大幅縮短開發時間，讓你更快地推出新的遊戲內容。"], "pitch": "各位投資人，我們帶來的是MagCache，一項革命性的影片生成技術，它能大幅提升AI生成影片的速度與品質。想像一下，未來的影音內容創作將不再受限於時間與算力，每個人都能輕鬆創造出專業級的影片。這項技術的應用潛力無窮，從個人化行銷影片、教育內容製作，到遊戲開發、元宇宙內容生成，都將迎來爆炸性的成長。我們預期MagCache將成為AI影音領域的關鍵基礎設施，如同GPU之於深度學習，成為推動產業發展的核心引擎。現在投資MagCache，就是投資影音內容的未來！我們將透過授權、雲端服務、以及與各大影音平台合作等方式，快速將技術商業化，為投資人帶來豐厚的回報。這不僅是一個技術投資，更是一個參與未來內容革命的絕佳機會！", "audio": "docs/data/audios/2506.09045v1.wav"}
{"query": "AI", "id": "2506.09049v1", "url": "http://arxiv.org/abs/2506.09049v1", "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:37:53.933768", "title_zh": "VIKI-R：透過強化學習協調具體化多代理人合作", "summary_zh": "在動態環境中協調多個具體化代理人是人工智慧的核心挑戰。VIKI-R是一個基於視覺語言模型（VLM）的雙階段框架，專為具體化多代理人合作而設計。首先，它使用思維鏈（Chain-of-Thought）標註的示範對預訓練的VLM進行微調，然後在多層次的獎勵信號下進行強化學習。VIKI-R在VIKI-Bench基準測試中表現出色，VIKI-Bench是一個針對具體化多代理人合作的分層基準，包含多樣的機器人形態、多視角視覺觀察和結構化的監督信號。實驗證明，VIKI-R超越了現有方法，並能促使異構代理人之間產生複合式的合作模式。VIKI-R和VIKI-Bench共同為提升具體化AI系統中多代理人、視覺驅動的合作提供了一個統一的測試平台和方法。", "applications": ["智慧倉儲管理：多個機器人協同搬運貨物，根據視覺資訊判斷最佳路徑和搬運方式，提升倉儲效率。", "自動駕駛車隊：多輛自駕車透過視覺資訊共享路況，協同避開障礙物、優化行駛路線，減少交通擁堵。", "協作型機器人手術：多個手術機器人透過視覺引導，協同完成複雜手術，提高手術精準度和安全性。"], "pitch": "各位創投先進，我們正在打造一個AI界的變形金剛聯盟！VIKI-R不僅僅是一個多代理人協作系統，它更是一個賦能平台，讓不同形態、不同功能的機器人能夠像團隊一樣高效協作。想像一下，在智慧工廠裡，不再是單打獨鬥的機器手臂，而是機器人團隊自主協調，完成複雜的生產任務；在無人礦區，不再是各自為政的礦車，而是機器人車隊協同開採，大幅提升資源利用率。VIKI-R的核心價值在於其視覺驅動的協作能力，這意味著它能適應各種複雜的環境，無需預先編程，即可自主學習和優化協作策略。我們相信，隨著AI技術的發展，多代理人協作將成為主流，而VIKI-R將引領這個潮流。我們預計在未來五年內，VIKI-R將廣泛應用於製造業、物流業、醫療保健等領域，創造數十億美元的市場價值。現在加入我們，一起打造AI協作的新時代！", "audio": "docs/data/audios/2506.09049v1.wav"}
{"query": "Foundation Model", "id": "2506.09022v1", "url": "http://arxiv.org/abs/2506.09022v1", "title": "Do MIL Models Transfer?", "summary": "Multiple Instance Learning (MIL) is a cornerstone approach in computational\npathology (CPath) for generating clinically meaningful slide-level embeddings\nfrom gigapixel tissue images. However, MIL often struggles with small, weakly\nsupervised clinical datasets. In contrast to fields such as NLP and\nconventional computer vision, where transfer learning is widely used to address\ndata scarcity, the transferability of MIL models remains poorly understood. In\nthis study, we systematically evaluate the transfer learning capabilities of\npretrained MIL models by assessing 11 models across 21 pretraining tasks for\nmorphological and molecular subtype prediction. Our results show that\npretrained MIL models, even when trained on different organs than the target\ntask, consistently outperform models trained from scratch. Moreover,\npretraining on pancancer datasets enables strong generalization across organs\nand tasks, outperforming slide foundation models while using substantially less\npretraining data. These findings highlight the robust adaptability of MIL\nmodels and demonstrate the benefits of leveraging transfer learning to boost\nperformance in CPath. Lastly, we provide a resource which standardizes the\nimplementation of MIL models and collection of pretrained model weights on\npopular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab", "authors": ["Daniel Shao", "Richard J. Chen", "Andrew H. Song", "Joel Runevic", "Ming Y. Lu", "Tong Ding", "Faisal Mahmood"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:39:24.390655", "title_zh": "MIL模型是否具備遷移能力？", "summary_zh": "多實例學習(MIL)是計算病理學的基石，能從超高解析度的組織影像中產生具有臨床意義的切片層級嵌入。然而，MIL常在小型、弱監督的臨床數據集上表現不佳。本研究系統性地評估了預訓練MIL模型的遷移學習能力，結果顯示，即使在與目標任務不同的器官上訓練，預訓練模型也始終優於從頭訓練的模型。在泛癌數據集上進行預訓練，能實現跨器官和任務的強泛化能力，超越了切片基礎模型，同時使用的預訓練數據更少。本研究強調了MIL模型的強大適應性，並證明了利用遷移學習來提升計算病理學性能的優勢。", "applications": ["1. **癌症篩檢App：**想像一下，未來只要用手機App掃描病理切片，就能利用預訓練好的MIL模型，快速判斷是否有癌症風險，幫助民眾及早發現、及早治療。", "2. **遠距醫療病理診斷：**偏鄉地區缺乏病理醫生，透過MIL模型，可以將病理切片影像上傳雲端，讓遠端的專家協助診斷，大幅提升醫療資源的可及性。", "3. **新藥開發加速器：**藥廠可以利用MIL模型分析大量病理影像數據，找出潛在的藥物靶點，加速新藥開發的進程，降低研發成本。"], "pitch": "各位投資人，我們正在打造一個病理影像AI的革命性平台，核心技術是基於遷移學習的多實例學習(MIL)模型。傳統病理診斷耗時且高度依賴專家經驗，我們的技術能大幅提升診斷效率和準確性，解決醫療資源不均的問題。想像一下，未來每個人都能透過手機進行初步的癌症風險評估，這將是一個千億美元級別的市場！\n\n我們的競爭優勢在於：一是我們已經驗證了MIL模型在病理影像分析上的卓越性能，並遠超傳統方法；二是我們擁有豐富的預訓練模型資源，能快速適應不同的病理診斷任務；三是我們正在建立一個開放的平台，讓更多的研究者和醫生能參與到模型的訓練和應用中來，形成一個強大的生態系統。\n\n我們預計在未來三年內，將我們的技術應用於癌症篩檢、遠距醫療和新藥開發等領域，並與醫院、診所和藥廠建立深度合作夥伴關係。我們相信，我們的技術將會徹底改變病理診斷的模式，為人類健康做出更大的貢獻。現在加入我們，一起開啟病理AI的新時代！", "audio": "docs/data/audios/2506.09022v1.wav"}
{"query": "Diffusion Model", "id": "2506.09027v1", "url": "http://arxiv.org/abs/2506.09027v1", "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.", "authors": ["Runqian Wang", "Kaiming He"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:40:43.699370", "title_zh": "擴散與分散：具備表徵正規化的圖像生成", "summary_zh": "這項研究提出了一種名為「分散損失」（Dispersive Loss）的簡單、隨插即用的正規化方法，能有效提升基於擴散模型的圖像生成效果。不同於以往的擴散模型，此方法著重於表徵學習，鼓勵內部表徵在隱藏空間中分散，類似於對比自監督學習，但無需正樣本配對，因此不干擾迴歸的採樣過程。相較於表徵對齊（REPA）方法，此方法更簡潔，無需預訓練、額外參數或外部資料。實驗證明，在ImageNet數據集上，此方法能穩定地提升多種模型的效能。這項研究有望彌合生成模型和表徵學習之間的差距。", "applications": ["修復老照片或模糊圖片：想像一下，你可以用手機App輕鬆修復爺爺奶奶的老照片，讓模糊的影像變得清晰，重溫美好的回憶。", "生成獨一無二的藝術作品：藝術家可以利用這項技術，創造出前所未見的藝術風格，生成獨特且令人驚豔的數位畫作，豐富我們的視覺體驗。", "客製化商品設計：設計師可以根據客戶的需求，快速生成各種產品設計方案，例如客製化的手機殼、T恤圖案等，滿足個性化需求。"], "pitch": "各位創投先進，我們正在開發一項革命性的圖像生成技術，它能大幅提升生成圖像的品質和真實感，且成本極低。想像一下，未來電商平台可以利用這項技術，自動生成各種商品的逼真圖片，大幅降低攝影成本；遊戲公司可以快速創建精美的遊戲場景和角色，縮短開發週期；醫療領域可以生成高清晰度的醫學影像，輔助診斷。我們的「分散損失」方法，無需大量數據和複雜的訓練，就能達到甚至超越現有技術的效果。這不僅能為我們帶來巨大的商業價值，更能推動AI技術在各個領域的應用，改變人們的生活方式。現在投資我們，您將成為這場圖像革命的領航者，共同創造一個充滿無限可能的未來！", "audio": "docs/data/audios/2506.09027v1.wav"}
{"query": "AI", "id": "2506.09002v1", "url": "http://arxiv.org/abs/2506.09002v1", "title": "Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models", "summary": "Unit testing is essential for ensuring software reliability and correctness.\nClassic Search-Based Software Testing (SBST) methods and concolic\nexecution-based approaches for generating unit tests often fail to achieve high\ncoverage due to difficulties in handling complex program units, such as\nbranching conditions and external dependencies. Recent work has increasingly\nutilized large language models (LLMs) to generate test cases, improving the\nquality of test generation by providing better context and correcting errors in\nthe model's output. However, these methods rely on fixed prompts, resulting in\nrelatively low compilation success rates and coverage. This paper presents\nPALM, an approach that leverages large language models (LLMs) to enhance the\ngeneration of high-coverage unit tests. PALM performs program analysis to\nidentify branching conditions within functions, which are then combined into\npath constraints. These constraints and relevant contextual information are\nused to construct prompts that guide the LLMs in generating unit tests. We\nimplement the approach and evaluate it in 10 open-source Rust crates.\nExperimental results show that within just two or three hours, PALM can\nsignificantly improves test coverage compared to classic methods, with\nincreases in overall project coverage exceeding 50% in some instances and its\ngenerated tests achieving an average coverage of 75.77%, comparable to human\neffort (71.30%), highlighting the potential of LLMs in automated test\ngeneration. We submitted 91 PALM-generated unit tests targeting new code. Of\nthese submissions, 80 were accepted, 5 were rejected, and 6 remain pending\nreview. The results demonstrate the effectiveness of integrating program\nanalysis with AI and open new avenues for future research in automated software\ntesting.", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Hange Shi", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:29:22.809571", "title_zh": "透過混合程式分析與大型語言模型提升 Rust 單元測試覆蓋率", "summary_zh": "本研究提出 PALM 方法，結合程式分析與大型語言模型（LLM），顯著提升 Rust 程式的單元測試覆蓋率。傳統測試方法難以處理複雜的分支條件和外部依賴，而 PALM 透過程式分析找出分支條件，將其轉化為路徑約束，並結合上下文資訊，引導 LLM 生成更有效的測試案例。實驗結果顯示，PALM 在數小時內即可大幅提升測試覆蓋率，部分專案甚至超過 50%，平均覆蓋率達到 75.77%，與人工測試相當。已提交的測試案例中，絕大多數已被接受，證明了此方法在自動化測試領域的潛力。", "applications": ["**自動修復程式錯誤：** 想像一下，程式碼寫完後，PALM 就像一個超級偵錯員，自動產生測試案例，找出潛在的錯誤，並協助開發者快速修復，減少程式崩潰的機會。", "**提升軟體品質：** 銀行、醫療等對軟體穩定性要求極高的產業，可以利用 PALM 確保程式碼的品質，避免因程式錯誤造成重大損失。", "**加速開發流程：** 開發者不再需要花費大量時間編寫測試案例，PALM 可以自動生成，讓他們更專注於核心功能的開發，加速產品上市時間。"], "pitch": "各位投資人，我們正處於AI驅動軟體開發的革命性時刻！PALM不僅僅是個測試工具，它是軟體品質保證的未來。想想看，全球每年因軟體錯誤造成的損失高達數千億美元，而PALM能有效降低這些損失，為企業節省巨額成本。更重要的是，隨著AI技術的不斷發展，PALM的自我學習能力將持續提升，測試覆蓋率和效率將遠超人工。我們預見，PALM將成為所有軟體開發團隊的標配，市場潛力無限。現在投資PALM，就是投資軟體開發的未來，讓我們一起打造更可靠、更穩定的數位世界！", "audio": "docs/data/audios/2506.09002v1.wav"}
{"query": "Foundation Model", "id": "2506.08982v1", "url": "http://arxiv.org/abs/2506.08982v1", "title": "On Finetuning Tabular Foundation Models", "summary": "Foundation models are an emerging research direction in tabular deep\nlearning. Notably, TabPFNv2 recently claimed superior performance over\ntraditional GBDT-based methods on small-scale datasets using an in-context\nlearning paradigm, which does not adapt model parameters to target datasets.\nHowever, the optimal finetuning approach for adapting tabular foundational\nmodels, and how this adaptation reshapes their internal mechanisms, remains\nunderexplored. While prior works studied finetuning for earlier foundational\nmodels, inconsistent findings and TabPFNv2's unique architecture necessitate\nfresh investigation. To address these questions, we first systematically\nevaluate various finetuning strategies on diverse datasets. Our findings\nestablish full finetuning as the most practical solution for TabPFNv2 in terms\nof time-efficiency and effectiveness. We then investigate how finetuning alters\nTabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.\nWe reveal that the success of finetuning stems from the fact that after\ngradient-based adaptation, the dot products of the query-representations of\ntest objects and the key-representations of in-context training objects more\naccurately reflect their target similarity. This improved similarity allows\nfinetuned TabPFNv2 to better approximate target dependency by appropriately\nweighting relevant in-context samples, improving the retrieval-based prediction\nlogic. From the practical perspective, we managed to finetune TabPFNv2 on\ndatasets with up to 50K objects, observing performance improvements on almost\nall tasks. More precisely, on academic datasets with I.I.D. splits, finetuning\nallows TabPFNv2 to achieve state-of-the-art results, while on datasets with\ngradual temporal shifts and rich feature sets, TabPFNv2 is less stable and\nprior methods remain better.", "authors": ["Ivan Rubachev", "Akim Kotelnikov", "Nikolay Kartashev"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:30:59.879609", "title_zh": "表格型資料基礎模型的微調研究", "summary_zh": "本研究探討如何微調表格型資料的基礎模型，特別是TabPFNv2。TabPFNv2原本擅長利用少量資料進行學習，無需調整模型參數。然而，如何有效微調此類模型，以及微調如何改變其內部運作機制，仍是未解之謎。研究發現，完整微調是TabPFNv2最有效率且效果最好的方法。微調後的模型，能更精準地判斷測試資料與訓練資料的相似度，進而提升預測準確性。實驗證明，微調後的TabPFNv2在包含五萬筆資料的數據集上，也能有效提升效能，在學術資料集上甚至達到最先進的水平。這項研究為表格型資料基礎模型的應用開闢了新的可能性。", "applications": ["**個人貸款風險評估：** 銀行或金融機構可以利用微調後的TabPFNv2，根據客戶的少量歷史資料（例如：年齡、收入、信用評分）更精準地預測貸款違約風險，快速決定是否核准貸款，降低壞帳率。", "**疾病診斷輔助：** 醫生可以使用微調後的TabPFNv2，基於少量的病患數據（例如：症狀、檢驗報告）輔助診斷罕見疾病，提高診斷效率和準確性，避免延誤治療。", "**客戶流失預測：** 電信公司或零售業者可以利用微調後的TabPFNv2，分析少量的客戶行為數據（例如：購買紀錄、瀏覽習慣）預測客戶是否可能流失，及早採取挽留措施，提升客戶忠誠度。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，能徹底改變表格型資料的應用方式。想像一下，在數據量有限的情況下，也能擁有媲美甚至超越大型模型的精準度。這就是微調後的TabPFNv2的潛力！\n\n傳統機器學習需要大量數據才能訓練出可靠的模型，但現實世界中，許多領域都面臨數據稀缺的挑戰。我們的技術能讓企業在小數據集上快速建立精準的預測模型，大幅降低數據收集和處理的成本。例如，新藥開發、罕見疾病診斷、客製化行銷等領域，都將因此受益。\n\n更重要的是，我們的技術具有高度的可擴展性。未來，我們計劃將此技術應用於更廣泛的領域，例如：金融風險管理、智能製造、智慧城市等。我們相信，微調後的TabPFNv2將成為企業決策的強大助力，為投資者帶來豐厚的回報。現在投資，您將站在AI革命的最前沿，共同開創無限可能！", "audio": "docs/data/audios/2506.08982v1.wav"}
{"query": "Diffusion Model", "id": "2506.08809v1", "url": "http://arxiv.org/abs/2506.08809v1", "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference", "summary": "High-resolution sinogram inpainting is essential for computed tomography\nreconstruction, as missing high-frequency projections can lead to visible\nartifacts and diagnostic errors. Diffusion models are well-suited for this task\ndue to their robustness and detail-preserving capabilities, but their\napplication to high-resolution inputs is limited by excessive memory and\ncomputational demands. To address this limitation, we propose HiSin, a novel\ndiffusion based framework for efficient sinogram inpainting via\nresolution-guided progressive inference. It progressively extracts global\nstructure at low resolution and defers high-resolution inference to small\npatches, enabling memory-efficient inpainting. It further incorporates\nfrequency-aware patch skipping and structure-adaptive step allocation to reduce\nredundant computation. Experimental results show that HiSin reduces peak memory\nusage by up to 31.25% and inference time by up to 18.15%, and maintains\ninpainting accuracy across datasets, resolutions, and mask conditions.", "authors": ["Jiaze E", "Srutarshi Banerjee", "Tekin Bicer", "Guannan Wang", "Yanfu Zhang", "Bin Ren"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:32:39.355347", "title_zh": "HiSin：透過解析度導向的漸進式推論實現高效能高解析度正弦圖修復", "summary_zh": "在電腦斷層掃描中，高解析度正弦圖修復至關重要，因為遺失的高頻投影可能導致明顯的偽影和診斷錯誤。擴散模型擅長此任務，但其在高解析度輸入上的應用受限於過多的記憶體和計算需求。我們提出HiSin，一種基於擴散的新穎框架，透過解析度導向的漸進式推論實現高效的正弦圖修復。它在低解析度下逐步提取全局結構，並將高解析度推論延遲到小塊區域，從而實現記憶體效率高的修復。此外，它還整合了頻率感知補丁跳過和結構自適應步驟分配，以減少冗餘計算。實驗結果表明，HiSin將峰值記憶體使用量減少了高達31.25%，推論時間減少了高達18.15%，並在資料集、解析度和遮罩條件下保持了修復準確性。", "applications": ["想像一下，醫院的CT掃描儀經常會因為各種原因產生一些雜訊或缺失數據，導致影像不清晰，影響醫生診斷。HiSin就像一個影像魔術師，能自動修復這些缺失的部分，讓醫生看到更清晰、更準確的影像，減少誤診的風險。", "考古學家在挖掘古代文物時，常常會使用X光或CT掃描來觀察文物內部的結構。如果掃描結果不完整或有干擾，HiSin就能派上用場，幫助他們重建清晰的文物內部影像，從而更好地研究古代文明。", "機場安檢人員使用X光機檢查行李時，有時會遇到影像模糊或遮蔽的情況。HiSin可以輔助安檢系統，修復這些影像，提高安檢效率，同時也能更準確地識別潛在的危險物品。"], "pitch": "各位投資人，我們正處於醫療影像AI革命的風口浪尖！HiSin不僅僅是一個修復演算法，它代表著更高效、更精準的醫療診斷的未來。現有的高解析度影像處理方案耗時且資源密集，嚴重限制了臨床應用。HiSin透過獨特的解析度導向漸進式推論，大幅降低了計算成本，讓高解析度影像修復成為可能，潛力巨大。想像一下，未來，我們的技術可以應用於自動駕駛的雷達信號處理、衛星影像的清晰化，甚至是在藝術品修復領域大放異彩！這是一個數十億美元的市場，而HiSin有機會成為這個市場的領導者。我們需要您的投資，一起將這項技術推向世界，為人類健康和科技進步做出貢獻！", "audio": "docs/data/audios/2506.08809v1.wav"}
{"query": "AI", "id": "2506.08998v1", "url": "http://arxiv.org/abs/2506.08998v1", "title": "On Monotonicity in AI Alignment", "summary": "Comparison-based preference learning has become central to the alignment of\nAI models with human preferences. However, these methods may behave\ncounterintuitively. After empirically observing that, when accounting for a\npreference for response $y$ over $z$, the model may actually decrease the\nprobability (and reward) of generating $y$ (an observation also made by\nothers), this paper investigates the root causes of (non) monotonicity, for a\ngeneral comparison-based preference learning framework that subsumes Direct\nPreference Optimization (DPO), Generalized Preference Optimization (GPO) and\nGeneralized Bradley-Terry (GBT). Under mild assumptions, we prove that such\nmethods still satisfy what we call local pairwise monotonicity. We also provide\na bouquet of formalizations of monotonicity, and identify sufficient conditions\nfor their guarantee, thereby providing a toolbox to evaluate how prone learning\nmodels are to monotonicity violations. These results clarify the limitations of\ncurrent methods and provide guidance for developing more trustworthy preference\nlearning algorithms.", "authors": ["Gilles Bareilles", "Julien Fageot", "Lê-Nguyên Hoang", "Peva Blanchard", "Wassim Bouaziz", "Sébastien Rouault", "El-Mahdi El-Mhamdi"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:53:57.472960", "title_zh": "論AI對齊中的單調性", "summary_zh": "基於比較的偏好學習已成為將AI模型與人類偏好對齊的核心方法。然而，這些方法有時會產生反直覺的行為。研究發現，當模型考慮到對回應y的偏好高於z時，實際上可能降低生成y的機率（和獎勵）。本研究深入探討了非單調性的根本原因，針對包含直接偏好優化（DPO）、廣義偏好優化（GPO）和廣義布拉德利-特里（GBT）的通用比較偏好學習框架。在溫和的假設下，證明了這些方法仍然滿足我們稱之為局部成對單調性的性質。此外，我們提供了一系列單調性的形式化描述，並確定了保證它們的充分條件，從而提供了一個工具箱來評估學習模型違反單調性的可能性。這些結果闡明了當前方法的局限性，並為開發更值得信賴的偏好學習算法提供了指導。", "applications": ["**個人化推薦系統：** 想像一下，你告訴AI你比較喜歡A餐廳勝過B餐廳，但系統之後卻一直推薦類似B餐廳的選項。這個研究能幫助改善推薦系統，確保它們真的會記住你的偏好，並推薦你更喜歡的內容。", "**自動駕駛：** 如果你告訴自動駕駛系統你比較喜歡平穩的駕駛風格，系統卻突然開始急加速或急煞車，你會覺得很奇怪也很危險。這個研究有助於確保AI系統的行為始終如一，符合你的偏好，提高安全性。", "**AI助理：** 當你告訴AI助理你喜歡某種音樂類型，結果它卻開始播放完全相反的音樂，這會讓人感到困惑。這個研究可以幫助AI助理更好地理解和記住你的偏好，提供更個性化的服務。"], "pitch": "各位創投先進，我們正在解決AI領域一個至關重要的問題：AI對齊。目前，AI系統在學習人類偏好時，經常出現「說一套做一套」的情況，導致用戶體驗不佳，甚至產生安全隱患。我們的研究成果，揭示了這種非單調性行為的根本原因，並提供了一套解決方案，讓AI系統真正理解並遵循人類的偏好。想像一下，一個能夠完全理解使用者意圖的AI，在醫療診斷、金融投資、教育輔導等領域，將會釋放出多麼巨大的商業價值！我們不僅僅是提供一個技術方案，更是打造一個更值得信賴、更人性化的AI未來。現在投資我們，您將站在AI對齊的最前沿，共同引領下一代AI技術的發展，掌握未來AI市場的制高點！未來，我們甚至可以將此技術應用於開發『讀心術』AI，透過分析人類微表情與生理數據，精準預測人類需求，打造劃時代的AI產品與服務，其商業潛力無可限量！", "audio": "docs/data/audios/2506.08998v1.wav"}
{"query": "Foundation Model", "id": "2506.08955v1", "url": "http://arxiv.org/abs/2506.08955v1", "title": "Segment Concealed Objects with Incomplete Supervision", "summary": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves\nsegmenting objects that seamlessly blend into their surrounding environments,\nutilizing incompletely annotated data, such as weak and semi-annotations, for\nmodel training. This task remains highly challenging due to (1) the limited\nsupervision provided by the incompletely annotated training data, and (2) the\ndifficulty of distinguishing concealed objects from the background, which\narises from the intrinsic similarities in concealed scenarios. In this paper,\nwe introduce the first unified method for ISCOS to address these challenges. To\ntackle the issue of incomplete supervision, we propose a unified mean-teacher\nframework, SEE, that leverages the vision foundation model, ``\\emph{Segment\nAnything Model (SAM)}'', to generate pseudo-labels using coarse masks produced\nby the teacher model as prompts. To mitigate the effect of low-quality\nsegmentation masks, we introduce a series of strategies for pseudo-label\ngeneration, storage, and supervision. These strategies aim to produce\ninformative pseudo-labels, store the best pseudo-labels generated, and select\nthe most reliable components to guide the student model, thereby ensuring\nrobust network training. Additionally, to tackle the issue of intrinsic\nsimilarity, we design a hybrid-granularity feature grouping module that groups\nfeatures at different granularities and aggregates these results. By clustering\nsimilar features, this module promotes segmentation coherence, facilitating\nmore complete segmentation for both single-object and multiple-object images.\nWe validate the effectiveness of our approach across multiple ISCOS tasks, and\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancing\nthe performance of existing models.", "authors": ["Chunming He", "Kai Li", "Yachao Zhang", "Ziyun Yang", "Youwei Pang", "Longxiang Tang", "Chengyu Fang", "Yulun Zhang", "Linghe Kong", "Xiu Li", "Sina Farsiu"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:55:36.593228", "title_zh": "使用不完整監督分割隱藏物體", "summary_zh": "本研究提出一種新的方法，用於在只有不完整標註資料的情況下，分割那些與周圍環境無縫融合的隱藏物體。由於標註資料不完整，以及隱藏物體與背景高度相似，這項任務極具挑戰性。我們提出名為SEE的統一平均教師框架，利用視覺基礎模型SAM生成偽標籤，並設計一系列策略來生成、儲存和監督這些偽標籤，以確保穩健的網路訓練。此外，我們設計了一種混合粒度特徵分組模組，通過聚類相似特徵，促進分割一致性，進而更完整地分割單個或多個物體。實驗結果表明，我們的模型在多個隱藏物體分割任務上都達到了最先進的性能，並且可以作為一個隨插即用的解決方案，提升現有模型的效能。", "applications": ["**智慧安防：** 在監視器畫面中，即使嫌犯試圖隱藏身形融入背景，這項技術也能準確識別並追蹤，提高破案率。", "**醫療影像分析：** 醫生可以利用這項技術，在X光片或MRI影像中更清楚地辨識出隱藏在組織中的微小腫瘤，及早發現並治療。", "**自動駕駛：** 在惡劣天氣或光線不足的情況下，車載鏡頭可能難以辨識道路上的行人或障礙物，這項技術能幫助車輛更準確地感知周圍環境，提升行車安全。"], "pitch": "各位投資人，想像一下，一個能看穿偽裝的世界！我們的技術正在開啟這個可能性。現今的AI視覺辨識在隱藏物體的辨識上仍然存在巨大盲點，這不僅是技術挑戰，更是潛在的巨大商機。我們的『不完整監督隱藏物體分割』技術，能讓機器在近乎無監督的情況下，學習辨識隱藏的目標，這代表什麼？\n\n首先，安防產業將迎來革命。想想機場安檢、邊境管制，甚至是商場防盜，我們的技術能有效提升安全等級，減少犯罪發生。\n\n其次，醫療診斷將更加精準。早期癌症的檢測往往仰賴醫生肉眼判斷，我們的技術能輔助醫生發現微小病灶，大幅提升治癒率。\n\n更重要的是，自動駕駛將更加安全可靠。在惡劣環境下，我們的技術能幫助汽車『看』得更清楚，減少事故發生。\n\n這項技術的應用範圍遠不止於此。想像一下，軍事偵察、工業瑕疵檢測、甚至是考古研究，都能因為我們的技術而取得突破性進展。\n\n我們不僅僅是在開發一個演算法，我們正在打造一個全新的視覺感知平台。我們預計在未來五年內，將這項技術推廣到全球市場，與各產業龍頭合作，共同打造一個更安全、更智慧的世界。現在加入我們，您將成為這場視覺革命的先驅！", "audio": "docs/data/audios/2506.08955v1.wav"}
{"query": "Diffusion Model", "id": "2506.08796v1", "url": "http://arxiv.org/abs/2506.08796v1", "title": "Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling", "summary": "Recently, the rectified flow (RF) has emerged as the new state-of-the-art\namong flow-based diffusion models due to its high efficiency advantage in\nstraight path sampling, especially with the amazing images generated by a\nseries of RF models such as Flux 1.0 and SD 3.0. Although a straight-line\nconnection between the noisy and natural data distributions is intuitive, fast,\nand easy to optimize, it still inevitably leads to: 1) Diversity concerns,\nwhich arise since straight-line paths only cover a fairly restricted sampling\nspace. 2) Multi-scale noise modeling concerns, since the straight line flow\nonly needs to optimize the constant velocity field $\\bm v$ between the two\ndistributions $\\bm\\pi_0$ and $\\bm\\pi_1$. In this work, we present\nDiscretized-RF, a new family of rectified flow (also called momentum flow\nmodels since they refer to the previous velocity component and the random\nvelocity component in each diffusion step), which discretizes the straight path\ninto a series of variable velocity field sub-paths (namely ``momentum fields'')\nto expand the search space, especially when close to the distribution\n$p_\\text{noise}$. Different from the previous case where noise is directly\nsuperimposed on $\\bm x$, we introduce noise on the velocity $\\bm v$ of the\nsub-path to change its direction in order to improve the diversity and\nmulti-scale noise modeling abilities. Experimental results on several\nrepresentative datasets demonstrate that learning momentum flow matching by\nsampling random velocity fields will produce trajectories that are both diverse\nand efficient, and can consistently generate high-quality and diverse results.\nCode is available at https://github.com/liuruixun/momentum-fm.", "authors": ["Zhiyuan Ma", "Ruixun Liu", "Sixian Liu", "Jianjun Li", "Bowen Zhou"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:56:45.617158", "title_zh": "流動多樣且高效：透過隨機速度場採樣學習動量流匹配", "summary_zh": "這項研究提出一種新的修正流模型，稱為Discretized-RF，它將傳統的直線路徑分割成一系列變速場子路徑，擴展搜尋空間，尤其是在接近雜訊分佈時。不同於直接在資料上疊加雜訊，此方法在子路徑的速度上引入雜訊，改變其方向，從而提升多樣性和多尺度雜訊建模能力。實驗結果表明，透過採樣隨機速度場學習動量流匹配，可以產生多樣且高效的軌跡，並能持續生成高品質且多樣化的結果。簡單來說，這項技術讓AI生成圖像更豐富、更真實，也更有效率。", "applications": ["遊戲開發：快速生成多樣化的遊戲場景和角色，節省美術設計時間，並提供更豐富的遊戲體驗。", "影視特效：高效創建逼真的視覺特效，例如自然災害、奇幻生物等，降低製作成本，提升視覺衝擊力。", "服裝設計：設計師可以快速生成各種服裝款式和紋理，激發靈感，並根據客戶需求進行個性化定制。"], "pitch": "各位創投先進，想像一下，未來AI不再只是複製現有圖像，而是能創造出前所未見的藝術品、設計出獨一無二的產品！我們的Discretized-RF技術，突破了傳統AI圖像生成的限制，讓AI更像一位真正的藝術家，擁有無限的創意和想像力。這項技術不僅能大幅提升圖像生成效率，降低成本，更能開創全新的商業模式。例如，我們可以打造一個AI設計平台，讓使用者輕鬆生成個性化的產品設計、藝術作品，甚至虛擬世界的場景和角色。未來，每個企業、每個個人都能擁有自己的AI設計師，創造出無限的商業價值。現在投資，您將站在AI圖像革命的最前沿，共同見證AI創造力的無限可能！", "audio": "docs/data/audios/2506.08796v1.wav"}
{"query": "AI", "id": "2506.08962v1", "url": "http://arxiv.org/abs/2506.08962v1", "title": "WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis", "summary": "This research-to-practice work-in-progress (WIP) paper presents an AI-enabled\nsmart tutor designed to provide homework assessment and feedback for students\nin an undergraduate circuit analysis course. We detail the tutor's design\nphilosophy and core components, including open-ended question answering and\nhomework feedback generation. The prompts are carefully crafted to optimize\nresponses across different problems. The smart tutor was deployed on the\nMicrosoft Azure platform and is currently in use in an undergraduate circuit\nanalysis course at the School of Electrical and Computer Engineering in a\nlarge, public, research-intensive institution in the Southeastern United\nStates. Beyond offering personalized instruction and feedback, the tutor\ncollects student interaction data, which is summarized and shared with the\ncourse instructor. To evaluate its effectiveness, we collected student\nfeedback, with 90.9% of responses indicating satisfaction with the tutor.\nAdditionally, we analyze a subset of collected data on preliminary circuit\nanalysis topics to assess tutor usage frequency for each problem and identify\nfrequently asked questions. These insights help instructors gain real-time\nawareness of student difficulties, enabling more targeted classroom\ninstruction. In future work, we will release a full analysis once the complete\ndataset is available after the Spring 2025 semester. We also explore the\npotential applications of this smart tutor across a broader range of\nengineering disciplines by developing improved prompts, diagram-recognition\nmethods, and database management strategies, which remain ongoing areas of\nresearch.", "authors": ["Liangliang Chen", "Huiru Xie", "Jacqueline Rohde", "Ying Zhang"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:31:03.189904", "title_zh": "WIP：大型語言模型增強型大學電路分析智慧導師", "summary_zh": "本研究展示一個AI驅動的智慧導師，專為大學電路分析課程的學生提供作業評估和回饋。它基於大型語言模型，能解答開放式問題並生成客製化的作業回饋。此智慧導師已部署在Microsoft Azure平台上，並在美國東南部一所大型公立研究型大學的電路分析課程中使用。除了提供個人化指導，它還收集學生互動數據，並將摘要分享給授課教師，幫助教師即時掌握學生學習困難，以便調整教學策略。初步的學生回饋顯示90.9%的學生對此導師感到滿意。未來，我們將進一步分析完整數據集，並探索其在更廣泛工程領域的應用潛力，例如改進提示、開發圖表識別方法和資料庫管理策略。", "applications": ["想像一下，高中生在學習物理電學時遇到困難，這個AI智慧導師可以一步一步引導他理解電路原理，就像一位隨時待命的私人教師。", "對於需要考取電機工程師證照的人來說，這個智慧導師可以提供客製化的模擬試題和詳盡的解答，幫助他們高效備考。", "電子工程師在設計複雜電路時，可以利用這個智慧導師快速驗證設計方案，找出潛在問題，節省大量的時間和成本。"], "pitch": "各位投資人，我們正在打造的是一個革命性的教育科技產品——基於大型語言模型的智慧導師，它不僅能個性化輔導學生，更能即時反饋學習數據，幫助老師精準教學。試想一下，未來每個學生都擁有一個24小時隨時待命的AI家教，學習效率將大幅提升！更重要的是，這個技術可以拓展到其他理工學科，甚至語言學習、藝術指導等領域，市場潛力巨大。我們不僅僅是提供一個輔導工具，我們是在打造一個全新的教育生態系統。預計未來五年內，我們的智慧導師將覆蓋全球數百萬理工科學生，成為教育科技領域的獨角獸！現在加入我們，共同開創AI教育的新時代！", "audio": "docs/data/audios/2506.08962v1.wav"}
{"query": "Foundation Model", "id": "2506.08949v1", "url": "http://arxiv.org/abs/2506.08949v1", "title": "SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation", "summary": "In the era of information explosion, efficiently leveraging large-scale\nunlabeled data while minimizing the reliance on high-quality pixel-level\nannotations remains a critical challenge in the field of medical imaging.\nSemi-supervised learning (SSL) enhances the utilization of unlabeled data by\nfacilitating knowledge transfer, significantly improving the performance of\nfully supervised models and emerging as a highly promising research direction\nin medical image analysis. Inspired by the ability of Vision Foundation Models\n(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised\nSAM-2), a novel approach that leverages SAM-2's robust feature extraction\ncapabilities to uncover latent knowledge in unlabeled medical images, thus\neffectively enhancing feature support for fully supervised medical image\nsegmentation. Specifically, building upon the single-stream \"weak-to-strong\"\nconsistency regularization framework, this paper introduces a Discriminative\nFeature Enhancement (DFE) mechanism to further explore the feature\ndiscrepancies introduced by various data augmentation strategies across\nmultiple views. By leveraging feature similarity and dissimilarity across\nmulti-scale augmentation techniques, the method reconstructs and models the\nfeatures, thereby effectively optimizing the salient regions. Furthermore, a\nprompt generator is developed that integrates Physical Constraints with a\nSliding Window (PCSW) mechanism to generate input prompts for unlabeled data,\nfulfilling SAM-2's requirement for additional prompts. Extensive experiments\ndemonstrate the superiority of the proposed method for semi-supervised medical\nimage segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,\nSSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous\nstate-of-the-art method by +3.65 Dice. Code will be available at\nhttps://github.com/AIGeeksGroup/SSS.", "authors": ["Hongjie Zhu", "Xiwei Liu", "Rundong Xue", "Zeyu Zhang", "Yong Xu", "Daji Ergu", "Ying Cai", "Yang Zhao"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:32:20.384344", "title_zh": "SSS：基於高效提示的半監督SAM-2醫學影像分割", "summary_zh": "在醫學影像領域，如何有效利用大量未標記數據，同時減少對高質量像素級標註的依賴，是個重要挑戰。本研究提出SSS方法，利用SAM-2強大的特徵提取能力，挖掘未標記醫學影像中的潛在知識，增強全監督醫學影像分割的特徵支持。透過“弱到強”一致性正則化框架，引入判別特徵增強機制，探索多視角數據增強策略帶來的特徵差異，重建和建模特徵，有效優化顯著區域。此外，開發結合物理約束和滑動窗口的提示生成器，滿足SAM-2對額外提示的需求。實驗證明，SSS在半監督醫學影像分割上優於現有方法。", "applications": ["遠距醫療影像判讀：偏鄉地區醫療資源有限，透過SSS技術，即使只有少量標記數據，也能訓練出準確的AI模型輔助醫生判讀X光片、CT掃描等影像，及早發現疾病。", "AI輔助手術導航：在手術過程中，醫生可以利用SSS技術快速分析患者的MRI影像，即時生成精確的器官和病灶分割圖，幫助醫生更精準地定位和切除病灶，減少手術風險。", "個人化健康管理：未來，SSS技術可以應用於穿戴式裝置或手機App，分析用戶上傳的醫療影像（如皮膚照片、眼底照片），進行初步的健康風險評估，提供個人化的健康建議。"], "pitch": "各位投資人，我們正站在醫學影像AI的黃金交叉點！SSS技術，不僅解決了醫學影像標註數據稀缺的痛點，更充分釋放了Vision Foundation Model在醫療領域的潛力。想像一下，全球醫療機構每年產生海量的未標記影像數據，SSS就像一把金鑰匙，能將這些沉睡的數據轉化為強大的AI診斷能力。我們的技術不僅能提升診斷效率、降低醫療成本，更能加速新藥研發、實現精準醫療。我們預計，未來五年內，SSS將成為醫學影像AI市場的關鍵技術，搶佔先機者，必將贏得豐厚回報！我們團隊擁有深厚的AI技術積累和豐富的醫療領域經驗，期待與各位攜手，共創醫學影像AI的新紀元！", "audio": "docs/data/audios/2506.08949v1.wav"}
{"query": "Diffusion Model", "id": "2506.08677v1", "url": "http://arxiv.org/abs/2506.08677v1", "title": "MAMBO: High-Resolution Generative Approach for Mammography Images", "summary": "Mammography is the gold standard for the detection and diagnosis of breast\ncancer. This procedure can be significantly enhanced with Artificial\nIntelligence (AI)-based software, which assists radiologists in identifying\nabnormalities. However, training AI systems requires large and diverse\ndatasets, which are often difficult to obtain due to privacy and ethical\nconstraints. To address this issue, the paper introduces MAMmography ensemBle\nmOdel (MAMBO), a novel patch-based diffusion approach designed to generate\nfull-resolution mammograms. Diffusion models have shown breakthrough results in\nrealistic image generation, yet few studies have focused on mammograms, and\nnone have successfully generated high-resolution outputs required to capture\nfine-grained features of small lesions. To achieve this, MAMBO integrates\nseparate diffusion models to capture both local and global (image-level)\ncontexts. The contextual information is then fed into the final patch-based\nmodel, significantly aiding the noise removal process. This thoughtful design\nenables MAMBO to generate highly realistic mammograms of up to 3840x3840\npixels. Importantly, this approach can be used to enhance the training of\nclassification models and extended to anomaly detection. Experiments, both\nnumerical and radiologist validation, assess MAMBO's capabilities in image\ngeneration, super-resolution, and anomaly detection, highlighting its potential\nto enhance mammography analysis for more accurate diagnoses and earlier lesion\ndetection.", "authors": ["Milica Škipina", "Nikola Jovišić", "Nicola Dall'Asen", "Vanja Švenda", "Anil Osman Tur", "Slobodan Ilić", "Elisa Ricci", "Dubravko Ćulibrk"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:33:24.093007", "title_zh": "MAMBO：用於乳房X光影像的高解析度生成方法", "summary_zh": "乳房X光攝影是乳癌檢測的黃金標準。為解決AI訓練數據不足的問題，本研究提出MAMBO，一種基於擴散模型的新穎方法，用於生成完整解析度的乳房X光影像。MAMBO整合了多個擴散模型，捕捉局部和全域的上下文資訊，生成高達3840x3840像素的逼真乳房X光片。這種方法能有效提升分類模型的訓練，並擴展至異常檢測。實驗證明，MAMBO在影像生成、超解析度及異常檢測方面表現出色，有潛力提升乳房X光分析的準確性，並提早發現病灶。", "applications": ["想像一下，未來在家就能使用手機App，上傳乳房X光片進行初步AI分析，及早發現潛在風險，省去奔波醫院的時間。", "醫療資源匱乏的偏遠地區，可以利用MAMBO生成的合成數據，訓練AI模型，提升當地醫療人員的診斷能力。", "醫學研究人員可以利用MAMBO生成大量不同病灶的乳房X光片，加速新藥開發和治療方案的驗證。"], "pitch": "各位投資人，我們正處於AI醫療革命的風口浪尖！MAMBO技術不僅能生成高擬真乳房X光影像，解決數據匱乏的難題，更重要的是，它能賦能AI，提升乳癌檢測的準確性和效率。想像一下，一個能及早發現微小病灶的AI系統，能拯救多少生命！MAMBO的應用前景廣闊，從遠程醫療、AI輔助診斷，到新藥開發，都蘊藏著巨大的商業價值。我們不僅僅在銷售軟體，我們是在投資未來，一個更健康、更智能的未來。現在加入我們，共同打造AI醫療的新紀元，回報將遠超您的想像！", "audio": "docs/data/audios/2506.08677v1.wav"}
{"query": "AI", "id": "2506.08945v1", "url": "http://arxiv.org/abs/2506.08945v1", "title": "Who is using AI to code? Global diffusion and impact of generative AI", "summary": "Generative coding tools promise big productivity gains, but uneven uptake\ncould widen skill and income gaps. We train a neural classifier to spot\nAI-generated Python functions in 80 million GitHub commits (2018-2024) by\n200,000 developers and track how fast--and where--these tools take hold. By\nDecember 2024, AI wrote an estimated 30.1% of Python functions from U.S.\ncontributors, versus 24.3% in Germany, 23.2% in France, 21.6% in India, 15.4%\nin Russia and 11.7% in China. Newer GitHub users use AI more than veterans,\nwhile male and female developers adopt at similar rates. Within-developer\nfixed-effects models show that moving to 30% AI use raises quarterly commits by\n2.4%. Coupling this effect with occupational task and wage data puts the annual\nvalue of AI-assisted coding in the United States at $9.6-$14.4 billion, rising\nto $64-$96 billion if we assume higher estimates of productivity effects\nreported by randomized control trials. Moreover, generative AI prompts learning\nand innovation, leading to increases in the number of new libraries and library\ncombinations that programmers use. In short, AI usage is already widespread but\nhighly uneven, and the intensity of use, not only access, drives measurable\ngains in output and exploration.", "authors": ["Simone Daniotti", "Johannes Wachs", "Xiangnan Feng", "Frank Neffke"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:37:04.495639", "title_zh": "誰在使用AI編碼？生成式AI的全球擴散與影響", "summary_zh": "這項研究分析了GitHub上8000萬個提交紀錄，發現生成式AI正在快速滲透程式碼開發。截至2024年底，美國開發者撰寫的Python函數中，約有30.1%由AI生成，領先其他國家。研究顯示，新進開發者比資深開發者更常使用AI，且男女開發者的採用率相似。更重要的是，AI的使用提升了開發效率，估計每年為美國帶來數十億美元的經濟價值。此外，AI還能促進學習與創新，增加程式設計師使用的程式庫種類。總之，AI編碼已廣泛應用，但使用程度不均，且使用強度而非僅僅是存取，才能真正推動產出和探索的顯著提升。", "applications": ["1. **新手程式設計師的救星：** 想像一下，程式新手再也不用害怕複雜的語法。AI編碼工具就像一位耐心的導師，能自動生成程式碼片段，讓學習曲線更平緩，更快上手。", "2. **效率提升神器：** 對於資深工程師來說，AI編碼工具可以自動完成重複性的工作，例如撰寫基礎程式碼或測試案例，讓他們能專注於更具挑戰性和創造性的任務，大幅提升工作效率。", "3. **個人化學習助手：** 學生可以使用AI編碼工具來生成不同的程式碼範例，從而更深入地理解程式設計概念。AI可以根據學生的學習進度和偏好，提供客製化的學習體驗。"], "pitch": "各位創投夥伴，我們正在見證一場程式碼革命！這項研究證明，生成式AI不僅是個噱頭，而是程式開發的未來。想像一下，一個程式設計師的生產力提升數倍，整個軟體開發的週期縮短，新創企業能更快推出產品，大型企業也能更靈活地應對市場變化。我們的技術不僅能追蹤AI編碼的使用情況，更能深入分析其對生產力、創新和經濟的影響。這代表著巨大的商業機會！我們可以開發更智能的AI編碼工具，提供客製化的程式碼生成服務，甚至建立一個AI驅動的程式碼市場。未來，AI編碼將成為各行各業的基礎設施，從醫療保健到金融，都將受益於AI編碼帶來的效率提升和創新加速。現在投資，就能搶佔先機，成為這場革命的領航者，共同打造一個AI賦能的程式碼未來！", "audio": "docs/data/audios/2506.08945v1.wav"}
{"query": "Foundation Model", "id": "2506.08936v1", "url": "http://arxiv.org/abs/2506.08936v1", "title": "BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models", "summary": "We present BioLangFusion, a simple approach for integrating pre-trained DNA,\nmRNA, and protein language models into unified molecular representations.\nMotivated by the central dogma of molecular biology (information flow from gene\nto transcript to protein), we align per-modality embeddings at the biologically\nmeaningful codon level (three nucleotides encoding one amino acid) to ensure\ndirect cross-modal correspondence. BioLangFusion studies three standard fusion\ntechniques: (i) codon-level embedding concatenation, (ii) entropy-regularized\nattention pooling inspired by multiple-instance learning, and (iii) cross-modal\nmulti-head attention -- each technique providing a different inductive bias for\ncombining modality-specific signals. These methods require no additional\npre-training or modification of the base models, allowing straightforward\nintegration with existing sequence-based foundation models. Across five\nmolecular property prediction tasks, BioLangFusion outperforms strong unimodal\nbaselines, showing that even simple fusion of pre-trained models can capture\ncomplementary multi-omic information with minimal overhead.", "authors": ["Amina Mollaysa", "Artem Moskale", "Pushpak Pati", "Tommaso Mansi", "Mangal Prakash", "Rui Liao"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:38:26.082676", "title_zh": "BioLangFusion：DNA、mRNA和蛋白質語言模型的多模態融合", "summary_zh": "BioLangFusion是一種整合DNA、mRNA和蛋白質預訓練語言模型的簡潔方法，旨在創建統一的分子表示。基於分子生物學中心法則（基因到轉錄本到蛋白質的信息流），我們在生物學上具有意義的密碼子層面（三個核苷酸編碼一個氨基酸）對齊了各模態的嵌入，確保直接的跨模態對應關係。BioLangFusion研究了三種標準融合技術：密碼子層面的嵌入串聯、受多實例學習啟發的熵正則化注意力池化，以及跨模態多頭注意力。這些方法無需額外的預訓練或修改基礎模型，即可與現有的基於序列的基礎模型直接整合。在五項分子特性預測任務中，BioLangFusion優於強大的單模態基線，表明即使是預訓練模型的簡單融合也能以最小的開銷捕獲互補的多組學信息。", "applications": ["個人化醫療：根據個人的基因、mRNA和蛋白質數據，更精準地預測藥物反應，從而實現更有效的治療。", "疾病診斷：通過分析患者的多組學數據，早期發現疾病的生物標記，提高診斷的準確性和效率。", "新藥開發：加速新藥的篩選和設計過程，通過模擬不同分子之間的相互作用，預測藥物的療效和安全性。"], "pitch": "各位投資人，想像一下，如果我們能像解讀語言一樣解讀生命密碼，精準預測疾病、設計藥物，甚至創造全新的生物技術，那將會是怎樣的變革？BioLangFusion正是這樣一個顛覆性的技術。它整合了DNA、mRNA和蛋白質等多組學數據，打造了一個強大的AI模型，能夠以前所未有的精度預測分子特性。這不僅僅是技術突破，更是開啟了個人化醫療、精準診斷和加速新藥開發的鑰匙。我們預計，BioLangFusion將在未來五年內徹底改變生物醫藥行業，創造數十億美元的市場價值。現在加入我們，共同投資BioLangFusion，搶占生物醫藥AI的制高點，共同見證生命科學的下一次重大突破！", "audio": "docs/data/audios/2506.08936v1.wav"}
{"query": "Diffusion Model", "id": "2506.08632v1", "url": "http://arxiv.org/abs/2506.08632v1", "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping", "summary": "Recent advancements in generative models have revolutionized video synthesis\nand editing. However, the scarcity of diverse, high-quality datasets continues\nto hinder video-conditioned robotic learning, limiting cross-platform\ngeneralization. In this work, we address the challenge of swapping a robotic\narm in one video with another: a key step for crossembodiment learning. Unlike\nprevious methods that depend on paired video demonstrations in the same\nenvironmental settings, our proposed framework, RoboSwap, operates on unpaired\ndata from diverse environments, alleviating the data collection needs. RoboSwap\nintroduces a novel video editing pipeline integrating both GANs and diffusion\nmodels, combining their isolated advantages. Specifically, we segment robotic\narms from their backgrounds and train an unpaired GAN model to translate one\nrobotic arm to another. The translated arm is blended with the original video\nbackground and refined with a diffusion model to enhance coherence, motion\nrealism and object interaction. The GAN and diffusion stages are trained\nindependently. Our experiments demonstrate that RoboSwap outperforms\nstate-of-the-art video and image editing models on three benchmarks in terms of\nboth structural coherence and motion consistency, thereby offering a robust\nsolution for generating reliable, cross-embodiment data in robotic learning.", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Dong Chen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:39:54.486327", "title_zh": "RoboSwap：一個基於GAN驅動的影片擴散框架，用於無監督機器手臂替換", "summary_zh": "RoboSwap 是一個創新的影片編輯框架，旨在解決機器人學習中數據稀缺的問題。它利用生成對抗網路（GAN）和擴散模型，實現了在影片中無監督地替換機器手臂。首先，RoboSwap 將機器手臂從背景中分割出來，並使用 GAN 將一個機器手臂轉換為另一個。然後，將轉換後的機器手臂與原始影片背景融合，並使用擴散模型來增強影片的連貫性、運動真實感和物體交互。實驗結果表明，RoboSwap 在結構連貫性和運動一致性方面優於現有的影片和圖像編輯模型，為機器人學習中生成可靠的跨平台數據提供了一個強大的解決方案。這降低了對大量配對影片資料的需求，加速機器人學習的進程。", "applications": ["想像一下，你可以用手機App輕鬆更換影片中的機器手臂，讓它看起來像是在執行不同的任務。比如，你想展示你的機器人手臂組裝家具的能力，但你只有它在組裝電子元件的影片，RoboSwap 就能幫你把影片中的電子元件替換成家具。", "在機器人教育領域，老師可以利用 RoboSwap 快速生成各種不同機器手臂執行任務的影片，讓學生更直觀地學習不同手臂的特性和應用，而無需實際購買和操作所有型號的機器手臂。", "製造業的工程師可以利用 RoboSwap 模擬不同機器手臂在生產線上的工作情況，優化生產流程，預先評估新手臂的性能，而無需進行昂貴的物理原型測試。"], "pitch": "RoboSwap 不僅僅是一個影片編輯工具，它是機器人學習領域的革命性技術！我們正在解決機器人數據稀缺這個長期存在的痛點，釋放機器人跨平台學習的巨大潛力。想像一下，一個機器人可以在虛擬環境中學習無數種技能，然後將這些技能無縫轉移到真實世界。RoboSwap 正是實現這一願景的關鍵。它能大幅降低機器人學習的成本和時間，加速機器人技術在各行各業的應用。我們的技術具有巨大的商業價值，從機器人教育、製造業到娛樂業，都有廣闊的市場前景。我們相信，RoboSwap 將成為機器人時代的 Adobe Photoshop，引領下一代機器人技術的發展。現在投資 RoboSwap，就是投資機器人技術的未來！", "audio": "docs/data/audios/2506.08632v1.wav"}
{"query": "AI", "id": "2506.08935v1", "url": "http://arxiv.org/abs/2506.08935v1", "title": "Can A Gamer Train A Mathematical Reasoning Model?", "summary": "While large language models (LLMs) have achieved remarkable performance in\nvarious tasks including mathematical reasoning, their development typically\ndemands prohibitive computational resources. Recent advancements have reduced\ncosts for training capable models, yet even these approaches rely on high-end\nhardware clusters. In this paper, we demonstrate that a single average gaming\nGPU can train a solid mathematical reasoning model, by integrating\nreinforcement learning and memory optimization techniques. Specifically, we\ntrain a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB\nmemory that achieves comparable or better performance on mathematical reasoning\nbenchmarks than models several times larger, in resource-constrained\nenvironments. Our results challenge the paradigm that state-of-the-art\nmathematical reasoning necessitates massive infrastructure, democratizing\naccess to high-performance AI research.\nhttps://github.com/shinandrew/YouronMath.", "authors": ["Andrew Shin"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:25:12.984985", "title_zh": "遊戲玩家也能訓練數學推理模型嗎？", "summary_zh": "本研究展示，僅需一張普通的遊戲級GPU，透過強化學習和記憶體優化技術，就能訓練出一個強大的數學推理模型。我們在配備16GB記憶體的RTX 3080 Ti上訓練了一個15億參數的模型，在資源有限的環境下，其數學推理能力可與規模更大的模型相媲美甚至超越。這項成果挑戰了頂尖數學推理必須依賴龐大基礎設施的傳統觀念，有助於普及高效能AI研究。", "applications": ["1. 孩子在家用電腦就能訓練AI家教：不必花大錢購買昂貴的伺服器，只要用家裡玩遊戲的電腦，就能訓練出能解答數學題目的AI，輔導孩子學習。", "2. 偏鄉學校也能有AI輔助教學：資源不足的學校，也能利用現有的電腦設備，訓練出針對性的AI模型，提升學生的學習效果。", "3. 個人開發者也能參與AI研究：降低了AI研究的門檻，讓更多有興趣的個人開發者，也能參與到數學推理AI的研究與開發中。"], "pitch": "各位投資人，想像一下，AI不再是巨頭的遊戲！我們突破性技術讓AI數學推理模型的訓練成本大幅降低，只需一張遊戲顯卡，就能達到甚至超越過去需要昂貴伺服器才能實現的性能。這意味著什麼？\n\n首先，市場潛力巨大！教育科技市場正蓬勃發展，我們的技術能讓每個家庭、每所學校都能擁有客製化的AI家教，大幅降低教育成本，提升學習效率。其次，我們將顛覆AI研究的格局，讓更多個人開發者和小型團隊參與進來，加速AI技術的創新與應用。更重要的是，這項技術可以應用於金融建模、科學研究等領域，創造巨大的商業價值。我們正在打造一個更普惠、更民主化的AI未來，現在加入，您將成為這場變革的領航者！", "audio": "docs/data/audios/2506.08935v1.wav"}
{"query": "Foundation Model", "id": "2506.08902v1", "url": "http://arxiv.org/abs/2506.08902v1", "title": "Intention-Conditioned Flow Occupancy Models", "summary": "Large-scale pre-training has fundamentally changed how machine learning\nresearch is done today: large foundation models are trained once, and then can\nbe used by anyone in the community (including those without data or compute\nresources to train a model from scratch) to adapt and fine-tune to specific\ntasks. Applying this same framework to reinforcement learning (RL) is appealing\nbecause it offers compelling avenues for addressing core challenges in RL,\nincluding sample efficiency and robustness. However, there remains a\nfundamental challenge to pre-train large models in the context of RL: actions\nhave long-term dependencies, so training a foundation model that reasons across\ntime is important. Recent advances in generative AI have provided new tools for\nmodeling highly complex distributions. In this paper, we build a probabilistic\nmodel to predict which states an agent will visit in the temporally distant\nfuture (i.e., an occupancy measure) using flow matching. As large datasets are\noften constructed by many distinct users performing distinct tasks, we include\nin our model a latent variable capturing the user intention. This intention\nincreases the expressivity of our model, and enables adaptation with\ngeneralized policy improvement. We call our proposed method\nintention-conditioned flow occupancy models (InFOM). Comparing with alternative\nmethods for pre-training, our experiments on $36$ state-based and $4$\nimage-based benchmark tasks demonstrate that the proposed method achieves $1.8\n\\times$ median improvement in returns and increases success rates by $36\\%$.\nWebsite: https://chongyi-zheng.github.io/infom Code:\nhttps://github.com/chongyi-zheng/infom", "authors": ["Chongyi Zheng", "Seohong Park", "Sergey Levine", "Benjamin Eysenbach"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:26:28.700147", "title_zh": "意圖條件式流動佔據模型", "summary_zh": "本研究提出一種名為「意圖條件式流動佔據模型」（InFOM）的強化學習預訓練方法。InFOM利用流動匹配技術，預測智能體在未來將訪問哪些狀態，並通過潛在變量捕捉不同使用者的意圖。這種方法能有效處理強化學習中長期依賴性的問題，提高樣本效率和穩健性。實驗結果顯示，InFOM在多項基準測試中，回報中位數提高了1.8倍，成功率提升了36%。InFOM為強化學習的大規模預訓練提供了一條新途徑，有助於開發更強大、更通用的智能體。", "applications": ["自動駕駛：InFOM可以幫助自動駕駛系統預測其他車輛或行人的意圖，從而做出更安全、更有效的決策，例如預測行人是否會突然衝出馬路，提前減速避讓。", "智慧醫療：InFOM可以用於輔助醫生制定治療方案，預測患者對不同治療方案的反應，並根據患者的個體差異進行個性化治療。", "機器人流程自動化（RPA）：InFOM可以讓機器人更好地理解使用者的意圖，從而更靈活、更智能地執行各種任務，例如自動處理電子郵件、填寫表單等。"], "pitch": "各位創投先進，我們正處於AI的黃金時代！想像一下，如果機器能像人一樣，不僅學習技能，更能理解『意圖』，會發生什麼？我們的「意圖條件式流動佔據模型」（InFOM）正是實現這一點的關鍵技術。它讓機器在強化學習中，能預測未來、理解使用者意圖，大幅提升學習效率和決策能力。這意味著，從自駕車到醫療診斷，再到工業自動化，InFOM擁有無限潛力！\n\n試想：未來的自駕車不再只是遵守交通規則，更能預測行人意圖，避免事故；醫療AI不再只是分析數據，更能理解患者需求，提供個性化治療；工廠機器人不再只是重複動作，更能理解生產目標，自主優化流程。InFOM將賦予機器前所未有的智能，徹底顛覆各行各業！\n\n我們已經證明InFOM在多個基準測試中表現出色，領先現有技術。現在，我們需要您的資金，將InFOM推向市場，打造一個全新的AI時代。這不僅僅是一項投資，更是一次參與AI革命的機會！讓我們一起攜手，讓InFOM成為AI領域的下一個獨角獸！", "audio": "docs/data/audios/2506.08902v1.wav"}
{"query": "Diffusion Model", "id": "2506.08617v1", "url": "http://arxiv.org/abs/2506.08617v1", "title": "Diffusion model for analyzing quantum fingerprints in conductance fluctuation", "summary": "A conditional diffusion model has been developed to analyze intricate\nconductance fluctuations called universal conductance fluctuations or quantum\nfingerprints appearing in quantum transport phenomena. The model reconstructs\nimpurity arrangements and quantum interference patterns in nanometals by using\nmagnetoconductance data, providing a novel approach to analyze complex data\nbased on machine learning. In addition, we visualize the attention weights in\nthe model, which efficiently extract information on the non-local correlation\nof the electron wave functions, and the score functions, which represent the\nforce fields in the wave-function space.", "authors": ["Naoto Yokoi", "Yuki Tanaka", "Yukito Nonaka", "Shunsuke Daimon", "Junji Haruyama", "Eiji Saitoh"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:27:18.184257", "title_zh": "用於分析電導波動中量子指紋的擴散模型", "summary_zh": "本研究開發了一種條件擴散模型，用於分析量子傳輸現象中複雜的電導波動，即所謂的通用電導波動或量子指紋。該模型利用磁電導數據重建奈米金屬中的雜質排列和量子干涉圖案，為基於機器學習的複雜數據分析提供了一種新穎方法。此外，我們可視化模型中的注意力權重，有效地提取電子波函數的非局部相關性信息，以及表示波函數空間中力場的評分函數。", "applications": ["想像一下，醫生可以利用這項技術，透過簡單的電導測量，就能了解奈米級藥物在人體內的分布和作用方式，從而更精準地用藥。", "工廠可以利用這項技術，監測奈米材料的生產過程，即時發現並修正瑕疵，確保產品品質穩定可靠。", "科學家可以利用這項技術，更深入地了解量子世界的奧秘，例如超導材料的特性，進而開發出更高效能的電子元件。"], "pitch": "各位投資人，我們帶來一項顛覆性的技術，它能解鎖量子世界的秘密！想像一下，我們能像掃描指紋一樣，精準掌握奈米材料的內部結構。這項基於擴散模型的量子指紋分析技術，不僅能應用於材料科學、醫學診斷，更能催生新一代的量子電腦和高效能電子元件。我們預期，隨著量子技術的發展，這項技術將成為各行各業不可或缺的工具，市場潛力無可限量。現在加入我們，一起開創量子科技的新紀元！", "audio": "docs/data/audios/2506.08617v1.wav"}
{"query": "AI", "id": "2506.09988v1", "url": "http://arxiv.org/abs/2506.09988v1", "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "published_date": "2025-06-11", "timestamp": "2025-06-12T03:51:41.981962", "title_zh": "EditInspector：文本引導圖像編輯評估基準", "summary_zh": "隨著生成式AI的快速發展，文本引導圖像編輯技術日益普及。然而，我們缺乏一個完善的框架來驗證和評估這些編輯的品質。為此，我們推出了EditInspector，一個基於人工標註的新基準，用於評估文本引導圖像編輯。透過EditInspector，我們評估了當前最先進的視覺和語言模型在各個維度上的編輯評估能力，包括準確性、偽影檢測、視覺品質、與場景的融合度、常識一致性以及描述編輯引起變化的能力。研究發現，現有模型在全面評估編輯方面存在困難，並且在描述變化時經常產生幻覺。為了解決這些問題，我們提出了兩種新方法，它們在偽影檢測和差異描述生成方面均優於現有模型。", "applications": ["**線上購物：**想像一下，你想買一件紅色外套，但網站上只有藍色的。你可以用文字描述「將外套顏色改成紅色」，AI就能自動幫你換顏色，讓你更清楚看到穿起來的效果，減少買錯的機會。", "**社交媒體濾鏡：**現在的濾鏡只能簡單美顏或加特效。未來，你可以用文字描述，例如「讓我的眼睛更大更有神」，AI就能根據你的描述，精準調整你的五官，打造獨一無二的個人化濾鏡。", "**遊戲角色客製化：**在遊戲中，你可以用文字描述你想要的角色外觀，例如「讓他的頭髮變成銀白色，眼睛變成藍色」，AI就能立即生成符合你描述的角色外觀，讓你的遊戲體驗更加個性化。"], "pitch": "各位投資人，我們正處於AI圖像編輯革命的風口浪尖！EditInspector不僅是一個基準，更是一個加速器，它將推動文本引導圖像編輯技術的發展，並催生出巨大的商業價值。想像一下，未來人人都能輕鬆創造出精美的圖像內容，無需專業技能，這將顛覆設計、行銷、電商等各個產業。我們的技術能有效解決現有AI編輯的缺陷，提高編輯品質和準確性，具有極高的市場競爭力。我們預計，在未來五年內，文本引導圖像編輯市場將達到數十億美元的規模，而EditInspector將成為這個市場的關鍵基礎設施。現在投資我們，您將有機會參與這場AI圖像編輯的革命，共同創造一個充滿創意和可能性的未來！我們不僅僅是在開發技術，更是在打造一個全新的視覺表達生態系統，邀請您與我們一同開創這個新時代！", "audio": "docs/data/audios/2506.09988v1.wav"}
{"query": "Foundation Model", "id": "2506.09982v1", "url": "http://arxiv.org/abs/2506.09982v1", "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation", "summary": "Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.", "authors": ["Zijie Wu", "Chaohui Yu", "Fan Wang", "Xiang Bai"], "published_date": "2025-06-11", "timestamp": "2025-06-12T03:52:55.119943", "title_zh": "AnimateAnyMesh：一個用於文本驅動通用網格動畫的前饋式4D基礎模型", "summary_zh": "AnimateAnyMesh是一個創新的前饋式框架，能快速根據文字描述生成任意3D網格動畫。它利用獨特的DyMeshVAE架構，有效壓縮和重建動態網格序列，同時分離空間和時間特徵，並保留局部拓撲結構。透過在壓縮潛在空間中使用基於修正流的訓練策略，實現高質量文本條件生成。此外，我們還創建了包含超過400萬個帶有文本註釋的多樣化動態網格序列的DyMesh數據集。實驗結果表明，該方法能在幾秒鐘內生成語義準確且時間連貫的網格動畫，在質量和效率上均顯著優於現有方法。我們的研究是使4D內容創建更易於訪問和實用的重要一步。", "applications": ["想像一下，你可以用手機拍一張靜態雕像的照片，然後輸入文字「跳舞」，雕像就能立刻在你手機螢幕上跳起舞來！", "遊戲開發者再也不用花費大量時間手動製作角色動畫，只要輸入指令「英雄奔跑」，就能自動生成逼真的跑步動作。", "建築師可以輸入「大樓在地震中搖晃」，立即模擬建築物在地震中的反應，以便更好地設計抗震結構。"], "pitch": "各位投資人，我們正在打造的是4D內容生成的未來！AnimateAnyMesh不僅是一個技術突破，更是一個潛力無限的市場金礦。想像一下，一個讓任何人都能輕鬆創造逼真3D動畫的世界，這將顛覆遊戲、電影、設計、教育等無數產業。我們的技術能大幅降低動畫製作門檻和成本，讓創意無限釋放。未來，我們可以將AnimateAnyMesh整合到元宇宙平台，讓用戶創造個性化虛擬化身和互動內容；授權給電商平台，讓消費者更直觀地預覽商品；甚至應用於醫療領域，模擬手術過程，提升培訓效果。我們擁有的不僅是領先的技術，更是創造一個全新內容生態的願景。現在加入我們，一起引領4D內容革命，共同分享這個千億級市場的巨大紅利！", "audio": "docs/data/audios/2506.09982v1.wav"}
{"query": "Diffusion Model", "id": "2506.09993v1", "url": "http://arxiv.org/abs/2506.09993v1", "title": "Text-Aware Image Restoration with Diffusion Models", "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "published_date": "2025-06-11", "timestamp": "2025-06-12T03:54:25.769230", "title_zh": "基於擴散模型的文本感知圖像修復", "summary_zh": "本研究提出「文本感知圖像修復」(TAIR) 任務，旨在同時恢復圖像的視覺內容和文本的準確性。現有基於擴散模型的圖像修復方法，在處理自然圖像時表現出色，但在還原圖像中的文字區域時，經常產生看似合理但錯誤的文字圖樣，稱為「文本圖像幻覺」。為解決此問題，我們建立了一個包含10萬張高質量場景圖像的大規模基準測試集SA-Text，並提出一個名為TeReDiff的多任務擴散框架，將擴散模型的內部特徵整合到文本檢測模塊中，實現聯合訓練，從而提取豐富的文本表示，並將其用作後續去噪步驟的提示。實驗結果表明，我們的TAIR方法在文本識別準確性方面顯著優於現有的圖像修復方法。", "applications": ["**智能交通監控：** 想像一下，透過AI修復模糊的車牌，即使在惡劣天氣或光線不足的情況下，也能清晰辨識違規車輛或追蹤失竊車輛，大幅提升執法效率。", "**歷史文獻數位化：** 將老舊、破損的古籍或文件圖像修復，使模糊的文字變得清晰可讀，讓珍貴的歷史資料得以保存和研究，重現歷史的樣貌。", "**安全監控系統：** 還原監視器畫面中模糊的文字，例如商店招牌或公告欄，協助警方快速掌握案發現場的關鍵資訊，提升破案率。"], "pitch": "各位投資人，我們團隊帶來的是革命性的「文本感知圖像修復」技術，簡稱TAIR。這項技術不僅能修復模糊圖像，更厲害的是，它能精準還原圖像中的文字資訊，解決了傳統圖像修復技術的痛點。試想一下，未來無人車需要清晰辨識路標，執法單位需要還原監視器畫面中的關鍵資訊，歷史學家需要解讀古籍中的模糊文字，這些都離不開TAIR。我們已建立大規模的基準測試集，並開發出領先業界的TeReDiff框架，實驗數據證明我們的技術遠勝於現有方案。我們預計，TAIR將在智能交通、安全監控、文獻數位化等領域掀起巨大變革，成為AI圖像處理領域的黃金標準。現在加入我們，您將參與一個極具潛力的市場，共同開創AI圖像修復的新紀元！", "audio": "docs/data/audios/2506.09993v1.wav"}
{"query": "AI", "id": "2506.09985v1", "url": "http://arxiv.org/abs/2506.09985v1", "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "published_date": "2025-06-11", "timestamp": "2025-06-12T06:37:55.238033", "title_zh": "V-JEPA 2：自我監督式影片模型實現理解、預測與規劃", "summary_zh": "V-JEPA 2是一種透過觀看大量網路影片，學習理解世界並採取行動的AI模型。它結合網路影片資料和少量機器人互動資料，預訓練出一個能理解、預測和規劃的架構。在動作理解和人類行為預測上，V-JEPA 2都表現出色。與大型語言模型結合後，它在影片問答任務上也達到領先水準。更厲害的是，V-JEPA 2還能應用於機器人規劃任務，讓機器人在新環境中，無需任何訓練或獎勵，就能完成拾取和放置物體的任務。這項技術展現了自我監督學習在現實世界中的巨大潛力。", "applications": ["智慧家庭：V-JEPA 2可以讓你的掃地機器人更聰明，不只會掃地，還能辨識障礙物、預測你的行走路線，甚至在你快跌倒時及時扶你一把！", "自動駕駛：有了V-JEPA 2，自駕車不只能看到紅綠燈，更能預測行人意圖、判斷路況變化，真正做到安全又聰明的自動駕駛。", "遠端醫療：醫生可以透過V-JEPA 2控制遠端的機器人手臂，進行精準的手術或檢查，即使身在偏遠地區也能獲得最好的醫療照護。"], "pitch": "各位投資人，想像一下，一個AI能像人類一樣，透過觀察學習、理解世界，並做出合理的預測和規劃。V-JEPA 2正是這樣的突破性技術！它不僅在學術界取得領先成果，更具備巨大的商業潛力。試想，將V-JEPA 2應用於工業自動化，可以大幅提升生產效率和品質；應用於無人倉儲，可以實現更智慧化的物流管理；甚至應用於虛擬實境，可以打造更逼真、更具互動性的沉浸式體驗。我們相信，V-JEPA 2將引領下一波AI革命，成為各行各業不可或缺的核心技術。現在投資V-JEPA 2，就是投資未來！我們預期在五年內，V-JEPA 2相關應用市場規模將達到數十億美元，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.09985v1.wav"}
{"query": "Foundation Model", "id": "2506.09883v1", "url": "http://arxiv.org/abs/2506.09883v1", "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "published_date": "2025-06-11", "timestamp": "2025-06-12T06:39:27.358869", "title_zh": "利用幾何蒸餾微調具備3D感知能力的視覺語言模型", "summary_zh": "現有的視覺語言模型在處理視覺和語言任務上表現出色，但在理解3D空間結構方面存在根本限制。我們提出一種名為「幾何蒸餾」的輕量級、無標註微調框架，將人類啟發的幾何線索注入到預訓練的視覺語言模型中，且不修改其架構。透過從現成的3D基礎模型（例如MASt3R、VGGT）中提取稀疏對應關係、相對深度關係和密集成本體積，我們的模型能夠塑造出具備幾何感知能力的表示，同時保持與自然圖像文本輸入的兼容性。在3D視覺語言推理和3D感知基準測試中，我們的模型始終優於先前的方法，以更低的計算成本實現了改進的3D空間推理。", "applications": ["**智慧導航：** 想像一下，你的手機不只知道你面前有什麼，還能理解這些東西的相對距離和空間關係。這樣，它就能更準確地導航，告訴你『走過紅色的郵筒後，左轉進入第二個巷口』，而不是模糊地說『往前走一段路後左轉』。", "**虛擬試穿/試用：** 在網路上買衣服或家具時，你可以透過手機鏡頭看到這些東西在你身上的效果，或者擺放在你家裡的样子，而且非常真實。這個技術可以讓你更放心地購物，減少退貨的麻煩。", "**AR遊戲互動：** AR遊戲可以更真實地與你的周圍環境互動。例如，遊戲中的怪物可以躲在你的桌子後面，而不是漂浮在半空中。這能讓遊戲體驗更有沉浸感和趣味性。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它能賦予AI如同人類般的3D空間感知能力。現有的AI在理解圖像和文字方面表現出色，但它們對真實世界的空間結構理解仍然有限。我們的「幾何蒸餾」技術，就像為AI裝上了一雙『3D眼鏡』，讓它們能夠真正『看懂』周圍的世界。這項技術的潛在商業價值巨大。想像一下，自動駕駛汽車可以更精準地避開障礙物，機器人可以更靈巧地執行任務，AR/VR體驗可以更加逼真。更重要的是，我們的技術成本低廉，可以輕鬆地整合到現有的AI系統中。我們相信，這項技術將會徹底改變人機互動的方式，並在自動駕駛、機器人、AR/VR等領域帶來巨大的商業機會。現在投資我們，您將成為這場變革的先驅者，共同開創AI的新時代！", "audio": "docs/data/audios/2506.09883v1.wav"}
{"query": "Diffusion Model", "id": "2506.09955v1", "url": "http://arxiv.org/abs/2506.09955v1", "title": "Canonical Latent Representations in Conditional Diffusion Models", "summary": "Conditional diffusion models (CDMs) have shown impressive performance across\na range of generative tasks. Their ability to model the full data distribution\nhas opened new avenues for analysis-by-synthesis in downstream discriminative\nlearning. However, this same modeling capacity causes CDMs to entangle the\nclass-defining features with irrelevant context, posing challenges to\nextracting robust and interpretable representations. To this end, we identify\nCanonical LAtent Representations (CLAReps), latent codes whose internal CDM\nfeatures preserve essential categorical information while discarding\nnon-discriminative signals. When decoded, CLAReps produce representative\nsamples for each class, offering an interpretable and compact summary of the\ncore class semantics with minimal irrelevant details. Exploiting CLAReps, we\ndevelop a novel diffusion-based feature-distillation paradigm, CaDistill. While\nthe student has full access to the training set, the CDM as teacher transfers\ncore class knowledge only via CLAReps, which amounts to merely 10 % of the\ntraining data in size. After training, the student achieves strong adversarial\nrobustness and generalization ability, focusing more on the class signals\ninstead of spurious background cues. Our findings suggest that CDMs can serve\nnot just as image generators but also as compact, interpretable teachers that\ncan drive robust representation learning.", "authors": ["Yitao Xu", "Tong Zhang", "Ehsan Pajouheshgar", "Sabine Süsstrunk"], "published_date": "2025-06-11", "timestamp": "2025-06-12T06:41:18.294383", "title_zh": "條件擴散模型中的典型潛在表示", "summary_zh": "條件擴散模型在生成任務中表現出色，但容易將類別特徵與無關背景糾纏，影響表示的穩健性和可解釋性。本研究提出典型潛在表示（CLAReps），它能保留關鍵類別信息，同時捨棄無關信號。解碼後，CLAReps能產生具代表性的類別樣本，簡潔地呈現核心語義，減少不必要的細節。我們基於CLAReps開發了名為CaDistill的特徵蒸餾方法，讓學生模型僅透過CLAReps（僅佔訓練數據的10%）學習，大幅提升對抗性魯棒性和泛化能力。這表明條件擴散模型不僅能生成圖像，還能作為精簡且可解釋的教師，驅動穩健的表示學習。", "applications": ["**AI藝術創作助手：** 輸入簡單的文字描述，就能生成特定風格或主題的圖像，且能有效避免生成不相關的背景元素，讓使用者更精準地控制創作內容，例如：輸入「貓咪坐在窗邊」，就能生成一張只有貓咪和窗戶的圖像，而不會出現雜亂的房間擺設。", "**醫療影像分析：** 在X光或MRI影像中，自動識別並突出顯示病灶區域，同時抑制正常組織的干擾，幫助醫生更快速、準確地診斷疾病，例如：在肺部X光片中，精準識別出腫瘤，並忽略肋骨的影響。", "**產品設計：** 根據使用者提供的關鍵需求，生成多種產品設計方案，並能有效避免生成不符合需求的設計元素，加速產品原型設計流程，例如：設計一款「輕便且防水的背包」，系統能生成多種符合條件的背包設計，而不會出現不防水的材質或過重的設計。"], "pitch": "各位投資人，我們正處於AI圖像生成技術的黃金時代，但現有模型往往產生過於複雜、難以控制的結果。想像一下，如果我們能像控制樂高積木一樣，精準地操控AI生成圖像的每一個細節，那將會開啟怎樣的商業潛力？\n\n我們的CLAReps技術，正是實現這一目標的關鍵。它就像一個AI圖像的「基因編輯器」，能讓我們精準地提取和重組圖像的核心特徵，生成高度可控、高度定制化的內容。\n\n這意味著，我們可以將AI圖像生成技術應用於更廣泛的領域：個性化廣告、定制化教育內容、甚至是虛擬世界的建設。想像一下，在元宇宙中，每個人都能輕鬆創建自己的專屬形象和場景，而無需掌握複雜的3D建模技術。這將是一個數十億美元的市場！\n\n更重要的是，我們的CaDistill特徵蒸餾方法，能大幅提升模型的魯棒性和泛化能力，使其在面對真實世界的複雜數據時，也能保持卓越的性能。這意味著，我們的技術不僅能生成漂亮的圖像，更能應用於自動駕駛、醫療診斷等對安全性要求極高的領域。\n\n我們相信，CLAReps技術將引領下一代AI圖像生成革命。現在加入我們，共同開創這個充滿無限可能的未來！", "audio": "docs/data/audios/2506.09955v1.wav"}
{"query": "AI", "id": "2506.09977v1", "url": "http://arxiv.org/abs/2506.09977v1", "title": "How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies", "summary": "Understanding how humans revise their beliefs in light of new information is\ncrucial for developing AI systems which can effectively model, and thus align\nwith, human reasoning. While theoretical belief revision frameworks rely on a\nset of principles that establish how these operations are performed, empirical\nevidence from cognitive psychology suggests that people may follow different\npatterns when presented with conflicting information. In this paper, we present\nthree comprehensive user studies showing that people consistently prefer\nexplanation-based revisions, i.e., those which are guided by explanations, that\nresult in changes to their belief systems that are not necessarily captured by\nclassical belief change theory. Our experiments systematically investigate how\npeople revise their beliefs with explanations for inconsistencies, whether they\nare provided with them or left to formulate them themselves, demonstrating a\nrobust preference for what may seem non-minimal revisions across different\ntypes of scenarios. These findings have implications for AI systems designed to\nmodel human reasoning or interact with humans, suggesting that such systems\nshould accommodate explanation-based, potentially non-minimal belief revision\noperators to better align with human cognitive processes.", "authors": ["Stylianos Loukas Vasileiou", "Antonio Rago", "Maria Vanina Martinez", "William Yeoh"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:53:19.277096", "title_zh": "人們如何修正不一致的信念？透過使用者研究檢驗人類的信念修正", "summary_zh": "本研究探討人類在面對新資訊時如何修正信念，這對於開發能有效模仿人類推理的人工智慧系統至關重要。研究發現，人們傾向於基於解釋來修正信念，即使這會導致與傳統信念變更理論不符的改變。透過三個使用者研究，我們發現人們更喜歡以解釋為基礎的修正，無論這些解釋是由系統提供還是由他們自己產生。這意味著，為了更好地與人類認知過程保持一致，AI系統應考慮納入基於解釋的、可能非最小化的信念修正運算。", "applications": ["**情境一：** 想像一個AI醫療診斷系統，當診斷結果與病患先前認知不符時，系統會提供詳細的解釋，例如說明罕見疾病的可能性或解釋檢測結果的誤差範圍，幫助病患理解並接受新的診斷。", "**情境二：** 在自動駕駛汽車中，如果汽車突然改變行駛路線，它會向乘客解釋原因，例如前方道路封閉或偵測到緊急狀況，讓乘客理解並信任汽車的決策。", "**情境三：** 線上教育平台可以根據學生的學習情況，提供個性化的學習建議。如果學生對某個概念理解有誤，系統不僅會指出錯誤，還會提供詳細的解釋和案例，幫助學生修正錯誤的觀念。"], "pitch": "各位投資人，我們正在開發一種革命性的人工智慧技術，它能像人類一樣思考和學習，關鍵在於理解並模擬人類的信念修正過程。傳統AI往往基於冷冰冰的邏輯，忽略了人類情感和解釋的重要性。我們的技術則不同，它能根據情境提供合理的解釋，讓人們更容易接受AI的決策，從而建立信任。試想一下，未來的AI助手不僅能完成任務，還能像一位智慧的顧問，提供建議並解釋背後的原因。這將顛覆醫療、教育、交通等各個領域。我們預計，這項技術將在五年內成為AI領域的標竿，帶來數十億美元的市場機會。現在加入我們，一起塑造AI的未來！", "audio": "docs/data/audios/2506.09977v1.wav"}
{"query": "Foundation Model", "id": "2506.09881v1", "url": "http://arxiv.org/abs/2506.09881v1", "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "authors": ["Siyu Chen", "Ting Han", "Chengzheng Fu", "Changshe Zhang", "Chaolei Wang", "Jinhe Su", "Guorong Cai", "Meiliu Wu"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:54:42.017818", "title_zh": "利用深度與語言實現開放詞彙領域泛化語義分割", "summary_zh": "本研究提出Vireo，一個創新的單階段框架，旨在解決開放詞彙領域泛化語義分割(OV-DGSS)問題。OV-DGSS的目標是在未見過的類別中生成像素級別的遮罩，同時保持在未見過領域中的穩健性，這對於惡劣條件下的自動駕駛等實際場景至關重要。Vireo基於凍結的視覺基礎模型(VFMs)，並通過深度VFMs整合場景幾何訊息，以提取領域不變的結構特徵。為了解決領域轉移下視覺和文本模態之間的差距，Vireo引入了GeoText Prompts、Coarse Mask Prior Embedding (CMPE) 和Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH) 三個關鍵組件。實驗結果表明，Vireo在領域泛化和開放詞彙識別方面均取得了最先進的性能，為多樣化和動態環境中的穩健視覺理解提供了一個統一且可擴展的解決方案。", "applications": ["1. 未來無人車能更精準辨識路上的障礙物，例如突然出現的小動物或是不熟悉的交通標誌，即使在雨天或濃霧中也能安全行駛。", "2. 智慧農業可以利用這項技術來辨識農田中不同的作物和雜草，幫助農民更有效率地噴灑農藥和施肥，提高農作物產量。", "3. 在醫療影像分析上，醫生能更準確地辨識X光片或斷層掃描中的病灶，像是腫瘤或骨折，提升診斷的準確性。"], "pitch": "各位投資人，想像一下，一個AI系統不僅能看懂這個世界，還能理解它！Vireo技術突破性地結合了深度感知和語言理解，讓機器在複雜、未知的環境中也能精準辨識物體。這意味著什麼？自動駕駛不再受限於特定路況，智慧城市管理更加高效，甚至醫療診斷也能大幅提升準確性。這項技術的應用潛力無窮，我們正在打造的是一個能真正理解世界的AI大腦！未來，我們可以將Vireo應用於虛擬實境、機器人導航、國防安全等領域，市場規模將是數十億美元級別。現在投資Vireo，您將站在AI革命的最前沿，共同開創一個更加智慧、安全、便捷的未來！", "audio": "docs/data/audios/2506.09881v1.wav"}
{"query": "Diffusion Model", "id": "2506.09932v1", "url": "http://arxiv.org/abs/2506.09932v1", "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:55:46.732901", "title_zh": "HadaNorm：透過均值中心轉換的擴散轉換器量化", "summary_zh": "擴散模型在圖像生成領域表現出色，但其高記憶體和計算需求限制了在資源有限設備上的應用。後訓練量化(PTQ)透過降低矩陣運算的位元寬度提供了解決方案。然而，標準PTQ方法難以處理離群值。本研究提出HadaNorm，一種新型線性轉換，透過在應用哈達瑪轉換之前正規化激活特徵通道來有效減輕離群值，從而實現更積極的激活量化。實驗證明，HadaNorm能有效降低轉換器各個組件的量化誤差，在效率和性能之間取得優於現有方法的平衡。", "applications": ["手機App修圖：即使手機硬體規格不高，也能快速生成高品質的圖片，讓修圖App更強大。", "智慧家居攝影機：在本地端即時處理影像，進行人臉辨識或物體偵測，無需將大量資料傳輸到雲端，保護隱私。", "無人機航拍：在電力有限的情況下，也能高效處理航拍影像，進行即時分析和地圖繪製。"], "pitch": "各位投資人，我們正處於AI圖像生成爆炸性成長的時代，但高昂的運算成本是阻礙其普及的關鍵。HadaNorm技術，能大幅降低擴散模型所需的硬體資源，讓AI圖像生成不再是雲端巨頭的專利，而是能真正走入每個人的生活。想像一下，未來的手機、無人機、甚至穿戴裝置，都能具備強大的圖像生成能力，這將開啟全新的應用場景和商業模式。我們的技術不僅優於現有方案，更具備極高的擴展性，未來可應用於其他AI模型，市場潛力無限。現在投資HadaNorm，就是投資AI的未來，讓我們一起打造更智慧、更普及的AI世界！", "audio": "docs/data/audios/2506.09932v1.wav"}
{"query": "AI", "id": "2506.09975v1", "url": "http://arxiv.org/abs/2506.09975v1", "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:29:20.805318", "title_zh": "當偵測失效時：微調模型產生類人社群媒體文本的力量", "summary_zh": "偵測AI生成的社群媒體文本極具挑戰，因文本短小且用語非正式。此問題至關重要，社群媒體是網路影響行動的重要攻擊載體，大量AI生成貼文可能被用來支持或反對特定政策。我們從威脅行為者的角度出發，創建包含超過50萬筆AI生成社群媒體貼文的數據集，涵蓋11個爭議性主題。研究表明，在攻擊者不公開其微調模型的情況下，偵測難度顯著增加。人類研究和消融實驗也證實了微調大型語言模型對各種偵測演算法的威脅。這項研究結果對所有偵測領域都有影響，因為微調是一種普遍適用且實際的大型語言模型使用方式。", "applications": ["**防止假新聞散播：**想像一下，這項技術能幫助社群平台自動標記或過濾掉由AI大量生成的、帶有特定政治立場的假新聞，讓大家看到的資訊更真實可靠。", "**保護品牌聲譽：**公司可以利用這項技術，監測網路上是否有大量AI生成的負面評論或攻擊，及時採取措施，保護品牌形象。", "**打擊網路詐騙：**偵測AI生成的詐騙訊息，例如假冒客服人員或親友的訊息，保護民眾免受詐騙。"], "pitch": "各位投資人，我們帶來的是一項劃時代的技術，它將顛覆AI內容偵測的遊戲規則。隨著AI生成內容的能力越來越強大，傳統的偵測方法已經捉襟見肘。我們的研究表明，通過微調模型，AI可以輕易生成難以與真人區分的社群媒體文本，這意味著網路上的虛假訊息、惡意攻擊和詐騙行為將更加猖獗。我們的技術能夠有效應對這種新型威脅，為社群平台、企業和政府提供強大的防禦能力。想像一下，未來每個社群平台、每個企業都需要我們的技術來保護自己免受AI生成內容的侵害，這將是一個數十億美元的市場！我們不僅能保護現有的網路生態，還將引領下一代AI內容安全技術的發展。現在加入我們，共同打造一個更安全、更真實的網路世界！", "audio": "docs/data/audios/2506.09975v1.wav"}
{"query": "Foundation Model", "id": "2506.09855v1", "url": "http://arxiv.org/abs/2506.09855v1", "title": "Foundation Model-Aided Deep Reinforcement Learning for RIS-Assisted Wireless Communication", "summary": "Reconfigurable intelligent surfaces (RIS) have emerged as a promising\ntechnology for enhancing wireless communication by dynamically controlling\nsignal propagation in the environment. However, their efficient deployment\nrelies on accurate channel state information (CSI), which leads to high channel\nestimation overhead due to their passive nature and the large number of\nreflective elements. In this work, we solve this challenge by proposing a novel\nframework that leverages a pre-trained open-source foundation model (FM) named\nlarge wireless model (LWM) to process wireless channels and generate versatile\nand contextualized channel embeddings. These embeddings are then used for the\njoint optimization of the BS beamforming and RIS configurations. To be more\nspecific, for joint optimization, we design a deep reinforcement learning (DRL)\nmodel to automatically select the BS beamforming vector and RIS phase-shift\nmatrix, aiming to maximize the spectral efficiency (SE). This work shows that a\npre-trained FM for radio signal understanding can be fine-tuned and integrated\nwith DRL for effective decision-making in wireless networks. It highlights the\npotential of modality-specific FMs in real-world network optimization.\nAccording to the simulation results, the proposed method outperforms the\nDRL-based approach and beam sweeping-based approach, achieving 9.89% and 43.66%\nhigher SE, respectively.", "authors": ["Mohammad Ghassemi", "Sara Farrag Mobarak", "Han Zhang", "Ali Afana", "Akram Bin Sediq", "Melike Erol-Kantarci"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:30:55.806908", "title_zh": "基於基礎模型的深度強化學習於RIS輔助無線通訊之應用", "summary_zh": "本研究提出一種創新的框架，利用預訓練的開源基礎模型（FM），即大型無線模型（LWM），來處理無線通道並生成多功能且情境化的通道嵌入。這些嵌入被用於聯合優化基地台波束成形和RIS配置。具體而言，我們設計了一個深度強化學習（DRL）模型，自動選擇基地台波束成形向量和RIS相位偏移矩陣，旨在最大化頻譜效率（SE）。模擬結果表明，該方法優於傳統的DRL和波束掃描方法，頻譜效率分別提高了9.89%和43.66%。此研究展示了針對無線電信號理解的預訓練FM可被微調並與DRL集成，從而在無線網路中實現有效的決策。", "applications": ["想像一下，在人潮擁擠的演唱會現場，手機訊號總是斷斷續續。有了這項技術，我們可以透過智慧控制牆面或燈具上的RIS，動態調整訊號傳輸路徑，確保每個人都能順暢地直播或分享精彩瞬間。", "在智慧工廠中，大量的感測器和機器人需要穩定的無線通訊。這項技術可以幫助工廠優化無線網路，減少訊號干擾，提高生產效率和安全性，實現真正的智慧製造。", "未來的無人機送貨，需要精準的定位和可靠的通訊。透過RIS輔助，我們可以克服建築物阻擋等問題，確保無人機能夠安全、準確地將包裹送到目的地。"], "pitch": "各位創投先進，我們正在打造無線通訊的未來！想像一下，5G/6G時代，網路流量爆炸性增長，現有的基地台建設速度遠遠跟不上需求。我們的技術，利用可重構智慧表面（RIS）和AI的力量，能大幅提升現有網路的容量和覆蓋範圍，無需大量新建基地台，成本效益極高！\n\n我們的創新之處在於，我們將大型無線模型（LWM）與深度強化學習（DRL）結合，讓網路能夠智慧地適應環境變化，動態優化訊號傳輸。這就像為無線網路裝上了一顆聰明的大腦！\n\n這項技術的潛在市場巨大。從智慧城市、工業物聯網到自動駕駛，各行各業都對更快速、更可靠的無線通訊有著迫切的需求。我們預計，未來五年內，RIS市場將呈現指數級增長，而我們將成為這個市場的領導者！\n\n我們不僅僅是在優化現有網路，更是在為未來的無線世界奠定基礎。現在加入我們，一起開創無線通訊的新紀元！", "audio": "docs/data/audios/2506.09855v1.wav"}
{"query": "Diffusion Model", "id": "2506.09740v1", "url": "http://arxiv.org/abs/2506.09740v1", "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:32:16.709693", "title_zh": "ELBO-T2IAlign：一種通用的基於ELBO的方法，用於校準擴散模型中像素級文字-圖像對齊", "summary_zh": "擴散模型在圖像生成方面表現出色，近年研究更顯示它們能透過注意力機制或損失函數編碼文字-圖像對齊資訊，這對圖像分割、文字引導的圖像編輯和組合圖像生成等下游任務至關重要。然而，現有方法過度依賴擴散模型中完美的文字-圖像對齊假設，但實際並非如此。本研究利用零樣本指代圖像分割作為代理任務，評估主流擴散模型的像素級圖像和類別級文字對齊。我們發現，小型、遮擋或罕見物體類別的圖像中容易出現錯位。因此，我們提出ELBO-T2IAlign，一種簡單有效的基於證據下界(ELBO)的方法來校準對齊，無需訓練且通用，適用於各種擴散模型架構。實驗證明了該方法的有效性。", "applications": ["想像一下，你可以對著手機說：『把照片裡那隻小狗的帽子換成聖誕帽』，AI就能精準地把小狗頭上的帽子替換掉，這就是精準文字圖像對齊的應用。", "如果你想設計一個獨一無二的T恤，只要輸入『一隻戴墨鏡的貓咪，背景是熱帶海灘』，AI就能自動生成高質量的圖像，讓你輕鬆印在T恤上，而且貓咪和海灘的組合會非常自然。", "未來的線上購物，不再需要攝影棚拍攝商品圖。只要輸入商品的文字描述，AI就能生成各種角度、各種場景下的商品圖片，大幅降低商家的運營成本。"], "pitch": "各位投資人，我們帶來的是ELBO-T2IAlign技術，它解決了擴散模型中文字與圖像對齊的核心問題。這項技術就像是為AI圖像生成引擎裝上了一顆更精準的定位系統，讓AI更能理解人類的意圖。試想一下，未來廣告設計、遊戲開發、電商行銷，都將因為這項技術而產生革命性的變化！更精準的圖像生成，意味著更高的用戶參與度、更低的製作成本、以及更無限的創意可能性。我們預期，這項技術將成為元宇宙內容生成的基石，並在未來五年內創造數十億美元的市場價值。現在加入我們，一起打造AI圖像生成的未來！", "audio": "docs/data/audios/2506.09740v1.wav"}
{"query": "AI", "id": "2506.09968v1", "url": "http://arxiv.org/abs/2506.09968v1", "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance", "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.", "authors": ["Wentao Ge", "Yuqing Sun", "Ziyan Wang", "Haoyue Zheng", "Weiyang He", "Piaohong Wang", "Qianyu Zhu", "Benyou Wang"], "published_date": "2025-06-11", "timestamp": "2025-06-12T18:36:01.129659", "title_zh": "SRLAgent：透過遊戲化與大型語言模型輔助強化自我調節學習技能", "summary_zh": "SRLAgent是一個利用遊戲化和大型語言模型(LLM)輔助的系統，旨在提升大學生的自我調節學習(SRL)能力。研究發現，許多大學生在目標設定、時間管理和反思學習方面遇到困難。SRLAgent基於Zimmerman的三階段SRL框架，提供一個互動的遊戲環境，讓學生進行目標設定、策略執行和自我反思。系統利用LLM提供即時回饋和鷹架，支援學生的獨立學習。實驗結果顯示，使用SRLAgent的學生在SRL技能上有顯著提升，且參與度更高。這項研究強調了在遊戲化環境中嵌入SRL鷹架和即時AI支援的價值，為教育科技的設計提供了新的方向，旨在促進更深入的學習和後設認知技能的發展。", "applications": ["想像一下，SRLAgent就像一位24小時的AI家教，隨時在你學習遇到困難時給予指導和鼓勵。無論是準備考試、撰寫報告，還是學習新技能，它都能幫助你更有效地設定目標、安排時間，並反思自己的學習過程，讓學習變得更輕鬆、更有趣。", "孩子們常常對學習感到厭倦，覺得很枯燥。SRLAgent可以將學習內容轉化為有趣的遊戲，讓孩子在玩樂中學習，激發他們的學習興趣和動力。家長也可以透過SRLAgent了解孩子的學習進度和困難，及時給予幫助。", "對於需要不斷學習新知識和技能的職場人士來說，SRLAgent是一個高效的學習夥伴。它可以幫助你快速掌握新知識、提升工作效率，並在工作中不斷成長。例如，學習新的程式語言、市場行銷策略，或是專案管理技巧。"], "pitch": "各位投資人，我們正處於AI賦能教育的黃金時代！SRLAgent不僅僅是一個學習工具，它是一個革命性的教育平台，利用遊戲化和LLM技術，個性化地提升學習者的自我調節能力。想像一下，全球數百萬大學生、職場人士，甚至中小學生，都在使用我們的平台，更高效、更自主地學習。這是一個龐大的市場，而且還在快速增長。SRLAgent的數據分析能力還能為教育機構提供寶貴的洞察，幫助他們優化課程設計和教學方法。我們預計，SRLAgent將成為未來教育領域的領頭羊，為投資者帶來豐厚的回報。現在加入我們，一起塑造教育的未來！", "audio": "docs/data/audios/2506.09968v1.wav"}
{"query": "AI", "id": "2506.09958v1", "url": "http://arxiv.org/abs/2506.09958v1", "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:25:14.263997", "title_zh": "Kvasir-VQA-x1：用於醫療推理和在腸胃內視鏡檢查中實現穩健MedVQA的多模態數據集", "summary_zh": "Kvasir-VQA-x1是一個大型腸胃內視鏡影像問答數據集，旨在提升醫療決策支援系統的效能。它擴展了原有的Kvasir-VQA數據集，新增了超過15萬個問題答案對，專門測試模型的臨床推理能力。這些問題由大型語言模型生成，並根據複雜度分層，同時加入模擬常見影像偽影的視覺增強，以確保模型能適應真實臨床場景。此數據集提供標準VQA效能和模型對視覺干擾的穩健性兩種評估方式，有助於開發更可靠有效的多模態AI系統，並已完全公開，遵循FAIR數據原則。", "applications": ["想像一下，醫生在做胃鏡檢查時，AI能即時回答醫生提出的問題，例如「這個病灶看起來像什麼？」、「這個區域是否需要切片檢查？」，幫助醫生做出更準確的判斷。", "如果我們開發一個App，讓民眾可以上傳自己的腸胃鏡照片，AI就能初步分析照片，提供可能的健康風險評估，讓民眾更了解自己的腸胃健康狀況。", "未來，AI可以整合病人的病歷資料和腸胃鏡影像，預測病人未來罹患腸胃疾病的風險，提醒病人及早進行預防和治療。"], "pitch": "各位投資人，我們正在打造腸胃內視鏡AI輔助診斷的未來！Kvasir-VQA-x1數據集是我們成功的基石，它能訓練出更聰明、更可靠的AI模型，協助醫生精準診斷腸胃疾病。想像一下，AI不僅能減少誤診率，還能大幅提升診斷效率，降低醫療成本！隨著人口老化和生活習慣改變，腸胃疾病的發病率不斷攀升，市場潛力巨大。我們的技術不僅能應用於醫院，更能推廣到基層診所，甚至進入家庭，成為每個人的腸胃健康守護者。我們有信心，這項技術將顛覆傳統醫療模式，為投資者帶來豐厚的回報！現在加入我們，一起開創AI醫療的新紀元！", "audio": "docs/data/audios/2506.09958v1.wav"}
{"query": "Foundation Model", "id": "2506.09755v1", "url": "http://arxiv.org/abs/2506.09755v1", "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era", "summary": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges.", "authors": ["Shuo Jiang", "Min Xie", "Frank Youhua Chen", "Jian Ma", "Jianxi Luo"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:26:48.175719", "title_zh": "智慧設計4.0：邁向具主動性人工智慧時代的典範演進", "summary_zh": "智慧設計（ID）在提升工程創新、效率、品質和生產力方面貢獻卓著。隨著大型語言模型（LLM）等基礎模型的興起，具備了通用知識推理能力，為工程設計帶來了新的變革。本文介紹了智慧設計4.0（ID 4.0），它是由具主動性的人工智慧系統所驅動的新典範。我們回顧了ID的四個發展階段，並提出了ID 4.0的概念框架，討論了其通過協調、自主的多代理系統實現工程設計流程端到端自動化的潛力。未來，ID 4.0將在更複雜的設計場景、更實際的設計實現、新型代理協調機制以及自主設計目標設定等方面，展現更大的適應性、自主性和有效性。", "applications": ["想像一下，未來設計師不再需要從頭開始繪製藍圖。透過ID 4.0，只要輸入需求，AI就能自動生成多個設計方案，甚至考慮到材料成本和環境影響，大幅縮短設計時間，讓建築師有更多時間與客戶溝通，打造更符合需求的夢想家園。", "如果家裡的電器壞了，不再需要等待維修人員上門。ID 4.0可以根據電器的型號和故障描述，自動生成維修步驟和所需零件，甚至可以透過AR技術，指導使用者自行維修，省時又省錢。", "在醫療領域，ID 4.0可以幫助醫生設計個性化的治療方案。例如，針對癌症患者，AI可以分析患者的基因數據和病理報告，設計出最有效的化療方案，並預測治療效果，提高治療成功率。"], "pitch": "各位投資人，我們正站在一個劃時代的轉捩點上！智慧設計4.0（ID 4.0）不僅僅是一個技術概念，它是一場工程設計領域的革命！想像一下，一個由AI驅動的設計團隊，24小時不間斷地工作，以驚人的速度和效率完成設計任務。這不僅能大幅降低成本，更能加速產品上市時間，搶佔市場先機。更重要的是，ID 4.0有能力解決人類無法解決的複雜設計問題，例如：設計出更節能環保的建築、更安全可靠的交通工具、甚至開發出全新的醫療設備。我們相信，ID 4.0將顛覆傳統產業，催生出無數的創新應用，並為投資者帶來豐厚的回報！現在投資ID 4.0，就是投資未來！讓我們一起攜手，打造一個更智慧、更美好的世界！", "audio": "docs/data/audios/2506.09755v1.wav"}
{"query": "Diffusion Model", "id": "2506.09665v1", "url": "http://arxiv.org/abs/2506.09665v1", "title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "summary": "We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.", "authors": ["Jacob Munkberg", "Zian Wang", "Ruofan Liang", "Tianchang Shen", "Jon Hasselgren"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:27:56.854870", "title_zh": "VideoMat：從影片擴散模型中提取PBR材質", "summary_zh": "VideoMat運用微調後的影片擴散模型，結合影片的內在分解和基於物理的可微渲染，從文字提示或單張圖像生成高品質的3D模型材質。首先，我們調整影片擴散模型，使其符合輸入的幾何形狀和光照條件，產生具有一致材質屬性的3D模型多視圖。接著，利用最新模型從生成的影片中提取內在屬性（基礎顏色、粗糙度、金屬度）。最後，我們將這些內在屬性與生成的影片一起輸入可微路徑追蹤器，以穩健地提取與常見內容創作工具直接相容的PBR材質。", "applications": ["遊戲開發者可以快速生成逼真的遊戲場景材質，省去繁瑣的手工調整。", "室內設計師可以將客戶提供的照片或影片轉換為3D模型材質，方便進行虛擬展示和修改。", "電商平台可以利用這項技術自動生成商品3D模型的材質，提升商品展示效果和消費者購買意願。"], "pitch": "各位投資人，想像一下，未來創建3D內容將不再需要耗時費力的手動建模和材質調整。VideoMat技術的出現，將徹底顛覆這個產業。我們利用AI的力量，能從簡單的影片或圖片中自動提取高品質的PBR材質，這意味著內容創作者可以大幅降低成本、加快開發速度，並創造出更逼真、更吸引人的3D內容。無論是遊戲、電影、電商還是元宇宙，對於高品質3D素材的需求都將是巨大的。VideoMat不僅僅是一個技術，更是一個通往無限可能的入口。我們預計，隨著元宇宙的發展和3D內容的普及，VideoMat將成為3D內容創作領域的關鍵基礎設施，市場潛力不可估量。現在投資VideoMat，就是投資3D內容的未來！", "audio": "docs/data/audios/2506.09665v1.wav"}
{"query": "AI", "id": "2506.09947v1", "url": "http://arxiv.org/abs/2506.09947v1", "title": "KI4Demokratie: An AI-Based Platform for Monitoring and Fostering Democratic Discourse", "summary": "Social media increasingly fuel extremism, especially right-wing extremism,\nand enable the rapid spread of antidemocratic narratives. Although AI and data\nscience are often leveraged to manipulate political opinion, there is a\ncritical need for tools that support effective monitoring without infringing on\nfreedom of expression. We present KI4Demokratie, an AI-based platform that\nassists journalists, researchers, and policymakers in monitoring right-wing\ndiscourse that may undermine democratic values. KI4Demokratie applies machine\nlearning models to a large-scale German online data gathered on a daily basis,\nproviding a comprehensive view of trends in the German digital sphere. Early\nanalysis reveals both the complexity of tracking organized extremist behavior\nand the promise of our integrated approach, especially during key events.", "authors": ["Rudy Alexandro Garrido Veliz", "Till Nikolaus Schaland", "Simon Bergmoser", "Florian Horwege", "Somya Bansal", "Ritesh Nahar", "Martin Semmann", "Jörg Forthmann", "Seid Muhie Yimam"], "published_date": "2025-06-11", "timestamp": "2025-06-13T02:03:16.537260", "title_zh": "KI4Demokratie：一個基於人工智慧的平台，用於監測和促進民主 discourse", "summary_zh": "KI4Demokratie是一個基於人工智慧的平台，旨在協助記者、研究人員和政策制定者監測可能破壞民主價值的右翼言論。該平台利用機器學習模型分析大規模的德國線上數據，每日更新，提供德國數位領域趨勢的全面視圖。初步分析顯示，追蹤有組織的極端主義行為具有複雜性，但也證明了我們整合方法的潛力，尤其是在關鍵事件期間。", "applications": ["新聞媒體可以利用這個平台，即時追蹤網路上的仇恨言論和假新聞，更快更準確地報導相關事件，提升新聞品質和公信力。", "政府部門可以運用這個平台，監控網路輿情，及早發現並處理可能危害社會穩定的言論，維護公共安全。", "教育機構可以利用這個平台，分析網路上的不實資訊和極端思想，設計更有針對性的課程，提升學生的媒體素養和批判性思考能力。"], "pitch": "各位投資人，今天向各位介紹的是KI4Demokratie，一個劃時代的AI平台，它不只是個工具，更是民主的守護者。在假新聞和極端言論氾濫的時代，KI4Demokratie能精準監測並分析網路輿情，協助媒體、政府和教育機構即時應對。想像一下，我們能提前預警社會動盪，阻止仇恨言論擴散，甚至能辨識並打擊境外勢力的認知作戰。這不僅關乎社會責任，更蘊藏巨大的商業價值。未來，我們將進一步開發AI模型，預測輿論走向，提供更精準的決策支持。KI4Demokratie的潛力無限，它將成為維護民主價值、引領輿論走向的關鍵力量。現在投資KI4Demokratie，您投資的不僅僅是一個平台，更是投資一個更安全、更健康的未來！", "audio": "docs/data/audios/2506.09947v1.wav"}
{"query": "Foundation Model", "id": "2506.09748v1", "url": "http://arxiv.org/abs/2506.09748v1", "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "published_date": "2025-06-11", "timestamp": "2025-06-13T02:04:34.519078", "title_zh": "基於語義和結構約束的無人機絕對視覺定位分層圖像匹配", "summary_zh": "本研究提出一種用於無人機絕對視覺定位的分層跨源圖像匹配方法，旨在解決在全球導航衛星系統（GNSS）信號不可用時的定位挑戰。該方法整合了語義感知和結構約束的粗略匹配模塊，以及輕量級的精細匹配模塊。粗略匹配模塊利用視覺基礎模型提取的語義特徵，在語義和結構約束下建立區域級的對應關係。隨後，精細匹配模塊提取精細特徵並建立像素級的對應關係。該方法無需依賴相對定位技術，通過圖像檢索模塊和分層圖像匹配模塊構建無人機絕對視覺定位流程。實驗結果表明，該方法在各種挑戰性條件下具有卓越的準確性和魯棒性。", "applications": ["無人機送貨：在沒有GPS訊號的環境下，例如城市高樓之間或室內，無人機能精準地將包裹送到目的地。", "災難救援：在地震或洪水等災難發生後，GPS訊號可能中斷，無人機可利用此技術快速定位受困人員，協助救援隊伍。", "自動駕駛：在隧道、地下停車場等GPS訊號弱的區域，自動駕駛汽車可以利用此技術進行精確導航。"], "pitch": "各位投資人，想像一下，一個完全自主、不受GPS限制的無人機世界！我們的分層圖像匹配技術，讓無人機在任何環境下都能精準定位，開啟無限可能。從智慧農業的精準噴灑，到智慧城市的無人巡檢，再到軍事領域的自主偵察，應用場景廣闊。更重要的是，隨著低軌衛星網路的普及，我們的技術將與之結合，打造全球無縫覆蓋的無人機定位系統，徹底顛覆物流、安防、以及各行各業的運作模式。我們不只在解決當前的痛點，更在佈局未來的藍圖！現在投資，您將成為這場無人機革命的領航者！", "audio": "docs/data/audios/2506.09748v1.wav"}
{"query": "Diffusion Model", "id": "2506.09644v1", "url": "http://arxiv.org/abs/2506.09644v1", "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "published_date": "2025-06-11", "timestamp": "2025-06-13T02:05:46.595423", "title_zh": "DGAE：擴散引導的自編碼器，用於高效的潛在表徵學習", "summary_zh": "本研究提出一種名為DGAE的新型自編碼器，旨在提升圖像和影片生成模型的效率。DGAE利用擴散模型引導解碼器，從壓縮後的潛在空間中恢復更多資訊，有效解決了高壓縮率下自編碼器性能下降的問題，同時將潛在空間的維度縮小了一倍。實驗結果顯示，DGAE在ImageNet-1K圖像生成任務上表現出色，並能加速擴散模型的收斂。簡而言之，DGAE在保證圖像品質的前提下，實現了更高效、更精簡的圖像和影片表示。", "applications": ["假設你手機容量不夠，DGAE技術可以幫你把照片、影片壓縮得更小，畫質卻幾乎不變，讓你省下更多空間。", "現在很多AI繪圖工具，如果用上DGAE，就能用更少的運算資源，畫出更高品質的圖片，速度也會更快。", "未來的虛擬實境（VR）或擴增實境（AR）應用，需要傳輸大量的影像資料，DGAE可以大幅降低傳輸的數據量，讓畫面更流暢，不再卡頓。"], "pitch": "各位投資人，我們正站在AI影像革命的浪潮之上！DGAE技術，是這場革命的關鍵引擎。它不僅僅是壓縮技術，更是提升AI生成模型效率的秘密武器。想像一下，未來AI生成的內容將無所不在，從遊戲、電影到廣告、教育，都需要高效、高品質的影像生成。DGAE能讓這些應用更快、更好、更便宜地實現。更重要的是，DGAE大幅降低了AI模型對算力的需求，讓AI技術不再是少數巨頭的專利，而是人人都能觸及的工具。我們預計，DGAE將成為未來AI影像處理的基礎設施，市場潛力無限。現在投資DGAE，就是投資AI的未來！", "audio": "docs/data/audios/2506.09644v1.wav"}
{"query": "AI", "id": "2506.10975v1", "url": "http://arxiv.org/abs/2506.10975v1", "title": "GenWorld: Towards Detecting AI-generated Real-world Simulation Videos", "summary": "The flourishing of video generation technologies has endangered the\ncredibility of real-world information and intensified the demand for\nAI-generated video detectors. Despite some progress, the lack of high-quality\nreal-world datasets hinders the development of trustworthy detectors. In this\npaper, we propose GenWorld, a large-scale, high-quality, and real-world\nsimulation dataset for AI-generated video detection. GenWorld features the\nfollowing characteristics: (1) Real-world Simulation: GenWorld focuses on\nvideos that replicate real-world scenarios, which have a significant impact due\nto their realism and potential influence; (2) High Quality: GenWorld employs\nmultiple state-of-the-art video generation models to provide realistic and\nhigh-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes\nvideos generated from diverse generators and various prompt modalities (e.g.,\ntext, image, video), offering the potential to learn more generalizable\nforensic features. We analyze existing methods and find they fail to detect\nhigh-quality videos generated by world models (i.e., Cosmos), revealing\npotential drawbacks of ignoring real-world clues. To address this, we propose a\nsimple yet effective model, SpannDetector, to leverage multi-view consistency\nas a strong criterion for real-world AI-generated video detection. Experiments\nshow that our method achieves superior results, highlighting a promising\ndirection for explainable AI-generated video detection based on physical\nplausibility. We believe that GenWorld will advance the field of AI-generated\nvideo detection. Project Page: https://chen-wl20.github.io/GenWorld", "authors": ["Weiliang Chen", "Wenzhao Zheng", "Yu Zheng", "Lei Chen", "Jie Zhou", "Jiwen Lu", "Yueqi Duan"], "published_date": "2025-06-12", "timestamp": "2025-06-13T03:52:05.588354", "title_zh": "GenWorld：邁向偵測AI生成的真實世界模擬影片", "summary_zh": "隨著影片生成技術蓬勃發展，AI生成影片的偵測需求日益增加。現有偵測器缺乏高品質的真實世界數據集，導致發展受限。本研究提出GenWorld，一個大規模、高品質的真實世界模擬數據集，專為AI生成影片偵測而設計。GenWorld模擬真實場景，採用多種先進影片生成模型，提供逼真的偽造影片，並包含來自不同生成器和提示模態的影片，以學習更具泛化性的鑑識特徵。我們提出SpannDetector模型，利用多視角一致性作為真實世界AI生成影片偵測的強大標準。實驗證明，本方法能有效偵測，為基於物理合理性的可解釋AI生成影片偵測，提供了一個有前景的方向。", "applications": ["新聞查核：當我們在社群媒體上看到一段關於災難或政治事件的影片時，可以使用GenWorld技術來驗證影片的真偽，避免被假新聞誤導。", "保險理賠：保險公司可以利用GenWorld技術來判斷提交的事故影片是否經過AI偽造，防止詐欺行為，例如虛報車禍或意外傷害。", "教育訓練：學校或企業可以使用GenWorld技術設計AI生成影片偵測課程，讓學生或員工學習如何辨識和應對AI偽造的影片，提升媒體素養。"], "pitch": "各位投資人，想像一下，在AI生成內容氾濫的未來，我們如何確保眼見為真？GenWorld不僅是一個數據集，更是一把對抗假訊息的利劍。我們開發的SpannDetector模型，能有效辨識AI生成的模擬影片，其應用範圍廣泛，從新聞媒體的真相驗證，到金融機構的詐欺防範，甚至是國安層面的資訊戰防禦，都有著巨大的潛力。隨著deepfake技術日益精進，市場對AI生成影片偵測的需求將會爆炸性成長。GenWorld團隊已掌握領先優勢，我們有信心將這項技術推向全球，成為AI時代的守門人，並為投資者帶來豐厚的回報。現在投資GenWorld，就是投資一個更真實、更安全的未來！", "audio": "docs/data/audios/2506.10975v1.wav"}
{"query": "Foundation Model", "id": "2506.10966v1", "url": "http://arxiv.org/abs/2506.10966v1", "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation", "summary": "Robotic manipulation in real-world settings remains challenging, especially\nregarding robust generalization. Existing simulation platforms lack sufficient\nsupport for exploring how policies adapt to varied instructions and scenarios.\nThus, they lag behind the growing interest in instruction-following foundation\nmodels like LLMs, whose adaptability is crucial yet remains underexplored in\nfair comparisons. To bridge this gap, we introduce GenManip, a realistic\ntabletop simulation platform tailored for policy generalization studies. It\nfeatures an automatic pipeline via LLM-driven task-oriented scene graph to\nsynthesize large-scale, diverse tasks using 10K annotated 3D object assets. To\nsystematically assess generalization, we present GenManip-Bench, a benchmark of\n200 scenarios refined via human-in-the-loop corrections. We evaluate two policy\ntypes: (1) modular manipulation systems integrating foundation models for\nperception, reasoning, and planning, and (2) end-to-end policies trained\nthrough scalable data collection. Results show that while data scaling benefits\nend-to-end methods, modular systems enhanced with foundation models generalize\nmore effectively across diverse scenarios. We anticipate this platform to\nfacilitate critical insights for advancing policy generalization in realistic\nconditions. Project Page: https://genmanip.axi404.top/.", "authors": ["Ning Gao", "Yilun Chen", "Shuai Yang", "Xinyi Chen", "Yang Tian", "Hao Li", "Haifeng Huang", "Hanqing Wang", "Tai Wang", "Jiangmiao Pang"], "published_date": "2025-06-12", "timestamp": "2025-06-13T03:53:19.853024", "title_zh": "GENMANIP：基於LLM驅動的模擬，實現通用指令遵循操作", "summary_zh": "現實環境中的機器人操作仍具挑戰性，尤其在泛化能力方面。現有模擬平台對探索策略如何適應不同指令和場景的支持不足，落後於指令遵循基礎模型（如LLM）的發展。為此，我們推出GenManip，一個真實的桌面模擬平台，專為策略泛化研究而設計。它通過LLM驅動的任務導向場景圖自動生成管道，利用10K個帶註釋的3D對象資源合成大規模、多樣化的任務。我們還提出了GenManip-Bench，包含200個場景的基準測試，並通過人機協作修正進行完善。評估結果表明，雖然數據擴展有助於端到端方法，但利用基礎模型增強的模塊化系統在多樣化場景中泛化效果更好。我們期望這個平台能促進對在真實條件下推進策略泛化的重要見解。", "applications": ["想像一下，你可以用口語指令，像是「把紅色的蘋果放到藍色的碗裡」，就能直接指揮家裡的機器人幫你整理東西，再也不用自己彎腰撿東西了！", "醫院裡，醫生可以透過語音指令，讓機器人自動準備手術需要的工具，大幅減少手術時間和人為錯誤的風險，提升醫療效率。", "在工廠裡，即使生產線臨時需要調整，工程師也能用簡單的指令，快速重新配置機器人的工作流程，應對客製化的訂單需求，提高生產彈性。"], "pitch": "各位投資人，我們正處於AI賦能機器人的黃金時代！GenManip平台不僅僅是一個模擬器，它是機器人智能的加速器。想像一下，未來的工廠、醫院、甚至家庭，都將充滿著能聽懂人話、靈活工作的機器人。GenManip通過LLM賦能，讓機器人能像人類一樣理解指令、適應變化，這將徹底顛覆傳統的自動化模式。我們的GenManip-Bench基準測試，將成為業界評估機器人智能的黃金標準。更重要的是，我們正在建立一個龐大的機器人技能庫，未來可以通過訂閱服務、license授權等方式，將這些技能賦予各行各業的機器人，創造巨大的商業價值。現在投資GenManip，就是投資機器人智能的未來，讓我們一起打造一個更智能、更便捷的世界！預計五年內，GenManip將成為機器人開發領域的領導者，市場估值有望突破十億美元！", "audio": "docs/data/audios/2506.10966v1.wav"}
{"query": "Diffusion Model", "id": "2506.10981v1", "url": "http://arxiv.org/abs/2506.10981v1", "title": "SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis", "summary": "Generative models have gained significant attention in novel view synthesis\n(NVS) by alleviating the reliance on dense multi-view captures. However,\nexisting methods typically fall into a conventional paradigm, where generative\nmodels first complete missing areas in 2D, followed by 3D recovery techniques\nto reconstruct the scene, which often results in overly smooth surfaces and\ndistorted geometry, as generative models struggle to infer 3D structure solely\nfrom RGB data. In this paper, we propose SceneCompleter, a novel framework that\nachieves 3D-consistent generative novel view synthesis through dense 3D scene\ncompletion. SceneCompleter achieves both visual coherence and 3D-consistent\ngenerative scene completion through two key components: (1) a\ngeometry-appearance dual-stream diffusion model that jointly synthesizes novel\nviews in RGBD space; (2) a scene embedder that encodes a more holistic scene\nunderstanding from the reference image. By effectively fusing structural and\ntextural information, our method demonstrates superior coherence and\nplausibility in generative novel view synthesis across diverse datasets.\nProject Page: https://chen-wl20.github.io/SceneCompleter", "authors": ["Weiliang Chen", "Jiayi Bi", "Yuanhui Huang", "Wenzhao Zheng", "Yueqi Duan"], "published_date": "2025-06-12", "timestamp": "2025-06-13T03:54:25.001679", "title_zh": "SceneCompleter：用於生成式新視圖合成的密集3D場景補全", "summary_zh": "SceneCompleter是一個創新的框架，透過密集3D場景補全實現3D一致的生成式新視圖合成。它採用幾何-外觀雙流擴散模型，在RGBD空間中聯合合成新視圖，並利用場景嵌入器從參考圖像中編碼更全面的場景理解。透過有效地融合結構和紋理信息，SceneCompleter在生成式新視圖合成中展現了卓越的連貫性和合理性。這項技術克服了傳統方法在僅從RGB數據推斷3D結構時遇到的困難，產生更真實、更精確的3D場景重建。", "applications": ["線上購物：想像一下，你可以在購買家具前，用手機掃描客廳，就能立即看到家具擺放在你家中的真實樣貌，甚至可以隨意調整顏色和大小，預覽效果。", "虛擬旅遊：不用出國，也能身歷其境！透過手機或VR裝置，你可以探索世界各地的名勝古蹟，甚至可以自由切換視角，彷彿親臨現場。", "室內設計：設計師可以快速生成不同風格的室內設計方案，讓客戶更直觀地看到設計效果，減少溝通成本，提高設計效率。"], "pitch": "各位投資人，我們相信SceneCompleter將徹底改變3D內容生成的方式。現有的技術往往需要大量的數據和複雜的建模過程，而SceneCompleter只需少量參考圖像，就能生成高品質、3D一致的新視圖。這項技術的應用前景廣闊，從電商、遊戲、影視到建築、設計，都將因此受益。想像一下，未來的遊戲開發者可以更快速地創建逼真的遊戲場景；電影製作人可以更輕鬆地進行特效製作；房地產公司可以為潛在買家提供更沉浸式的看房體驗。我們預計，SceneCompleter將成為元宇宙時代的關鍵技術之一，並在未來幾年內帶來數十億美元的市場機會。現在投資我們，您將成為這場變革的領先者！", "audio": "docs/data/audios/2506.10981v1.wav"}
{"query": "AI", "id": "2506.10953v1", "url": "http://arxiv.org/abs/2506.10953v1", "title": "Build the web for agents, not agents for the web", "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.", "authors": ["Xing Han Lù", "Gaurav Kamath", "Marius Mosbach", "Siva Reddy"], "published_date": "2025-06-12", "timestamp": "2025-06-13T06:37:59.401318", "title_zh": "為代理人建構網路，而非為網路建構代理人", "summary_zh": "近年來，大型語言模型（LLM）和多模態模型的進步激發了開發網路代理人的濃厚興趣，這些AI系統能夠自主導航並在網路環境中完成任務。儘管它們在自動化複雜網路互動方面具有巨大潛力，但由於人類設計的介面與LLM能力之間存在根本不匹配，當前的方法面臨著重大挑戰。本論文倡導網路代理人研究中的範式轉變：我們應該開發一種專門為代理人能力優化的新型互動範式，而不是強迫網路代理人適應為人類設計的介面。為此，我們引入了代理人網路介面（AWI）的概念，這是一種專門為代理人導航網站而設計的介面。我們建立了六項AWI設計指導原則，強調安全性、效率和標準化，以考慮所有主要利益相關者的利益。這種重新定義旨在克服現有介面的根本限制，為更有效、可靠和透明的網路代理人設計鋪平道路，這將是更廣泛的機器學習社群的共同努力。", "applications": ["想像一下，你可以對著手機說：「幫我訂一張下星期五晚上七點在信義威秀影城的電影票，要《奧本海默》。」AI代理人就會自動瀏覽電影網站，找到合適的場次並完成訂票，完全不需要你手動操作。", "假設你需要比較不同銀行的貸款利率。你只需告訴AI代理人你的需求，它就會自動訪問各家銀行的網站，收集相關資訊，並整理成一個易於閱讀的表格，讓你輕鬆做出選擇。", "如果家裡長輩不擅長使用網路購物，可以透過AI代理人協助他們。他們只要口頭描述想要購買的商品，AI代理人就會自動搜尋商品、比較價格，並完成下單，省去複雜的操作步驟。"], "pitch": "各位投資人，我們正在打造一個革命性的網路互動方式，讓AI代理人成為您的數位助手。想像一下，未來的網路不再需要繁瑣的操作，而是由AI代理人根據您的指令自動完成。我們的Agentic Web Interface（AWI）將徹底改變電子商務、金融服務、客戶服務等領域。透過AWI，企業可以大幅降低人力成本，提高效率，並提供更個性化的服務。我們預計，AWI將成為下一代網路的基礎設施，創造一個數千億美元的市場。現在加入我們，一起開創AI驅動的網路新時代！未來，我們可以將AWI技術授權給各大企業，收取授權費用，或者開發基於AWI的SaaS產品，例如智能客服機器人、自動化行銷工具等。甚至可以將AWI整合到元宇宙中，讓用戶在虛擬世界中也能享受到便捷的AI服務。", "audio": "docs/data/audios/2506.10953v1.wav"}
{"query": "Foundation Model", "id": "2506.10956v1", "url": "http://arxiv.org/abs/2506.10956v1", "title": "Distillation of atomistic foundation models across architectures and chemical domains", "summary": "Machine-learned interatomic potentials have transformed computational\nresearch in the physical sciences. Recent atomistic `foundation' models have\nchanged the field yet again: trained on many different chemical elements and\ndomains, these potentials are widely applicable, but comparably slow and\nresource-intensive to run. Here we show how distillation via synthetic data can\nbe used to cheaply transfer knowledge from atomistic foundation models to a\nrange of different architectures, unlocking much smaller, more efficient\npotentials. We demonstrate speed-ups of $> 10\\times$ by distilling from one\ngraph-network architecture into another, and $> 100\\times$ by leveraging the\natomic cluster expansion framework. We showcase applicability across chemical\nand materials domains: from liquid water to hydrogen under extreme conditions;\nfrom porous silica and a hybrid halide perovskite solar-cell material to\nmodelling organic reactions. Our work shows how distillation can support the\nroutine and computationally efficient use of current and future atomistic\nfoundation models in real-world scientific research.", "authors": ["John L. A. Gardner", "Daniel F. Thomas du Toit", "Chiheb Ben Mahmoud", "Zoé Faure Beaulieu", "Veronika Juraskova", "Laura-Bianca Paşca", "Louise A. M. Rosset", "Fernanda Duarte", "Fausto Martelli", "Chris J. Pickard", "Volker L. Deringer"], "published_date": "2025-06-12", "timestamp": "2025-06-13T06:39:52.830955", "title_zh": "跨架構與化學領域的原子基礎模型知識蒸餾", "summary_zh": "本研究展示如何透過合成數據的知識蒸餾，將大型原子基礎模型的知識轉移到更小、更高效的模型上。這些基礎模型雖然應用廣泛，但運算速度較慢且耗費資源。透過知識蒸餾，我們成功將模型速度提升10倍以上，甚至超過100倍，同時保持其在不同化學和材料領域的準確性。這項技術可應用於液態水、極端條件下的氫、多孔二氧化矽、鈣鈦礦太陽能電池材料以及有機反應建模等領域。知識蒸餾有助於在實際科研中更有效率地使用現有和未來的原子基礎模型。", "applications": ["想像一下，藥廠可以利用這項技術，加速新藥的開發流程。透過更快速的分子動力學模擬，他們能更精準地預測藥物與蛋白質的交互作用，縮短研發時程，更快推出救命新藥。", "在材料科學領域，工程師可以利用這項技術，設計出更堅固、更輕盈的材料，應用於航空、汽車等產業。例如，開發更耐高溫的引擎材料，提升飛機的燃油效率，或打造更輕巧的電動車車身，增加續航里程。", "農民也可以受惠於此技術。透過模擬土壤中的化學反應，可以更精準地了解肥料的使用效率，減少不必要的浪費，並降低對環境的影響，實現更永續的農業發展。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它將徹底改變材料科學、化學工程和藥物開發等領域。原子基礎模型雖然強大，但運算成本高昂，阻礙了其廣泛應用。我們的知識蒸餾技術，就像是為這些模型裝上了渦輪增壓引擎，大幅提升其運算效率，同時保持其精準度。這意味著，我們可以將原本需要超級電腦才能完成的模擬，在一般的伺服器上運行，甚至在行動裝置上實現。想像一下，未來新藥開發的成本將大幅降低，新材料的設計速度將加快數倍，甚至可以根據個人基因，客製化出最適合的藥物和材料。這不僅僅是一項技術突破，更是一個龐大的市場機會。我們正在打造一個全新的模擬生態系統，一個讓科學家、工程師和研究人員都能輕鬆使用原子級別模擬的世界。現在加入我們，一起引領這場科學模擬的革命，共同創造一個更美好的未來！", "audio": "docs/data/audios/2506.10956v1.wav"}
{"query": "Diffusion Model", "id": "2506.10978v1", "url": "http://arxiv.org/abs/2506.10978v1", "title": "Fine-Grained Perturbation Guidance via Attention Head Selection", "summary": "Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer (DiT) architectures where\nquality-relevant computations are distributed across layers. In this paper, we\ninvestigate the granularity of attention perturbations, ranging from the layer\nlevel down to individual attention heads, and discover that specific heads\ngovern distinct visual concepts such as structure, style, and texture quality.\nBuilding on this insight, we propose \"HeadHunter\", a systematic framework for\niteratively selecting attention heads that align with user-centric objectives,\nenabling fine-grained control over generation quality and visual attributes. In\naddition, we introduce SoftPAG, which linearly interpolates each selected\nhead's attention map toward an identity matrix, providing a continuous knob to\ntune perturbation strength and suppress artifacts. Our approach not only\nmitigates the oversmoothing issues of existing layer-level perturbation but\nalso enables targeted manipulation of specific visual styles through\ncompositional head selection. We validate our method on modern large-scale\nDiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,\ndemonstrating superior performance in both general quality enhancement and\nstyle-specific guidance. Our work provides the first head-level analysis of\nattention perturbation in diffusion models, uncovering interpretable\nspecialization within attention layers and enabling practical design of\neffective perturbation strategies.", "authors": ["Donghoon Ahn", "Jiwon Kang", "Sanghyun Lee", "Minjae Kim", "Jaewon Min", "Wooseok Jang", "Saungwu Lee", "Sayak Paul", "Susung Hong", "Seungryong Kim"], "published_date": "2025-06-12", "timestamp": "2025-06-13T06:41:07.555945", "title_zh": "基於注意力頭選擇的細粒度擾動引導", "summary_zh": "本研究探索在擴散模型中，如何更精確地控制圖像生成。現有方法透過擾動模型來引導生成方向，尤其是在無法使用無分類器引導的場景中，注意力擾動表現出色。但過去方法缺乏精準的擾動位置選擇。本研究發現，不同注意力頭控制著不同的視覺概念，例如結構、風格和紋理品質。因此，我們提出HeadHunter框架，透過迭代選擇與使用者目標一致的注意力頭，實現對生成品質和視覺屬性的細粒度控制。此外，我們引入SoftPAG，透過線性插值調整擾動強度，減少生成瑕疵。此方法不僅解決了過度平滑問題，還能透過組合頭選擇來精準操控視覺風格。經驗證，在Stable Diffusion 3和FLUX.1等模型上，此方法在提升整體品質和風格引導方面表現優異。", "applications": ["**個性化頭像生成：** 想讓你的AI頭像更具特色？例如，指定髮型、服裝風格，甚至調整臉部表情的細微之處，都能輕鬆實現。", "**產品設計微調：** 設計師可以精確調整產品的視覺元素，例如材質紋理、光澤度，快速迭代出符合市場需求的設計方案。", "**藝術風格轉換：** 想讓照片呈現特定畫家的風格？透過選擇特定的注意力頭，就能將照片轉換成印象派、抽象派等不同風格，創作獨一無二的藝術作品。"], "pitch": "想像一下，一個AI圖像生成引擎，不僅能創造出逼真的圖像，還能像一位技藝精湛的畫家，精準地按照你的指令調整每一個細節。我們的技術“HeadHunter”正是實現這一願景的關鍵。它賦予了AI前所未有的控制力，讓使用者能夠以前所未有的方式塑造視覺內容。這項技術的潛在商業價值巨大：從遊戲、娛樂產業的角色和場景設計，到廣告行銷的客製化素材，再到電商平台的商品展示，甚至是醫療影像的精準分析，HeadHunter都能帶來革命性的改變。我們相信，HeadHunter將成為下一代圖像生成技術的核心引擎，引領AI視覺創作的新時代。現在投資，你將站在這波浪潮的最前端，共同開創一個由AI驅動的無限可能的視覺未來！", "audio": "docs/data/audios/2506.10978v1.wav"}
{"query": "AI", "id": "2506.10934v1", "url": "http://arxiv.org/abs/2506.10934v1", "title": "Dynamic Epistemic Friction in Dialogue", "summary": "Recent developments in aligning Large Language Models (LLMs) with human\npreferences have significantly enhanced their utility in human-AI collaborative\nscenarios. However, such approaches often neglect the critical role of\n\"epistemic friction,\" or the inherent resistance encountered when updating\nbeliefs in response to new, conflicting, or ambiguous information. In this\npaper, we define dynamic epistemic friction as the resistance to epistemic\nintegration, characterized by the misalignment between an agent's current\nbelief state and new propositions supported by external evidence. We position\nthis within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit,\n2011), where friction emerges as nontrivial belief-revision during the\ninteraction. We then present analyses from a situated collaborative task that\ndemonstrate how this model of epistemic friction can effectively predict belief\nupdates in dialogues, and we subsequently discuss how the model of belief\nalignment as a measure of epistemic resistance or friction can naturally be\nmade more sophisticated to accommodate the complexities of real-world dialogue\nscenarios.", "authors": ["Timothy Obiso", "Kenneth Lai", "Abhijnan Nath", "Nikhil Krishnaswamy", "James Pustejovsky"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:28:42.207994", "title_zh": "對話中的動態認知摩擦", "summary_zh": "本研究探討大型語言模型(LLM)在人機協作中忽略的「認知摩擦」問題。認知摩擦指的是當人們面對新的、衝突的或模糊的資訊時，更新信念時遇到的阻力。我們將「動態認知摩擦」定義為認知整合的阻力，也就是個體當前信念狀態與外部證據支持的新觀點之間的落差。我們利用動態認知邏輯框架，分析了在協作任務中，認知摩擦如何影響對話中的信念更新。研究結果顯示，此模型能有效預測對話中的信念變化，並可進一步完善，以應對真實對話的複雜性。", "applications": ["情境一：醫療診斷輔助。醫生在與AI系統討論病患病情時，AI的建議可能與醫生的經驗判斷不同。了解認知摩擦能幫助AI更好地呈現資訊，減少醫生因觀點差異而產生的抗拒感，提升診斷效率。", "情境二：協商談判。在商業談判中，不同立場的參與者對同一份合約條款可能有不同解讀。AI可以分析雙方的認知摩擦點，提供更具說服力的論點，促進達成共識。", "情境三：教育學習。學生在學習新知識時，如果新知識與既有觀念衝突，容易產生認知摩擦。AI導師可以根據學生的認知結構，調整教學策略，降低學習阻力，提升學習效果。"], "pitch": "各位創投先進，想像一下，我們正在打造的不僅僅是AI，而是真正能與人類有效協作的智能夥伴！現今的大型語言模型雖然強大，卻常常因為忽略人類的認知習慣，導致溝通效率低下，甚至產生誤解。我們的「動態認知摩擦」模型，就像是為AI裝上了一顆同理心，讓它能理解人類的認知盲點，減少溝通阻力，進而提升協作效率。試想一下，在醫療、金融、教育等領域，如果AI能更順暢地與專業人士協作，將釋放出多麼巨大的生產力！我們不僅掌握了核心技術，更擁有廣闊的應用前景。我們預期，未來五年內，這項技術將成為人機協作領域的關鍵標準，並為我們帶來數十億美元的市場價值。現在加入我們，您將成為這場人機協作革命的領航者！", "audio": "docs/data/audios/2506.10934v1.wav"}
{"query": "Foundation Model", "id": "2506.10914v1", "url": "http://arxiv.org/abs/2506.10914v1", "title": "Foundation Models for Causal Inference via Prior-Data Fitted Networks", "summary": "Prior-data fitted networks (PFNs) have recently been proposed as a promising\nway to train tabular foundation models. PFNs are transformers that are\npre-trained on synthetic data generated from a prespecified prior distribution\nand that enable Bayesian inference through in-context learning. In this paper,\nwe introduce CausalFM, a comprehensive framework for training PFN-based\nfoundation models in various causal inference settings. First, we formalize the\nconstruction of Bayesian priors for causal inference based on structural causal\nmodels (SCMs) in a principled way and derive necessary criteria for the\nvalidity of such priors. Building on this, we propose a novel family of prior\ndistributions using causality-inspired Bayesian neural networks that enable\nCausalFM to perform Bayesian causal inference in various settings, including\nback-door, front-door, and instrumental variable adjustment. Finally, we\ninstantiate CausalFM and explicitly train a foundation model for estimating\nconditional average treatment effects (CATEs) using back-door adjustment. We\nshow that CausalFM performs competitively for CATE estimation using various\nsynthetic and semi-synthetic benchmarks. In sum, our framework can be used as a\ngeneral recipe to train foundation models for various causal inference\nsettings. In contrast to the current state-of-the-art in causal inference,\nCausalFM offers a novel paradigm with the potential to fundamentally change how\npractitioners perform causal inference in medicine, economics, and other\ndisciplines.", "authors": ["Yuchen Ma", "Dennis Frauen", "Emil Javurek", "Stefan Feuerriegel"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:30:01.015475", "title_zh": "基於先驗資料擬合網路之因果推論基礎模型", "summary_zh": "本研究提出CausalFM，一個基於先驗資料擬合網路（PFNs）的因果推論基礎模型框架。PFNs透過在合成資料上預訓練Transformer，並利用上下文學習實現貝氏推論。CausalFM基於結構因果模型（SCMs），系統化地構建因果推論的貝氏先驗，並提出新型的因果啟發式貝氏神經網路先驗分佈，使其能在後門、前門和工具變數調整等多種情境下執行貝氏因果推論。實驗證明，CausalFM在條件平均處理效應（CATE）估計方面表現出色，為醫學、經濟學等領域的因果推論提供了一種全新的方法。", "applications": ["**個人化醫療：**想像一下，醫生可以利用CausalFM分析你的病歷、基因數據和生活習慣，準確預測哪種治療方案對你最有效，避免不必要的副作用，就像擁有一個超級聰明的醫療顧問。", "**精準行銷：**電商平台可以利用CausalFM分析你的購物行為、瀏覽紀錄和社群互動，精準預測你對哪些產品感興趣，並提供客製化的推薦和優惠，讓你每次購物都能滿載而歸。", "**政策模擬：**政府可以利用CausalFM模擬不同政策對經濟、社會和環境的影響，例如調整稅收政策、推動綠色能源等，從而制定更有效的政策，提升社會福祉。"], "pitch": "各位投資人，我們正站在一個劃時代的轉捩點上！CausalFM不僅僅是一個模型，它是一個通往因果關係理解的鑰匙，一個重塑決策方式的引擎。想像一下，一個AI可以精準預測新藥的療效、評估行銷活動的真實影響、甚至預測氣候變遷的長期後果。這就是CausalFM的潛力！在醫療領域，CausalFM能加速新藥研發，降低臨床試驗成本，實現精準醫療；在金融領域，CausalFM能更準確地預測市場風險，優化投資組合；在政策制定領域，CausalFM能幫助政府制定更有效的政策，提升社會福祉。我們相信，CausalFM將成為未來AI發展的核心動力，引領下一波科技革命。現在加入我們，共同開創一個更智慧、更美好的未來！", "audio": "docs/data/audios/2506.10914v1.wav"}
{"query": "Diffusion Model", "id": "2506.10971v1", "url": "http://arxiv.org/abs/2506.10971v1", "title": "What Exactly Does Guidance Do in Masked Discrete Diffusion Models", "summary": "We study masked discrete diffusion models with classifier-free guidance\n(CFG). Assuming no score error nor discretization error, we derive an explicit\nsolution to the guided reverse dynamics, so that how guidance influences the\nsampling behavior can be precisely characterized. When the full data\ndistribution is a mixture over classes and the goal is to sample from a\nspecific class, guidance amplifies class-specific regions while suppresses\nregions shared with other classes. This effect depends on the guidance strength\n$w$ and induces distinct covariance structures in the sampled distribution.\nNotably, we observe quantitatively different behaviors in $1$D and $2$D. We\nalso show that for large $w$, the decay rate of the total variation\n($\\mathrm{TV}$) along the reverse dynamics is double-exponential in $w$ for\nboth $1$D and $2$D. These findings highlight the role of guidance, not just in\nshaping the output distribution, but also in controlling the dynamics of the\nsampling trajectory. Our theoretical analysis is supported by experiments that\nillustrate the geometric effects of guidance and its impact on convergence.", "authors": ["He Ye", "Rojas Kevin", "Tao Molei"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:31:33.022686", "title_zh": "遮罩離散擴散模型中，引導究竟做了什麼？", "summary_zh": "本研究深入探討具備無分類器引導（CFG）的遮罩離散擴散模型。在假設沒有分數誤差和離散化誤差的前提下，我們推導出引導反向動力學的顯式解，從而精確地描述引導如何影響取樣行為。當完整數據分佈是各類別的混合，且目標是從特定類別中取樣時，引導會放大特定類別區域，同時抑制與其他類別共享的區域。這種效應取決於引導強度w，並在取樣分佈中產生不同的共變異數結構。我們觀察到一維和二維中定量不同的行為。此外，我們證明對於大的w，沿反向動力學的總變異（TV）的衰減率在一維和二維中都是w的雙指數。這些發現突顯了引導的作用，不僅在於塑造輸出分佈，還在於控制取樣軌跡的動力學。我們的理論分析得到了實驗的支持，這些實驗說明了引導的幾何效應及其對收斂的影響。", "applications": ["AI繪圖助手：讓使用者指定風格或主題，AI就能精準生成符合需求的圖像，例如：『畫一張梵谷風格的貓咪』。", "醫療影像分析：協助醫生更準確地辨識X光片或MRI中的病灶，例如：『找出肺部CT影像中疑似腫瘤的區域』。", "語音合成：根據文字生成更自然、更具情感的語音，例如：『用溫柔的語氣朗讀這段睡前故事』。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能精準控制生成式AI的輸出結果，就像一位技藝精湛的畫家，能完全按照您的指令創作。想像一下，這項技術能讓AI繪圖工具不再隨機生成，而是精準呈現使用者想要的風格和細節；能讓醫療影像分析AI更準確地找出病灶，大幅提升診斷效率；能讓語音合成AI更富情感，創造更逼真的虛擬助手。這項技術的核心優勢在於其對擴散模型『引導』的精準控制，這意味著我們能大幅降低AI生成內容的隨機性，提高內容品質和實用性。未來，我們將把這項技術應用於各個領域，從娛樂、醫療到教育，打造一個由精準AI驅動的全新世界。我們相信，這項技術的商業價值將是無限的，現在加入我們，您將有機會成為這場AI革命的領航者！", "audio": "docs/data/audios/2506.10971v1.wav"}
{"query": "AI", "id": "2506.10927v1", "url": "http://arxiv.org/abs/2506.10927v1", "title": "The Role of Generative AI in Facilitating Social Interactions: A Scoping Review", "summary": "Reduced social connectedness increasingly poses a threat to mental health,\nlife expectancy, and general well-being. Generative AI (GAI) technologies, such\nas large language models (LLMs) and image generation tools, are increasingly\nintegrated into applications aimed at enhancing human social experiences.\nDespite their growing presence, little is known about how these technologies\ninfluence social interactions. This scoping review investigates how GAI-based\napplications are currently designed to facilitate social interaction, what\nforms of social engagement they target, and which design and evaluation\nmethodologies designers use to create and evaluate them. Through an analysis of\n30 studies published since 2020, we identify key trends in application domains\nincluding storytelling, socio-emotional skills training, reminiscence,\ncollaborative learning, music making, and general conversation. We highlight\nthe role of participatory and co-design approaches in fostering both effective\ntechnology use and social engagement, while also examining socio-ethical\nconcerns such as cultural bias and accessibility. This review underscores the\npotential of GAI to support dynamic and personalized interactions, but calls\nfor greater attention to equitable design practices and inclusive evaluation\nstrategies.", "authors": ["T. T. J. E. Arets", "G. Perugia", "M. Houben", "W. A. IJsselsteijn"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:53:11.065720", "title_zh": "生成式人工智慧在促進社交互動中的作用：範圍界定綜述", "summary_zh": "社交連結的減少對心理健康和壽命構成威脅。生成式AI（GAI），如大型語言模型和圖像生成工具，正被整合到增強人類社交體驗的應用中。本研究分析了30篇2020年後發表的文獻，探討GAI如何促進社交互動，以及設計者使用的設計和評估方法。研究發現，GAI在故事敘述、社交情感技能訓練、懷舊、協作學習、音樂創作和一般對話等領域有應用。強調參與式設計對技術有效性和社交互動的重要性，並關注文化偏見和可訪問性等社會倫理問題。GAI有潛力支持動態和個人化的互動，但需要更關注公平的設計實踐和包容性的評估策略。", "applications": ["想像一下，獨居長輩可以使用AI聊天機器人，不僅能閒話家常，還能根據長輩的興趣，播放懷舊歌曲、分享老照片，甚至協助長輩與遠方的親友視訊通話，讓他們感受到被關懷。", "學校可以利用AI設計角色扮演遊戲，讓學生在安全的情境下練習社交技巧，例如如何表達自己的想法、如何處理衝突，或者如何與不同文化背景的人溝通。", "企業可以使用AI工具來促進團隊合作，例如AI可以分析團隊成員的溝通模式，找出潛在的衝突點，並提供改善建議，或者協助團隊進行腦力激盪，產生更多創新想法。"], "pitch": "各位投資人，我們正站在一個社交革命的起點！孤獨感是21世紀的流行病，而我們的技術正是解藥。生成式AI不僅僅是個工具，它是連接人與人的橋樑，是情感的催化劑。想像一下，一個AI伴侶能為自閉症兒童提供社交訓練，一個AI導師能為內向者提供自信表達的指導。我們的技術能打破地理限制，讓世界各地的人們建立有意義的連結。市場潛力巨大！從銀髮照護到青少年心理健康，從企業團隊建設到國際文化交流，我們的技術無所不在。我們不僅僅在開發AI，我們在創造更美好、更互聯的世界。現在投資我們，你投資的不僅僅是一個公司，而是投資一個更有愛的未來！未來，我們甚至可以打造一個完全由AI驅動的虛擬社交平台，讓使用者在其中體驗前所未有的社交互動，創造無限商機！", "audio": "docs/data/audios/2506.10927v1.wav"}
{"query": "Foundation Model", "id": "2506.10579v1", "url": "http://arxiv.org/abs/2506.10579v1", "title": "Equations of state and stability condition of mixed p-spin glass model", "summary": "The Sherrington-Kirkpatrick (SK) is a foundational model for understanding\nspin glass systems. It is based on the pairwise interaction between each two\nspins in a fully connected lattice with quenched disordered interactions. The\nnature of long-range interaction among spins in the (SK) model simplifies the\nstudy of this system by eliminating fluctuations. An advanced (SK) model, known\nas the p-spin model, introduces higher-order interactions that involve the\ninteraction of P spins. This research focuses on the general Hamiltonian of the\nspin glass model with long-range interaction, referred to as the mixed p-spin\nglass model, which consists of adding all p-spin interaction terms. This\nresearch aims to derive the equation of states for this Hamiltonian, formulate\nthe equation of state within the framework of the first replica symmetry\nbreaking, and determine both the stability condition of the replica symmetric\nsolution and the stability of the replicas belonging to the same group in the\nfirst step of replica symmetry breaking.", "authors": ["Ali Talebi"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:54:21.164725", "title_zh": "混合p-自旋玻璃模型的狀態方程式與穩定性條件", "summary_zh": "本研究探討一種更複雜的自旋玻璃模型，稱為混合p-自旋模型。這種模型考慮了多個自旋之間的交互作用，而不僅僅是兩兩之間的交互作用。研究目標是推導出此模型的狀態方程式，並在第一步複製對稱性破壞的框架下，確定複製對稱解的穩定性條件，以及屬於同一組的複製品的穩定性。這有助於更深入理解複雜系統的行為，例如材料的磁性或神經網路的運作。", "applications": ["材料科學：想像一下，我們可以設計出具有特定磁性或電性的新型材料，應用於更高效的儲存設備或更靈敏的感測器。例如，開發出在極端溫度下仍能穩定運作的磁性材料。", "神經網路：這個模型有助於我們理解大腦中神經元之間的複雜互動，進而開發出更強大的人工智慧。例如，設計出更有效率的機器學習演算法，提升AI的學習速度和準確性。", "金融風險管理：金融市場的波動性也可以用類似的模型來描述。透過理解這些複雜的交互作用，我們可以更準確地預測市場風險，並設計出更有效的風險管理策略。"], "pitch": "各位投資人，我們正在開發一種革命性的模型，能夠解開複雜系統的底層奧秘。混合p-自旋玻璃模型不僅僅是一個學術概念，它擁有巨大的商業潛力。想像一下，如果我們能夠精準預測新材料的特性，加速新藥開發，甚至預測金融市場的崩盤，這將帶來多大的價值？我們的研究團隊已經在這個領域取得了突破性進展，掌握了關鍵的狀態方程式和穩定性條件。我們相信，透過您的投資，我們可以將這個模型應用於各個領域，從材料科學到人工智慧，再到金融科技，創造一個全新的產業生態。這不僅是一項投資，更是一場科技革命，讓我們一起引領未來！", "audio": "docs/data/audios/2506.10579v1.wav"}
{"query": "Diffusion Model", "id": "2506.10963v1", "url": "http://arxiv.org/abs/2506.10963v1", "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "summary": "In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning--a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits--low entity fidelity, weak\nrelations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,\nunderscoring the benchmark's difficulty. To spur further progress, we release\nFLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines\na reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.", "authors": ["Yuxuan Luo", "Yuhui Yuan", "Junwen Chen", "Haonan Cai", "Ziyi Yue", "Yuwei Yang", "Fatima Zohra Daha", "Ji Li", "Zhouhui Lian"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:55:51.388040", "title_zh": "MMMG：一個用於文字生成圖像推理的大規模、多學科、多層級生成基準", "summary_zh": "本研究提出知識圖像生成的新任務，並創建了大規模多學科多層級知識圖像生成基準（MMMG），以評估圖像生成模型的推理能力。知識圖像對人類文明和學習至關重要。MMMG包含4456個專家驗證的圖像提示對，涵蓋10個學科、6個教育程度和多種知識格式。為簡化評估，採用統一的知識圖譜（KG）表示。我們還引入MMMG-Score來評估生成的知識圖像，結合了基於圖編輯距離的事實準確性和視覺清晰度評估。評估顯示，現有模型存在推理缺陷，GPT-4o的MMMG-Score僅為50.20。我們發布了FLUX-Reason，一個有效的開源基準，結合推理LLM和擴散模型，並在16000個知識圖像提示對上進行訓練。", "applications": ["**輔助學習：** 小明在學習歷史，只要輸入「秦始皇焚書坑儒」，系統就能自動生成一張圖，清楚呈現事件的來龍去脈，幫助他更快理解和記憶。", "**簡報製作：** 企劃人員小美要向老闆報告市場分析，只要輸入「近五年飲料市場趨勢圖」，系統就能快速生成專業的圖表，省下大量製作時間。", "**兒童教育：** 爸媽想教小朋友認識動物，只要輸入「獅子的生活習性」，系統就能生成一張生動有趣的圖片，配上簡單的文字說明，讓小朋友輕鬆學習。"], "pitch": "各位投資人，我們正在開發一項顛覆性的技術：基於MMMG基準的知識圖像生成引擎。想像一下，未來人們不再需要死記硬背，只要輸入文字，就能立即生成清晰易懂的知識圖像，大幅提升學習效率和資訊傳播速度。這項技術的應用範圍極廣，從教育、醫療到商業簡報，都能看到它的身影。我們相信，隨著AI技術的發展，視覺化知識將成為主流。我們的團隊將持續優化模型，擴大知識庫，並積極探索商業模式，例如訂閱制、API服務等。我們預計在三年內，搶佔知識圖像生成市場的領導地位，為投資者帶來豐厚的回報！這不僅僅是一項技術，更是一場知識革命，邀請您與我們一同參與，共同創造未來！", "audio": "docs/data/audios/2506.10963v1.wav"}
{"query": "AI", "id": "2506.10916v1", "url": "http://arxiv.org/abs/2506.10916v1", "title": "Semi-Automated Quality Assurance in Digital Pathology: Tile Classification Approach", "summary": "Quality assurance is a critical but underexplored area in digital pathology,\nwhere even minor artifacts can have significant effects. Artifacts have been\nshown to negatively impact the performance of AI diagnostic models. In current\npractice, trained staff manually review digitized images prior to release of\nthese slides to pathologists which are then used to render a diagnosis.\nConventional image processing approaches, provide a foundation for detecting\nartifacts on digital pathology slides. However, current tools do not leverage\ndeep learning, which has the potential to improve detection accuracy and\nscalability. Despite these advancements, methods for quality assurance in\ndigital pathology remain limited, presenting a gap for innovation.\n  We propose an AI algorithm designed to screen digital pathology slides by\nanalyzing tiles and categorizing them into one of 10 predefined artifact types\nor as background. This algorithm identifies and localizes artifacts, creating a\nmap that highlights regions of interest. By directing human operators to\nspecific tiles affected by artifacts, the algorithm minimizes the time and\neffort required to manually review entire slides for quality issues.\n  From internal archives and The Cancer Genome Atlas, 133 whole slide images\nwere selected and 10 artifacts were annotated using an internally developed\nsoftware ZAPP (Mayo Clinic, Jacksonville, FL). Ablation study of multiple\nmodels at different tile sizes and magnification was performed. InceptionResNet\nwas selected. Single artifact models were trained and tested, followed by a\nlimited multiple instance model with artifacts that performed well together\n(chatter, fold, and pen). From the results of this study we suggest a hybrid\ndesign for artifact screening composed of both single artifact binary models as\nwell as multiple instance models to optimize detection of each artifact.", "authors": ["Meredith VandeHaar", "M. Clinch", "I. Yilmaz", "M. A. Rahman", "Y. Xiao", "F. Dogany", "H. M. Alazab", "A. Nassar", "Z. Akkus", "B. Dangott"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:28:20.680718", "title_zh": "數位病理學中的半自動化品質保證：切片分類方法", "summary_zh": "數位病理切片中的偽影會嚴重影響AI診斷模型的準確性。目前仰賴人工檢查數位化切片，耗時費力。本研究提出一種AI演算法，通過分析切片並將其分類為10種預定義的偽影類型或背景，來篩選數位病理切片。該演算法能識別並定位偽影，生成突出顯示感興趣區域的地圖，引導操作員關注受偽影影響的特定切片，從而最大限度地減少人工審查整個切片以發現品質問題所需的時間和精力。實驗結果表明，採用單一偽影二元模型和多重實例模型的混合設計，能優化各種偽影的檢測。", "applications": ["醫院品管人員可以利用這套系統快速篩檢數位病理切片，確保診斷品質，減少人為疏失。", "遠距醫療平台可以整合這項技術，讓病理專家在線上就能快速判斷切片品質，提升遠端診斷的效率和準確性。", "病理學系的學生可以利用這套系統學習辨識各種偽影，加速學習進度，提升專業能力。"], "pitch": "各位投資人，我們正在革新數位病理學的品質保證！想像一下，AI醫生在診斷癌症時，不會再因為切片上的污漬或摺痕而誤判。我們的半自動化品質保證系統，能精準識別並標記數位病理切片中的偽影，大幅提升AI診斷的準確性與效率。這不僅能減少醫療失誤，更能加速新藥開發，甚至實現個人化醫療。數位病理市場正以驚人的速度成長，而我們的技術將成為這個市場的關鍵基礎設施。我們預計，在未來五年內，所有大型醫院和研究機構都將採用類似的解決方案。現在投資，您將成為這場醫療AI革命的領頭羊，共同打造一個更精準、更高效的醫療未來！", "audio": "docs/data/audios/2506.10916v1.wav"}
{"query": "Foundation Model", "id": "2506.10395v1", "url": "http://arxiv.org/abs/2506.10395v1", "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation", "summary": "Recent advances in large language models (LLMs) have enabled multimodal\nfoundation models to tackle both image understanding and generation within a\nunified framework. Despite these gains, unified models often underperform\ncompared to specialized models in either task. A key challenge in developing\nunified models lies in the inherent differences between the visual features\nneeded for image understanding versus generation, as well as the distinct\ntraining processes required for each modality. In this work, we introduce\nPisces, an auto-regressive multimodal foundation model that addresses this\nchallenge through a novel decoupled visual encoding architecture and tailored\ntraining techniques optimized for multimodal generation. Combined with\nmeticulous data curation, pretraining, and finetuning, Pisces achieves\ncompetitive performance in both image understanding and image generation. We\nevaluate Pisces on over 20 public benchmarks for image understanding, where it\ndemonstrates strong performance across a wide range of tasks. Additionally, on\nGenEval, a widely adopted benchmark for image generation, Pisces exhibits\nrobust generative capabilities. Our extensive analysis reveals the synergistic\nrelationship between image understanding and generation, and the benefits of\nusing separate visual encoders, advancing the field of unified multimodal\nmodels.", "authors": ["Zhiyang Xu", "Jiuhai Chen", "Zhaojiang Lin", "Xichen Pan", "Lifu Huang", "Tianyi Zhou", "Madian Khabsa", "Qifan Wang", "Di Jin", "Michihiro Yasunaga", "Lili Yu", "Xi Victoria Lin", "Shaoliang Nie"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:29:25.920042", "title_zh": "雙魚座：用於圖像理解與生成的自迴歸基礎模型", "summary_zh": "本研究提出名為「雙魚座」的自迴歸多模態基礎模型，旨在解決圖像理解與生成在統一模型中表現不佳的問題。雙魚座採用獨特的解耦視覺編碼架構，並針對多模態生成進行優化訓練。透過精心的數據管理、預訓練與微調，雙魚座在圖像理解和生成方面均展現了卓越的性能。在超過20個公共圖像理解基準測試中，雙魚座表現出色；在GenEval圖像生成基準測試中，也展現了強大的生成能力。研究揭示了圖像理解與生成之間的協同關係，以及使用獨立視覺編碼器的優勢，推動了統一多模態模型領域的發展。", "applications": ["智慧相機：相機能自動理解照片內容，例如識別出人物、地點、物品，並根據場景提供最佳拍攝建議，甚至自動生成風格化的照片。", "輔助設計：設計師可以口頭描述設計理念，系統自動生成草圖或3D模型，加速設計流程，並提供更多創意靈感。", "教育娛樂：孩子可以與AI互動，描述想像中的畫面，AI立即生成圖像，激發孩子創造力，並學習圖像相關知識。"], "pitch": "各位投資人，我們相信圖像理解和生成技術是AI的下一個爆點！「雙魚座」模型在統一架構下同時實現了卓越的圖像理解和生成能力，打破了傳統模型在兩者間的trade-off。想像一下，未來電商平台能根據用戶上傳的房屋照片，自動推薦合適的家具擺設；醫療領域能根據X光片自動診斷疾病；自動駕駛能更精準地理解周遭環境。雙魚座不僅能賦能現有產業，更能催生全新的商業模式。我們團隊擁有頂尖的AI專家和豐富的產業經驗，現在加入，您將有機會成為下一波AI浪潮的領航者！我們預計在三年內，雙魚座能成為市場上最受歡迎的圖像AI引擎，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.10395v1.wav"}
{"query": "Diffusion Model", "id": "2506.10962v1", "url": "http://arxiv.org/abs/2506.10962v1", "title": "SpectralAR: Spectral Autoregressive Visual Generation", "summary": "Autoregressive visual generation has garnered increasing attention due to its\nscalability and compatibility with other modalities compared with diffusion\nmodels. Most existing methods construct visual sequences as spatial patches for\nautoregressive generation. However, image patches are inherently parallel,\ncontradicting the causal nature of autoregressive modeling. To address this, we\npropose a Spectral AutoRegressive (SpectralAR) visual generation framework,\nwhich realizes causality for visual sequences from the spectral perspective.\nSpecifically, we first transform an image into ordered spectral tokens with\nNested Spectral Tokenization, representing lower to higher frequency\ncomponents. We then perform autoregressive generation in a coarse-to-fine\nmanner with the sequences of spectral tokens. By considering different levels\nof detail in images, our SpectralAR achieves both sequence causality and token\nefficiency without bells and whistles. We conduct extensive experiments on\nImageNet-1K for image reconstruction and autoregressive generation, and\nSpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project\npage: https://huang-yh.github.io/spectralar/.", "authors": ["Yuanhui Huang", "Weiliang Chen", "Wenzhao Zheng", "Yueqi Duan", "Jie Zhou", "Jiwen Lu"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:30:56.769031", "title_zh": "SpectralAR：頻譜自迴歸視覺生成", "summary_zh": "現有的自迴歸視覺生成方法多以圖像的空間色塊作為序列，但色塊本質上是平行的，與自迴歸模型的因果關係相悖。本研究提出SpectralAR框架，從頻譜角度實現視覺序列的因果性。首先，利用嵌套頻譜標記將圖像轉換為有序的頻譜標記，由低頻到高頻表示圖像的不同細節層次。然後，以粗略到精細的方式，利用頻譜標記序列進行自迴歸生成。實驗證明，SpectralAR在ImageNet-1K圖像重建和自迴歸生成任務上表現出色，僅用64個標記和3.1億參數就達到了3.02 gFID。", "applications": ["智慧型修圖App：使用者只需簡單描述想要修改的內容，例如「讓天空更藍」、「讓花朵更鮮豔」，App就能利用SpectralAR技術，精準地調整圖像的頻譜分佈，生成高品質的修改後圖片。", "AI藝術創作工具：藝術家或設計師可以使用SpectralAR來生成獨特的藝術作品。透過調整頻譜參數，可以創造出各種風格的圖像，例如印象派、抽象派等，激發無限的創作靈感。", "醫療影像增強：SpectralAR可以應用於醫療影像處理，例如X光片或MRI掃描。透過增強特定頻率的信號，可以更清晰地顯示病灶，幫助醫生更準確地診斷疾病。"], "pitch": "各位投資人，想像一下，未來AI生成圖像不再需要耗費大量算力，也不再受限於模糊不清的細節。SpectralAR技術的誕生，正是要打破這些限制。它就像一個高效的圖像解碼器，能將圖像分解成頻譜信息，再以極高的效率重新組合，生成高品質的圖像。這意味著，我們可以將這項技術應用於各行各業：從電商平台的商品圖像優化，到遊戲公司的AI美術資源生成，再到電影特效的快速渲染，市場潛力巨大。更重要的是，SpectralAR的低算力需求，讓它非常適合部署在移動設備上，這將開啟一個全新的移動AI圖像處理時代。我們相信，SpectralAR將成為AI視覺領域的下一個獨角獸，現在加入，您將有機會共同見證這項技術顛覆世界的時刻！", "audio": "docs/data/audios/2506.10962v1.wav"}
{"query": "AI", "id": "2506.10908v1", "url": "http://arxiv.org/abs/2506.10908v1", "title": "Probably Approximately Correct Labels", "summary": "Obtaining high-quality labeled datasets is often costly, requiring either\nextensive human annotation or expensive experiments. We propose a method that\nsupplements such \"expert\" labels with AI predictions from pre-trained models to\nconstruct labeled datasets more cost-effectively. Our approach results in\nprobably approximately correct labels: with high probability, the overall\nlabeling error is small. This solution enables rigorous yet efficient dataset\ncuration using modern AI models. We demonstrate the benefits of the methodology\nthrough text annotation with large language models, image labeling with\npre-trained vision models, and protein folding analysis with AlphaFold.", "authors": ["Emmanuel J. Candès", "Andrew Ilyas", "Tijana Zrnic"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:36:07.748516", "title_zh": "可能近似正確標籤", "summary_zh": "取得高品質標記資料集通常成本高昂。本研究提出一種方法，利用預訓練模型的AI預測來補充「專家」標籤，以更具成本效益的方式構建標記資料集。我們的方案能產生「可能近似正確」的標籤：整體標記錯誤的機率很高，但錯誤率很小。此方案能使用現代AI模型進行嚴謹且高效的資料集管理。我們透過大型語言模型的文本標註、預訓練視覺模型的圖像標記，以及AlphaFold的蛋白質摺疊分析，展示了此方法的優勢。", "applications": ["想像一下，以後醫生看X光片，AI會先幫忙標記出可疑的地方，醫生再仔細檢查，這樣可以減少誤判，讓診斷更準確快速。", "以後網購，AI可以自動判斷商品圖片的內容，例如衣服的款式、顏色，消費者只要輸入關鍵字，就能更快找到想要的商品。", "農民伯伯可以用手機拍照，AI就能判斷農作物有沒有生病、缺什麼養分，及時採取措施，提高產量。"], "pitch": "各位投資人，我們正在革新資料標記領域！想像一下，現在AI訓練需要海量資料，但人工標記成本高昂且耗時。我們的技術能利用現有AI模型，輔助專家進行資料標記，大幅降低成本，同時保證標記品質。這就像是為AI訓練提供了一條高速公路，加速AI發展。我們已經在文本、圖像和生物領域驗證了其有效性，證明其廣泛的應用潛力。未來，我們可以將此技術應用於自動駕駛、醫療診斷、金融風控等各個領域，市場前景無限。現在投資我們，就是投資AI的未來！", "audio": "docs/data/audios/2506.10908v1.wav"}
{"query": "Foundation Model", "id": "2506.10386v1", "url": "http://arxiv.org/abs/2506.10386v1", "title": "Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial", "summary": "The burial state of anthropogenic objects on the seafloor provides insight\ninto localized sedimentation dynamics and is also critical for assessing\necological risks, potential pollutant transport, and the viability of recovery\nor mitigation strategies for hazardous materials such as munitions. Accurate\nburial depth estimation from remote imagery remains difficult due to partial\nocclusion, poor visibility, and object degradation. This work introduces a\ncomputer vision pipeline, called PoseIDON, which combines deep foundation model\nfeatures with multiview photogrammetry to estimate six degrees of freedom\nobject pose and the orientation of the surrounding seafloor from ROV video.\nBurial depth is inferred by aligning CAD models of the objects with observed\nimagery and fitting a local planar approximation of the seafloor. The method is\nvalidated using footage of 54 objects, including barrels and munitions,\nrecorded at a historic ocean dumpsite in the San Pedro Basin. The model\nachieves a mean burial depth error of approximately 10 centimeters and resolves\nspatial burial patterns that reflect underlying sediment transport processes.\nThis approach enables scalable, non-invasive mapping of seafloor burial and\nsupports environmental assessment at contaminated sites.", "authors": ["Jerry Yan", "Chinmay Talegaonkar", "Nicholas Antipa", "Eric Terrill", "Sophia Merrifield"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:37:10.124025", "title_zh": "利用6DoF姿態基礎模型繪製海洋沉積物覆蓋圖", "summary_zh": "這項研究提出名為PoseIDON的電腦視覺流程，結合深度學習基礎模型特徵與多視角攝影測量技術，從水下機器人影片中估算海底物體的六自由度姿態及周圍海床方向。藉由將物體的CAD模型與觀測影像對齊，並擬合海床局部平面近似，推斷埋藏深度。實驗結果顯示，該模型能以約10公分的平均誤差估算埋藏深度，並解析反映沉積物傳輸過程的空間埋藏模式。這項技術實現了海底埋藏的可擴展、非侵入式繪圖，並支持受污染場址的環境評估。", "applications": ["海洋工程：精準定位海底管線或結構物，避免施工時誤觸或損壞。", "環境監測：追蹤海底垃圾或有害物質的分布與移動，評估對生態環境的影響。", "考古研究：協助考古學家定位與記錄沉沒的古物或遺跡，重現歷史場景。"], "pitch": "各位投資人，我們正在革新海洋環境監測方式！想像一下，我們能精準掌握海底廢棄物、沉船殘骸，甚至是潛在的汙染源。PoseIDON技術不僅能繪製高精度的海底地圖，還能預測沉積物移動，協助我們保護海洋生態，降低潛在風險。這項技術的應用範圍廣泛，從海洋工程、環境監測到考古研究，都有巨大的商業潛力。隨著海洋資源的開發日益重要，PoseIDON將成為不可或缺的工具。我們預期，未來能將這項技術應用於深海採礦、海底能源開發等領域，創造更大的經濟效益。現在投資PoseIDON，就是投資海洋的未來！", "audio": "docs/data/audios/2506.10386v1.wav"}
{"query": "Diffusion Model", "id": "2506.10955v1", "url": "http://arxiv.org/abs/2506.10955v1", "title": "ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems", "summary": "There has been a flurry of activity around using pretrained diffusion models\nas informed data priors for solving inverse problems, and more generally around\nsteering these models using reward models. Training-free methods like diffusion\nposterior sampling (DPS) and its many variants have offered flexible heuristic\nalgorithms for these tasks, but when the reward is not informative enough,\ne.g., in hard inverse problems with low signal-to-noise ratio, these techniques\nveer off the data manifold, failing to produce realistic outputs. In this work,\nwe devise a simple wrapper, ReGuidance, for boosting both the sample realism\nand reward achieved by these methods. Given a candidate solution $\\hat{x}$\nproduced by an algorithm of the user's choice, we propose inverting the\nsolution by running the unconditional probability flow ODE in reverse starting\nfrom $\\hat{x}$, and then using the resulting latent as an initialization for\nDPS. We evaluate our wrapper on hard inverse problems like large box\nin-painting and super-resolution with high upscaling. Whereas state-of-the-art\nbaselines visibly fail, we find that applying our wrapper on top of these\nbaselines significantly boosts sample quality and measurement consistency. We\ncomplement these findings with theory proving that on certain multimodal data\ndistributions, ReGuidance simultaneously boosts the reward and brings the\ncandidate solution closer to the data manifold. To our knowledge, this\nconstitutes the first rigorous algorithmic guarantee for DPS.", "authors": ["Aayush Karan", "Kulin Shah", "Sitan Chen"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:38:29.432841", "title_zh": "ReGuidance：一個用於提升困難反問題樣本品質的簡單擴散模型封裝器", "summary_zh": "本研究提出ReGuidance，一個簡單的封裝器，旨在提升預訓練擴散模型在解決反問題時的樣本真實性和獎勵。針對如擴散後驗採樣(DPS)等無訓練方法在低訊噪比的困難反問題中表現不佳的問題，ReGuidance通過反轉候選解，將其轉換為潛在空間的初始化點，再利用DPS進行優化。實驗結果顯示，在大型圖像修復和高倍率超解析度等任務中，ReGuidance能顯著提升樣本品質和一致性，超越現有技術。理論分析也證明，ReGuidance能同時提升獎勵並使候選解更接近數據流形，為DPS提供了首個嚴格的演算法保證。", "applications": ["老照片修復：透過ReGuidance技術，可以將模糊不清的老照片還原成清晰、細節豐富的影像，讓珍貴的回憶重現光彩。", "醫療影像增強：在X光片或MRI等醫療影像中，ReGuidance能提升影像解析度，幫助醫生更精準地診斷疾病，例如偵測微小的腫瘤。", "犯罪現場重建：利用ReGuidance技術，可以將模糊的監視器畫面清晰化，幫助警方還原犯罪現場的細節，提高破案率。"], "pitch": "各位創投先進，我們正在開發一項突破性的圖像處理技術，ReGuidance，它能大幅提升反問題的解決方案品質，尤其是在現有技術難以應付的困難場景。想像一下，未來我們可以將模糊的衛星圖像轉化為高解析度的地圖，用於精準農業、城市規劃，甚至國防安全。或者，我們能將低畫質的影片升級為4K甚至8K，重塑影視娛樂產業。ReGuidance不僅能提升現有圖像處理技術的性能，更開啟了全新的商業模式。我們預計，隨著AIoT和元宇宙的發展，ReGuidance在智慧城市、自動駕駛、虛擬實境等領域將有巨大的應用潛力。現在投資，您將成為這場圖像革命的先驅！", "audio": "docs/data/audios/2506.10955v1.wav"}
{"query": "AI", "id": "2506.10897v1", "url": "http://arxiv.org/abs/2506.10897v1", "title": "GenPlanX. Generation of Plans and Execution", "summary": "Classical AI Planning techniques generate sequences of actions for complex\ntasks. However, they lack the ability to understand planning tasks when\nprovided using natural language. The advent of Large Language Models (LLMs) has\nintroduced novel capabilities in human-computer interaction. In the context of\nplanning tasks, LLMs have shown to be particularly good in interpreting human\nintents among other uses. This paper introduces GenPlanX that integrates LLMs\nfor natural language-based description of planning tasks, with a classical AI\nplanning engine, alongside an execution and monitoring framework. We\ndemonstrate the efficacy of GenPlanX in assisting users with office-related\ntasks, highlighting its potential to streamline workflows and enhance\nproductivity through seamless human-AI collaboration.", "authors": ["Daniel Borrajo", "Giuseppe Canonaco", "Tomás de la Rosa", "Alfredo Garrachón", "Sriram Gopalakrishnan", "Simerjot Kaur", "Marianela Morales", "Sunandita Patra", "Alberto Pozanco", "Keshav Ramani", "Charese Smiley", "Pietro Totis", "Manuela Veloso"], "published_date": "2025-06-12", "timestamp": "2025-06-13T21:24:27.999205", "title_zh": "GenPlanX：計畫生成與執行", "summary_zh": "GenPlanX結合大型語言模型（LLM）與傳統AI規劃技術，讓使用者能以自然語言描述複雜任務，並自動生成行動方案。系統包含一個執行與監控框架，確保計畫順利進行。GenPlanX擅長理解人類意圖，能協助處理辦公室相關任務，簡化工作流程，提升生產力。透過無縫的人機協作，GenPlanX展現了強大的效率和便利性，為AI在任務管理領域的應用帶來了新的可能性。", "applications": ["想像一下，你可以用口語告訴GenPlanX：『幫我安排明天早上九點的會議，通知所有相關人員，並準備好會議簡報。』GenPlanX就能自動完成這些繁瑣的步驟。", "如果你是餐廳老闆，你可以對GenPlanX說：『今天晚餐時段，如果排隊超過15分鐘，就發送優惠券給顧客，並加派人手。』GenPlanX就能根據實際情況自動調整營運策略。", "對於行動不便的人士，他們可以透過GenPlanX控制智慧家居設備，例如：『幫我打開客廳的燈，並將室溫調到25度。』GenPlanX就能理解並執行這些指令，提升生活品質。"], "pitch": "各位投資人，我們正處於AI賦能的黃金時代！GenPlanX不僅僅是一個AI規劃工具，它是一個革命性的生產力平台。試想一下，未來的辦公室裡，員工不再需要耗費大量時間處理重複性任務，GenPlanX能自動完成，讓員工專注於更具創造性的工作。這將大幅提升企業的運營效率和競爭力。更進一步，GenPlanX可以應用於智慧城市管理、智慧工廠、甚至是個人化的健康管理。我們預期GenPlanX將成為未來人機協作的關鍵基礎設施，市場潛力無限。現在投資GenPlanX，就是投資未來！我們有信心在五年內將GenPlanX打造成為市場領導者，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.10897v1.wav"}
{"query": "Foundation Model", "id": "2506.10335v1", "url": "http://arxiv.org/abs/2506.10335v1", "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting", "summary": "3D Gaussian splatting (3DGS) is an innovative rendering technique that\nsurpasses the neural radiance field (NeRF) in both rendering speed and visual\nquality by leveraging an explicit 3D scene representation. Existing 3DGS\napproaches require a large number of calibrated views to generate a consistent\nand complete scene representation. When input views are limited, 3DGS tends to\noverfit the training views, leading to noticeable degradation in rendering\nquality. To address this limitation, we propose a Point-wise Feature-Aware\nGaussian Splatting framework that enables real-time, high-quality rendering\nfrom sparse training views. Specifically, we first employ the latest stereo\nfoundation model to estimate accurate camera poses and reconstruct a dense\npoint cloud for Gaussian initialization. We then encode the colour attributes\nof each 3D Gaussian by sampling and aggregating multiscale 2D appearance\nfeatures from sparse inputs. To enhance point-wise appearance representation,\nwe design a point interaction network based on a self-attention mechanism,\nallowing each Gaussian point to interact with its nearest neighbors. These\nenriched features are subsequently decoded into Gaussian parameters through two\nlightweight multi-layer perceptrons (MLPs) for final rendering. Extensive\nexperiments on diverse benchmarks demonstrate that our method significantly\noutperforms NeRF-based approaches and achieves competitive performance under\nfew-shot settings compared to the state-of-the-art 3DGS methods.", "authors": ["Lintao Xiang", "Hongpei Zheng", "Yating Huang", "Qijun Yang", "Hujun Yin"], "published_date": "2025-06-12", "timestamp": "2025-06-13T21:25:49.757884", "title_zh": "PointGS：基於點注意力感知與高斯濺灑的稀疏視角合成", "summary_zh": "本研究提出PointGS，一種基於高斯濺灑（3DGS）的新框架，旨在解決在稀疏視角下3D場景重建品質下降的問題。PointGS首先利用最新的立體基礎模型估計精確的相機姿態，並重建密集點雲以初始化高斯分佈。接著，通過採樣和聚合來自稀疏輸入的多尺度2D外觀特徵，編碼每個3D高斯的顏色屬性。此外，設計了一個基於自注意力機制的點交互網路，增強點級外觀表示。實驗結果表明，PointGS在稀疏視角下，顯著優於基於NeRF的方法，並在效能上與最先進的3DGS方法競爭。", "applications": ["線上購物：想像一下，你可以在網路上用手機掃描房間，就能立刻看到新沙發擺在家裡的樣子，而且非常真實，就像真的擺在那邊一樣，不再需要靠想像。", "室內設計：設計師可以直接用少量照片快速建立房屋的3D模型，並即時調整設計，客戶也能透過手機或平板，身歷其境地感受設計成果。", "遊戲開發：遊戲開發者能更快速地建立逼真的3D遊戲場景，減少建模時間，提升遊戲的真實感和沉浸感。"], "pitch": "各位投資人，我們PointGS技術，是3D建模領域的革命性突破！傳統NeRF技術需要大量照片才能生成高品質3D模型，而我們的技術，只需要少量照片，就能達到甚至超越的效果！這意味著什麼？意味著成本大幅降低，效率顯著提升！想像一下，未來電商平台可以輕鬆提供AR試穿、試擺服務，房地產商可以打造更真實的線上看房體驗，遊戲公司可以快速創造出令人驚豔的3A級場景。我們的技術，是元宇宙的基石，是虛擬實境的引擎，更是各行各業數位轉型的加速器！我們團隊擁有頂尖的AI和3D建模專家，我們相信，PointGS將引領下一代3D內容的發展，成為市場領導者。現在投資我們，您將搭上元宇宙的早班車，共同開創一個全新的數位時代！", "audio": "docs/data/audios/2506.10335v1.wav"}
{"query": "Diffusion Model", "id": "2506.10892v1", "url": "http://arxiv.org/abs/2506.10892v1", "title": "The Diffusion Duality", "summary": "Uniform-state discrete diffusion models hold the promise of fast text\ngeneration due to their inherent ability to self-correct. However, they are\ntypically outperformed by autoregressive models and masked diffusion models. In\nthis work, we narrow this performance gap by leveraging a key insight:\nUniform-state diffusion processes naturally emerge from an underlying Gaussian\ndiffusion. Our method, Duo, transfers powerful techniques from Gaussian\ndiffusion to improve both training and sampling. First, we introduce a\ncurriculum learning strategy guided by the Gaussian process, doubling training\nspeed by reducing variance. Models trained with curriculum learning surpass\nautoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we\npresent Discrete Consistency Distillation, which adapts consistency\ndistillation from the continuous to the discrete setting. This algorithm\nunlocks few-step generation in diffusion language models by accelerating\nsampling by two orders of magnitude. We provide the code and model checkpoints\non the project page: http://s-sahoo.github.io/duo", "authors": ["Subham Sekhar Sahoo", "Justin Deschenaux", "Aaron Gokaslan", "Guanghan Wang", "Justin Chiu", "Volodymyr Kuleshov"], "published_date": "2025-06-12", "timestamp": "2025-06-13T21:26:58.099685", "title_zh": "擴散二元性", "summary_zh": "本研究提出一種名為Duo的方法，旨在提升均勻狀態離散擴散模型在文本生成方面的效能。透過觀察到均勻狀態擴散過程源於底層的高斯擴散，Duo將高斯擴散的強大技術轉移到離散擴散模型，從而改善訓練和採樣。研究中引入了基於高斯過程的課程學習策略，透過降低方差，使訓練速度加倍。此外，研究還提出了離散一致性蒸餾算法，加速採樣達兩個數量級，實現了擴散語言模型中的少步生成。實驗結果顯示，使用課程學習訓練的模型在多個基準測試中超越了自迴歸模型。", "applications": ["**AI寫作助手：** 想像一下，你只需要提供幾個關鍵字，AI就能快速生成高品質的文章、新聞稿或產品描述，省時省力，讓內容創作變得更加輕鬆。", "**即時翻譯：** 未來，AI翻譯不再需要等待，可以即時生成流暢自然的翻譯文本，打破語言障礙，促進跨文化交流。", "**遊戲對話生成：** 遊戲中的NPC對話可以更加生動有趣，AI可以根據玩家的行為和選擇，生成個性化的對話內容，提升遊戲體驗。"], "pitch": "各位創投，我們正站在AI內容生成的下一個浪潮之上！Duo技術突破了傳統擴散模型的瓶頸，實現了更快速、更高效的文本生成。想像一下，一個可以瞬間生成高品質內容的AI引擎，它將顛覆內容創作、行銷、教育等各個領域。我們的技術不僅能大幅降低內容生產成本，更能創造出前所未有的商業模式。從個性化廣告到AI寫作工具，從智能客服到虛擬偶像，Duo的應用潛力無限。現在投資Duo，就是投資AI內容生成的未來，我們堅信，Duo將引領下一代AI內容革命，為您帶來超乎想像的回報！", "audio": "docs/data/audios/2506.10892v1.wav"}
{"query": "AI", "id": "2506.10890v1", "url": "http://arxiv.org/abs/2506.10890v1", "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation", "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter", "authors": ["Zhao Zhang", "Yutao Cheng", "Dexiang Hong", "Maoke Yang", "Gonglei Shi", "Lei Ma", "Hui Zhang", "Jie Shao", "Xinglong Wu"], "published_date": "2025-06-12", "timestamp": "2025-06-14T01:58:54.526246", "title_zh": "CreatiPoster：邁向可編輯與可控制的多層圖形設計生成", "summary_zh": "CreatiPoster是一個能從自然語言指令或素材生成可編輯多層圖形設計的框架。它利用RGBA大型多模態模型生成JSON規格，詳細描述每一圖層（文字或素材）的佈局、層級、內容與風格，並生成簡潔的背景提示。接著，條件背景模型會根據前景圖層合成連貫的背景。我們建立基準測試，證明CreatiPoster超越了領先的開源方法和商業系統。我們還釋出了10萬個多層設計的免費版權語料庫，以促進進一步研究。CreatiPoster支援畫布編輯、文字疊加、響應式調整大小、多語言適配和動畫海報等應用，推動AI輔助圖形設計的普及。", "applications": ["想像一下，你想要在社群媒體上發布一張吸睛的活動宣傳海報，但你沒有設計經驗。使用CreatiPoster，你只需要輸入活動主題和一些關鍵字，它就能自動生成多款專業級的海報設計，讓你輕鬆選擇最喜歡的並進行微調。", "假設你是一位小型企業主，需要快速製作產品宣傳單張。有了CreatiPoster，你只需上傳產品圖片和簡單描述，它就能立即生成多種排版精美的宣傳單張，省去聘請設計師的費用和時間。", "如果你是一位老師，需要製作一份生動有趣的教學簡報。CreatiPoster可以讓你輕鬆添加文字、圖片和動畫效果，即使沒有任何設計基礎，也能快速製作出專業級的簡報，提升教學效果。"], "pitch": "各位投資人，我們向您推薦CreatiPoster，這是一個革命性的AI圖形設計平台，它將徹底改變圖形設計行業。目前，圖形設計高度依賴專業設計師，成本高昂且耗時。CreatiPoster利用最先進的AI技術，讓任何人都能輕鬆生成高品質、可編輯的圖形設計，大幅降低設計門檻和成本。想像一下，未來每個人都能成為自己的設計師，企業不再需要花費巨額預算聘請設計團隊。CreatiPoster的潛在市場規模巨大，涵蓋社群媒體、電子商務、教育、廣告等各個領域。我們相信，CreatiPoster將成為圖形設計領域的領頭羊，為投資者帶來豐厚的回報。我們不僅僅是在開發一個工具，我們是在打造一個全新的設計生態系統，一個讓創意無限延伸的未來。", "audio": "docs/data/audios/2506.10890v1.wav"}
{"query": "Foundation Model", "id": "2506.10230v1", "url": "http://arxiv.org/abs/2506.10230v1", "title": "Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation", "summary": "Latent diffusion models (LDM) could alleviate data scarcity challenges\naffecting machine learning development for medical imaging. However, medical\nLDM training typically relies on performance- or scientific\naccessibility-limiting strategies including a reliance on short-prompt text\nencoders, the reuse of non-medical LDMs, or a requirement for fine-tuning with\nlarge data volumes. We propose a Class-Conditioned Efficient Large Language\nmodel Adapter (CCELLA) to address these limitations. CCELLA is a novel\ndual-head conditioning approach that simultaneously conditions the LDM U-Net\nwith non-medical large language model-encoded text features through\ncross-attention and with pathology classification through the timestep\nembedding. We also propose a joint loss function and a data-efficient LDM\ntraining framework. In combination, these strategies enable\npathology-conditioned LDM training for high-quality medical image synthesis\ngiven limited data volume and human data annotation, improving LDM performance\nand scientific accessibility. Our method achieves a 3D FID score of 0.025 on a\nsize-limited prostate MRI dataset, significantly outperforming a recent\nfoundation model with FID 0.071. When training a classifier for prostate cancer\nprediction, adding synthetic images generated by our method to the training\ndataset improves classifier accuracy from 69% to 74%. Training a classifier\nsolely on our method's synthetic images achieved comparable performance to\ntraining on real images alone.", "authors": ["Emerson P. Grabke", "Masoom A. Haider", "Babak Taati"], "published_date": "2025-06-11", "timestamp": "2025-06-14T02:00:24.987299", "title_zh": "基於提示引導的潛在擴散模型與預測類別條件的3D前列腺MRI生成", "summary_zh": "本研究提出一種名為CCELLA的新穎雙頭條件方法，旨在解決醫學影像機器學習開發中數據稀缺的問題。CCELLA透過交叉注意力將大型語言模型編碼的文本特徵和時間步嵌入的病理分類同時作用於潛在擴散模型的U-Net。此外，我們還提出聯合損失函數和數據高效的訓練框架。實驗結果表明，在有限的數據量和人工標註下，我們的模型能有效生成高質量的病理條件醫學影像，顯著提升模型效能與易用性。在3D前列腺MRI數據集上，我們的模型取得了優異的FID分數，並在提升前列腺癌預測分類器的準確度方面展現了卓越的性能。", "applications": ["想像一下，未來醫生可以利用這個技術，輸入簡單的描述，例如「早期前列腺癌」，就能自動生成大量不同階段、不同病灶位置的前列腺MRI影像。這能幫助醫生更全面地了解病情，制定更精準的治療方案。", "對於醫學院學生來說，這項技術就像一個虛擬的病患資料庫。學生可以透過調整提示詞，模擬各種罕見病例，提升診斷能力，而不用擔心找不到真實案例。", "藥廠可以利用這個技術來模擬藥物對不同病患的影響。透過生成大量虛擬MRI影像，觀察藥物在不同病理條件下的作用，加速新藥研發進程，並降低臨床試驗的風險。"], "pitch": "各位投資人，我們團隊開發的CCELLA技術，正在徹底革新醫學影像領域！想像一下，AI能夠根據簡單的描述，自動生成無限量的、高度逼真的3D醫療影像。這不僅能解決醫學影像數據稀缺的難題，更能加速疾病診斷、藥物研發，甚至實現個性化醫療。目前，我們在前列腺MRI生成方面已經取得了突破性進展，未來可擴展到其他器官、其他疾病，打造一個龐大的、可定制的虛擬病患資料庫。這將是一個數十億美元的市場，我們有信心成為這個領域的領導者。現在加入我們，您將有機會參與一場醫學影像的革命，共同創造一個更健康、更智能的未來！我們預計五年內，該技術可以授權給各大醫院、醫學院和藥廠，產生穩定的現金流，並在資本市場上獲得高度認可。", "audio": "docs/data/audios/2506.10230v1.wav"}
{"query": "Diffusion Model", "id": "2506.10776v1", "url": "http://arxiv.org/abs/2506.10776v1", "title": "ME: Trigger Element Combination Backdoor Attack on Copyright Infringement", "summary": "The capability of generative diffusion models (DMs) like Stable Diffusion\n(SD) in replicating training data could be taken advantage of by attackers to\nlaunch the Copyright Infringement Attack, with duplicated poisoned image-text\npairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew\noutstanding performance in attacking SD in text-to-image tasks. However, the\nfeasible data resources in this area are still limited, some of them are even\nconstrained or prohibited due to the issues like copyright ownership or\ninappropriate contents; And not all of the images in current datasets are\nsuitable for the proposed attacking methods; Besides, the state-of-the-art\n(SoTA) performance of SBD is far from ideal when few generated poisoning\nsamples could be adopted for attacks. In this paper, we raised new datasets\naccessible for researching in attacks like SBD, and proposed Multi-Element (ME)\nattack method based on SBD by increasing the number of poisonous visual-text\nelements per poisoned sample to enhance the ability of attacking, while\nimporting Discrete Cosine Transform (DCT) for the poisoned samples to maintain\nthe stealthiness. The Copyright Infringement Rate (CIR) / First Attack Epoch\n(FAE) we got on the two new datasets were 16.78% / 39.50 and 51.20% / 23.60,\nrespectively close to or even outperformed benchmark Pokemon and Mijourney\ndatasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI\nand DCT earned CIR / FAE of 0.23% / 84.00 and 12.73% / 65.50, both better than\noriginal SBD, which failed to attack at all.", "authors": ["Feiyu Yang", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "published_date": "2025-06-12", "timestamp": "2025-06-14T02:01:44.303869", "title_zh": "ME：針對著作權侵權的觸發元素組合後門攻擊", "summary_zh": "本研究針對生成式擴散模型（如Stable Diffusion）在複製訓練資料時可能遭受的著作權侵權攻擊，提出了一種名為「多元素（ME）攻擊」的新方法。此方法基於現有的SilentBadDiffusion (SBD) 攻擊，通過增加每個受汙染樣本中毒性視覺文本元素的數量，來增強攻擊能力。同時，引入離散餘弦變換（DCT）來保持受汙染樣本的隱蔽性。實驗結果顯示，在新的數據集上，ME攻擊在著作權侵權率和首次攻擊週期方面均表現出色，甚至超越了現有基準。即使在低採樣率下，ME攻擊也優於原始SBD，展現了更強的攻擊能力和隱蔽性，對生成式AI的安全性提出了新的挑戰。", "applications": ["情境一：假設一位藝術家想保護自己的作品不被AI模型學習並模仿。透過這項技術，他可以在作品中加入肉眼難以察覺的「浮水印」，一旦AI模型生成類似作品，就能證明其侵權。", "情境二：新聞媒體可以使用這項技術，在發布的照片中嵌入隱藏的觸發元素。如果有人未經授權使用這些照片，並透過AI生成工具進行修改，嵌入的觸發元素仍然存在，有助於追蹤侵權行為。", "情境三：電商平台可以利用此技術，在商品圖片中加入隱藏的防偽標記。消費者可以透過特定App掃描圖片，驗證商品是否為正品，防止假冒偽劣產品。"], "pitch": "各位投資人，生成式AI正以前所未有的速度發展，但也帶來了嚴重的著作權和智慧財產權問題。想像一下，如果AI可以輕易複製藝術家的風格、音樂家的旋律，甚至工程師的設計，那將會對創意產業造成毀滅性的打擊。我們的「多元素攻擊」技術，不僅能有效防禦針對生成式AI的著作權侵權攻擊，更能成為保護數位內容的關鍵武器。我們正在建立一個全新的數位內容保護生態系統，透過隱藏的觸發元素，讓每一份作品都擁有可追溯的「數位指紋」。未來，我們可以將這項技術應用於圖片、音樂、影片、3D模型等各種數位內容，甚至能與區塊鏈技術結合，建立一個安全、透明的數位版權交易平台。這不僅是一個技術，更是一個巨大的市場機會，一個能重新定義數位內容價值的未來！我們相信，這項技術將成為AI時代的基礎設施，保護創意，激發創新，帶來巨大的商業價值。", "audio": "docs/data/audios/2506.10776v1.wav"}
{"query": "AI", "id": "2506.10862v1", "url": "http://arxiv.org/abs/2506.10862v1", "title": "OmniFluids: Unified Physics Pre-trained Modeling of Fluid Dynamics", "summary": "High-fidelity and efficient simulation of fluid dynamics drive progress in\nvarious scientific and engineering applications. Traditional computational\nfluid dynamics methods offer strong interpretability and guaranteed\nconvergence, but rely on fine spatial and temporal meshes, incurring\nprohibitive computational costs. Physics-informed neural networks (PINNs) and\nneural operators aim to accelerate PDE solvers using deep learning techniques.\nHowever, PINNs require extensive retraining and careful tuning, and purely\ndata-driven operators demand large labeled datasets. Hybrid physics-aware\nmethods embed numerical discretizations into network architectures or loss\nfunctions, but achieve marginal speed gains and become unstable when balancing\ncoarse priors against high-fidelity measurements. To this end, we introduce\nOmniFluids, a unified physics pre-trained operator learning framework that\nintegrates physics-only pre-training, coarse-grid operator distillation, and\nfew-shot fine-tuning, which enables fast inference and accurate prediction\nunder limited or zero data supervision. For architectural design, the key\ncomponents of OmniFluids include a mixture of operators, a multi-frame decoder,\nand factorized Fourier layers, which enable efficient and scalable modeling of\ndiverse physical tasks while maintaining seamless integration with\nphysics-based supervision. Across a broad range of two- and three-dimensional\nbenchmarks, OmniFluids significantly outperforms state-of-the-art AI-driven\nmethods in flow field reconstruction and turbulence statistics accuracy,\ndelivering 10-100x speedups compared to classical solvers, and accurately\nrecovers unknown physical parameters from sparse, noisy data. This work\nestablishes a new paradigm for efficient and generalizable surrogate modeling\nin complex fluid systems under limited data availability.", "authors": ["Rui Zhang", "Qi Meng", "Han Wan", "Yang Liu", "Zhi-Ming Ma", "Hao Sun"], "published_date": "2025-06-12", "timestamp": "2025-06-14T03:47:32.958191", "title_zh": "OmniFluids：流體力學的統一物理預訓練模型", "summary_zh": "OmniFluids是一個創新的流體力學模擬框架，它結合了物理預訓練、粗網格算子提煉和少量樣本微調，即使在數據有限的情況下，也能實現快速且準確的預測。該框架利用混合算子、多幀解碼器和分解傅立葉層等關鍵組件，高效地模擬各種物理任務，同時與基於物理的監督無縫集成。在廣泛的二維和三維基準測試中，OmniFluids在流場重建和湍流統計精度方面顯著優於現有AI方法，速度提升10-100倍，並能從稀疏噪聲數據中準確恢復未知的物理參數。這為複雜流體系統中高效且通用的代理建模開創了新範例。", "applications": ["天氣預報：更精準地預測颱風路徑和降雨量，提早做好防災準備，減少損失。", "汽車設計：模擬汽車周圍的氣流，優化車身外形，降低風阻，提高燃油效率。", "醫療器材：設計更有效的呼吸器或血液幫浦，改善病患的治療效果。"], "pitch": "各位投資人，想像一下，我們能以100倍的速度預測天氣變化、設計更節能的交通工具、甚至改善醫療器材的效能，而這一切都仰賴於我們革命性的OmniFluids技術。傳統的流體力學模擬耗時耗力，但OmniFluids通過結合物理知識和AI，打破了這一瓶頸。這不僅僅是一個模擬工具，更是一個能驅動各行各業創新的引擎。從航空航天到能源，從醫療到環境保護，OmniFluids的應用前景無可限量。我們預計，隨著AI技術的不斷發展，OmniFluids將成為流體力學模擬領域的行業標準，帶來數十億美元的市場機會。現在加入我們，共同打造一個更高效、更智能的未來！", "audio": "docs/data/audios/2506.10862v1.wav"}
{"query": "Foundation Model", "id": "2506.10217v1", "url": "http://arxiv.org/abs/2506.10217v1", "title": "Data-Centric Safety and Ethical Measures for Data and AI Governance", "summary": "Datasets play a key role in imparting advanced capabilities to artificial\nintelligence (AI) foundation models that can be adapted to various downstream\ntasks. These downstream applications can introduce both beneficial and harmful\ncapabilities -- resulting in dual use AI foundation models, with various\ntechnical and regulatory approaches to monitor and manage these risks. However,\ndespite the crucial role of datasets, responsible dataset design and ensuring\ndata-centric safety and ethical practices have received less attention. In this\nstudy, we pro-pose responsible dataset design framework that encompasses\nvarious stages in the AI and dataset lifecycle to enhance safety measures and\nreduce the risk of AI misuse due to low quality, unsafe and unethical data\ncontent. This framework is domain agnostic, suitable for adoption for various\napplications and can promote responsible practices in dataset creation, use,\nand sharing to facilitate red teaming, minimize risks, and increase trust in AI\nmodels.", "authors": ["Srija Chakraborty"], "published_date": "2025-06-11", "timestamp": "2025-06-14T03:48:55.130988", "title_zh": "以數據為中心的數據與人工智慧治理之安全與倫理措施", "summary_zh": "本研究關注人工智慧基礎模型訓練中至關重要的數據集，指出目前對數據集本身的安全性與倫理規範重視不足。我們提出一個以數據為中心的責任設計框架，涵蓋人工智慧與數據集生命週期的各個階段，旨在提升安全性，降低因低品質、不安全或不道德的數據內容導致人工智慧被濫用的風險。此框架適用於各種領域，促進數據集創建、使用和共享的負責任實踐，以利於紅隊演練，最大限度地降低風險，並增強對人工智慧模型的信任。", "applications": ["**智慧醫療診斷輔助：** 透過高質量、倫理審查過的醫療影像數據集，AI能更精準地診斷疾病，減少誤判，但同時確保患者隱私不被洩露。", "**公平的貸款審核系統：** 運用經過去偏見處理的金融數據，AI能更客觀地評估貸款申請，避免種族、性別歧視，讓更多人獲得公平的金融服務。", "**安全的自動駕駛技術：** 透過嚴格審查、包含各種極端情況的駕駛數據集，AI能訓練出更安全的自動駕駛系統，減少交通事故發生，保障用路人安全。"], "pitch": "各位投資人，我們正面臨AI發展的關鍵轉捩點！AI模型能力日新月異，但數據品質卻成為制約其發展的瓶頸。想像一下，如果AI的學習基礎是錯誤或帶有偏見的數據，那它所做出的決策將會造成多大的社會影響？我們提出的『以數據為中心的安全與倫理框架』，正是解決這個問題的關鍵。這不僅僅是一個技術框架，更是一個預防AI風險的防火牆，能有效提升AI模型的可靠性、公平性與安全性。試想，在自動駕駛、金融風控、醫療診斷等高風險領域，誰不希望使用經過嚴格審查的AI系統？這將是一個數十億美元級別的市場！我們將與各行業領先者合作，建立行業標準，並提供數據安全諮詢服務，引領AI安全發展的新時代。現在投資，您將成為AI安全領域的先驅，共享AI發展的巨大紅利！", "audio": "docs/data/audios/2506.10217v1.wav"}
{"query": "Diffusion Model", "id": "2506.10711v1", "url": "http://arxiv.org/abs/2506.10711v1", "title": "PDESpectralRefiner: Achieving More Accurate Long Rollouts with Spectral Adjustment", "summary": "Generating accurate and stable long rollouts is a notorious challenge for\ntime-dependent PDEs (Partial Differential Equations). Recently, motivated by\nthe importance of high-frequency accuracy, a refiner model called PDERefiner\nutilizes diffusion models to refine outputs for every time step, since the\ndenoising process could increase the correctness of modeling high frequency\npart. For 1-D Kuramoto-Sivashinsky equation, refiner models can degrade the\namplitude of high frequency part better than not doing refinement process.\nHowever, for some other cases, the spectrum might be more complicated. For\nexample, for a harder PDE like Navior-Stokes equation, diffusion models could\nover-degrade the higher frequency part. This motivates us to release the\nconstraint that each frequency weighs the same. We enhance our refiner model\nwith doing adjustments on spectral space, which recovers Blurring diffusion\nmodels. We developed a new v-prediction technique for Blurring diffusion\nmodels, recovering the MSE training objective on the first refinement step. We\nshow that in this case, for different model backbones, such as U-Net and neural\noperators, the outputs of PDE-SpectralRefiner are more accurate for both\none-step MSE loss and rollout loss.", "authors": ["Li Luo", "Shangsong Liang"], "published_date": "2025-06-12", "timestamp": "2025-06-14T03:50:07.892602", "title_zh": "PDESpectralRefiner：透過頻譜調整實現更精準的長時間推演", "summary_zh": "時間相關偏微分方程式（PDEs）的精準長時間推演是個難題。現有的PDERefiner模型利用擴散模型修正每個時間步的輸出，藉由去噪過程提升高頻部分的準確性。然而，對於Navier-Stokes等更複雜的PDEs，擴散模型可能過度降低高頻部分。因此，我們提出PDESpectralRefiner，在頻譜空間進行調整，恢復模糊擴散模型。我們開發了一種新的v-prediction技術，恢復第一步修正的MSE訓練目標。實驗證明，無論使用U-Net或神經算子作為模型骨幹，PDESpectralRefiner在單步MSE損失和推演損失方面都更為精準。", "applications": ["天氣預報：更精準地預測未來幾天的天氣變化，特別是極端天氣事件，幫助政府和民眾提前做好準備。", "流體力學模擬：應用於汽車或飛機設計，更準確地模擬空氣動力學特性，提升性能並降低能耗。", "金融市場預測：模擬金融市場的波動，協助投資者做出更明智的決策，降低投資風險。"], "pitch": "各位投資人，我們正在開發一種革命性的技術，PDESpectralRefiner，它能大幅提升偏微分方程式模擬的精準度，特別是在長時間的推演中。想像一下，如果我們能更準確地預測天氣、設計更節能的交通工具、甚至預測金融市場的崩盤，這將帶來多大的價值？現有的模擬技術往往在高頻資訊處理上有所不足，導致長時間預測的誤差累積。我們的技術突破了這個瓶頸，透過頻譜調整，能更精準地捕捉高頻資訊，實現更可靠的長時間模擬。這項技術的應用範圍極廣，從航空航天、汽車工業、能源勘探到金融服務，都將受益於更精準的模擬結果。我們相信，PDESpectralRefiner將成為未來模擬技術的基石，引領各個行業進入一個更精準、更高效的時代。現在投資，您將成為這場技術革命的先驅，共同分享這巨大的市場潛力！", "audio": "docs/data/audios/2506.10711v1.wav"}
{"query": "AI", "id": "2506.10857v1", "url": "http://arxiv.org/abs/2506.10857v1", "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos", "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.", "authors": ["Jiashuo Yu", "Yue Wu", "Meng Chu", "Zhifei Ren", "Zizheng Huang", "Pei Chu", "Ruijie Zhang", "Yinan He", "Qirui Li", "Songze Li", "Zhenxiang Li", "Zhongying Tu", "Conghui He", "Yu Qiao", "Yali Wang", "Yi Wang", "Limin Wang"], "published_date": "2025-06-12", "timestamp": "2025-06-14T06:34:19.194319", "title_zh": "VRBench：長篇敘事影片中多步驟推理的基準測試", "summary_zh": "VRBench是一個專為評估大型模型在長篇敘事影片中多步驟推理能力而設計的基準測試。它包含1010個平均時長1.6小時的長影片，以及9468個人工標註的多步驟問答對和30292個帶時間戳的推理步驟。這些影片經過多階段篩選，確保情節連貫。VRBench設計了一個多階段評估流程，從結果和過程兩個層面評估模型，並提出一個由大型語言模型引導的進度級別評分指標，全面評估推理鏈的品質。透過對12個大型語言模型和16個視覺語言模型的廣泛評估，我們進行了深入分析，並提供了有價值的見解，以推進多步驟推理領域的發展。簡單來說，VRBench就是一個評估AI看長篇故事理解能力的考試，讓AI不只知道發生什麼，還知道為什麼發生，以及事情的先後順序。", "applications": ["AI劇本分析師：讓AI自動分析電影、電視劇的情節發展，找出劇情漏洞或不合理之處，協助編劇提升劇本品質。", "個人化教育影片：根據學生的學習進度，AI可以從長篇教學影片中提取相關片段，組成客製化的複習教材，提升學習效率。", "智能客服：當使用者回報家電故障時，AI可以透過分析使用者提供的操作影片，一步一步引導使用者排除問題，減少客服人力成本。"], "pitch": "各位投資人，我們正在打造的是AI界的「福爾摩斯」！VRBench不僅僅是一個基準測試，更是一個讓AI具備深度理解和推理能力的加速器。想像一下，未來的AI可以像人類一樣，理解複雜的故事劇情，分析事件的因果關係，甚至預測未來的發展趨勢。這將顛覆內容創作、教育、客服等各個領域。在內容創作方面，AI可以協助編劇產出更具吸引力的故事；在教育方面，AI可以提供更個人化的學習體驗；在客服方面，AI可以更有效地解決使用者問題。更重要的是，VRBench的技術可以應用於自動駕駛、金融分析、醫療診斷等需要高度推理能力的領域。我們相信，隨著VRBench的發展，AI將不再只是執行指令的機器，而是真正具備思考和理解能力的智慧夥伴。現在加入我們，一起開創AI推理的新時代，共享百億美元的市場潛力！", "audio": "docs/data/audios/2506.10857v1.wav"}
{"query": "Foundation Model", "id": "2506.10205v1", "url": "http://arxiv.org/abs/2506.10205v1", "title": "AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent", "summary": "To address the enormous size of Large Language Models (LLMs), model\ncompression methods, such as quantization and pruning, are often deployed,\nespecially on edge devices. In this work, we focus on layer-wise post-training\nquantization and pruning. Drawing connections between activation-aware weight\npruning and sparse approximation problems, and motivated by the success of\nIterative Hard Thresholding (IHT), we propose a unified method for\nActivation-aware Weight pruning and quantization via Projected gradient descent\n(AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM\npruning and quantization methods. Theoretical convergence guarantees of the\nproposed method for pruning are also provided.", "authors": ["Jing Liu", "Toshiaki Koike-Akino", "Ye Wang", "Hassan Mansour", "Matthew Brand"], "published_date": "2025-06-11", "timestamp": "2025-06-14T06:35:52.884428", "title_zh": "AWP：基於激活感知權重剪枝與量化之投影梯度下降法", "summary_zh": "為了解決大型語言模型（LLM）體積過於龐大的問題，我們提出了一種名為AWP（Activation-aware Weight Pruning and Quantization）的新方法，它利用投影梯度下降法，針對LLM進行逐層的激活感知權重剪枝和量化。簡單來說，AWP就像一位精明的裁縫，能精準地裁剪掉模型中不重要的部分，並用更簡潔的方式表達重要的資訊，進而大幅縮小模型體積，同時保持甚至提升模型效能。實驗證明，AWP在LLM的剪枝和量化方面，超越了現有的最佳方法，並提供理論上的收斂保證。", "applications": ["智慧型手機上的AI助手：讓手機在離線狀態下也能快速且準確地理解你的語音指令，例如查詢天氣、設定鬧鐘或播放音樂，無需耗用大量網路流量和電力。", "嵌入式系統中的自然語言處理：應用於智慧家電、自動駕駛汽車等資源有限的設備上，使它們能夠理解並回應人類的自然語言指令，提升使用者體驗和安全性。", "醫療影像診斷：將大型醫療影像模型壓縮後部署於小型醫療設備上，讓醫生能夠在資源匱乏的地區進行即時診斷，提高醫療服務的可及性。"], "pitch": "各位創投先進，我們正站在AI普及化的浪潮之上！大型語言模型（LLM）擁有強大的能力，但其龐大的體積限制了它們在邊緣設備上的應用。AWP技術正是解決這個問題的關鍵！想像一下，我們能將原本需要雲端伺服器才能運行的AI模型，移植到手機、汽車甚至穿戴裝置上，這將徹底改變人機互動的方式。AWP不僅能有效壓縮模型，還能提升模型效能，降低功耗。這意味著更快的反應速度、更長的電池續航力以及更低的運營成本。我們的技術擁有廣泛的應用前景，從智慧家居到自動駕駛，再到醫療健康，都將因為AWP而產生革命性的變化。我們預計，未來三年內，邊緣AI市場將呈現爆發式增長，而AWP將成為這個市場的領頭羊。現在投資AWP，您將搶佔先機，共同打造一個更智慧、更便捷的未來！", "audio": "docs/data/audios/2506.10205v1.wav"}
{"query": "Diffusion Model", "id": "2506.10685v1", "url": "http://arxiv.org/abs/2506.10685v1", "title": "Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework", "summary": "With the rapid advancements in deep learning, traditional CAPTCHA schemes are\nincreasingly vulnerable to automated attacks powered by deep neural networks\n(DNNs). Existing adversarial attack methods often rely on original image\ncharacteristics, resulting in distortions that hinder human interpretation and\nlimit applicability in scenarios lacking initial input images. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel\nframework generating high-fidelity adversarial examples guided by\nattacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC\nenhances CAPTCHA diversity and supports both targeted and untargeted attacks.\nFor targeted attacks, the EDICT method optimizes dual latent variables in a\ndiffusion model for superior image quality. In untargeted attacks, especially\nfor black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-UAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show BP-UAC\nachieves high attack success rates across diverse systems, generating natural\nCAPTCHAs indistinguishable to humans and DNNs.", "authors": ["Xia Du", "Xiaoyuan Liu", "Jizhe Zhou", "Zheng Lin", "Chi-man Pun", "Zhe Chen", "Wei Ni", "Jun Luo"], "published_date": "2025-06-12", "timestamp": "2025-06-14T06:37:07.135720", "title_zh": "無源對抗性驗證碼：一種雙階段對抗性驗證碼框架", "summary_zh": "本研究提出一種名為「無源對抗性驗證碼」（UAC）的新框架，旨在解決傳統驗證碼容易受到深度學習攻擊的問題。UAC利用大型語言模型（LLM）生成高品質的對抗性範例，並支援定向和非定向攻擊。針對定向攻擊，EDICT方法優化擴散模型中的雙潛在變數，以提高圖像品質。對於非定向攻擊，特別是在黑盒場景中，我們引入了雙路徑無源對抗性驗證碼（BP-UAC），這是一種採用多模態梯度和雙路徑優化的兩步優化策略，可有效實現錯誤分類。實驗結果表明，BP-UAC在各種系統中均實現了較高的攻擊成功率，並生成了人類和深度神經網路難以區分的自然驗證碼。", "applications": ["防止網路機器人惡意註冊：網站可以用這種技術生成的驗證碼，有效阻止機器人大量註冊帳號，保護用戶資訊安全。", "抵禦惡意刷票行為：投票活動採用這種驗證碼，可以防止機器人自動刷票，確保投票結果的公正性。", "保護線上遊戲帳號安全：線上遊戲可以使用這種驗證碼，防止外掛程式自動登入和執行遊戲，維護遊戲平衡和玩家權益。"], "pitch": "各位投資人，我們正在開發下一代驗證碼技術——「無源對抗性驗證碼」（UAC）。傳統驗證碼已經不堪一擊，而UAC利用大型語言模型和創新的雙路徑優化策略，能產生讓機器難以破解，但人類容易辨識的驗證碼。想像一下，未來所有的網站、APP、甚至物聯網設備，都需要更強大的安全防護。UAC不僅能有效抵禦現有的機器人攻擊，還能不斷進化，適應未來的新型攻擊。這是一個數十億美元的市場！我們預計在未來五年內，UAC將成為網路安全領域的關鍵技術，並為我們的投資者帶來豐厚的回報。現在加入我們，一起打造更安全的網路世界！", "audio": "docs/data/audios/2506.10685v1.wav"}
{"query": "AI", "id": "2506.10829v1", "url": "http://arxiv.org/abs/2506.10829v1", "title": "LLM-Driven Personalized Answer Generation and Evaluation", "summary": "Online learning has experienced rapid growth due to its flexibility and\naccessibility. Personalization, adapted to the needs of individual learners, is\ncrucial for enhancing the learning experience, particularly in online settings.\nA key aspect of personalization is providing learners with answers customized\nto their specific questions. This paper therefore explores the potential of\nLarge Language Models (LLMs) to generate personalized answers to learners'\nquestions, thereby enhancing engagement and reducing the workload on educators.\nTo evaluate the effectiveness of LLMs in this context, we conducted a\ncomprehensive study using the StackExchange platform in two distinct areas:\nlanguage learning and programming. We developed a framework and a dataset for\nvalidating automatically generated personalized answers. Subsequently, we\ngenerated personalized answers using different strategies, including 0-shot,\n1-shot, and few-shot scenarios. The generated answers were evaluated using\nthree methods: 1. BERTScore, 2. LLM evaluation, and 3. human evaluation. Our\nfindings indicated that providing LLMs with examples of desired answers (from\nthe learner or similar learners) can significantly enhance the LLMs' ability to\ntailor responses to individual learners' needs.", "authors": ["Mohammadreza Molavi", "Mohammadreza Tavakoli", "Mohammad Moein", "Abdolali Faraji", "Gábor Kismihók"], "published_date": "2025-06-12", "timestamp": "2025-06-14T09:27:53.956518", "title_zh": "大型語言模型驅動的個人化答案生成與評估", "summary_zh": "本研究探索大型語言模型（LLMs）在生成個人化答案方面的潛力，旨在提升線上學習體驗並減輕教育者的工作負擔。研究人員利用StackExchange平台，針對語言學習和程式設計兩個領域，開發了一個驗證自動生成個人化答案的框架和數據集。透過零樣本、單樣本和少樣本等不同策略，生成個人化答案，並使用BERTScore、LLM評估和人工評估三種方法進行評估。研究結果表明，向LLMs提供期望答案的範例，能顯著提高其針對個人學習者需求客製化回應的能力。", "applications": ["**線上課程的個人化輔導：**想像一下，你在學程式設計，遇到一個很難的Bug。一般的論壇可能要等很久才有解答，而且答案不一定適合你。有了這項技術，AI可以根據你過去的學習紀錄和提問方式，快速生成專屬於你的解答，就像一位隨時待命的個人助教。", "**語言學習的客製化練習：**學習外語時，每個人遇到的難點都不同。AI可以根據你的學習進度和弱點，提供客製化的練習題和例句，讓你不再只是死背單字，而是真正理解並運用語言。", "**企業內部的知識庫：**員工在工作中遇到問題，常常需要查閱大量的資料。AI可以根據員工的職位、專長和過去的提問紀錄，快速找到最相關的資訊，並用簡潔易懂的方式呈現，大幅提升工作效率。"], "pitch": "各位投資人，我們正在打造一個教育界的革命性產品！想像一下，未來的教育不再是千篇一律的課程，而是根據每個人的需求量身打造的學習體驗。透過LLM驅動的個人化答案生成技術，我們可以讓AI成為每個人的專屬家教，隨時提供最適合的解答和指導。這不僅能大幅提升學習效率和參與度，更能有效解決教育資源分配不均的問題。市場規模方面，全球線上教育市場正在高速成長，而個人化學習更是未來的趨勢。我們的技術不僅能應用於K-12教育、高等教育，還能應用於企業培訓、技能提升等領域，潛力無限。更進一步，我們可以將這項技術應用於客戶服務領域，讓AI根據客戶的個人資料和過去的互動紀錄，提供更精準、更貼心的服務，大幅提升客戶滿意度和忠誠度。我們相信，這項技術將徹底改變教育和服務產業，成為下一個獨角獸企業！", "audio": "docs/data/audios/2506.10829v1.wav"}
{"query": "Foundation Model", "id": "2506.10157v1", "url": "http://arxiv.org/abs/2506.10157v1", "title": "One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence", "summary": "Medical foundation models, including language models trained on clinical\nnotes, vision-language models on medical images, and multimodal models on\nelectronic health records, can summarize clinical notes, answer medical\nquestions, and assist in decision-making. Adapting these models to new\npopulations, specialties, or settings typically requires fine-tuning, careful\nprompting, or retrieval from knowledge bases. This can be impractical, and\nlimits their ability to interpret unfamiliar inputs and adjust to clinical\nsituations not represented during training. As a result, models are prone to\ncontextual errors, where predictions appear reasonable but fail to account for\ncritical patient-specific or contextual information. These errors stem from a\nfundamental limitation that current models struggle with: dynamically adjusting\ntheir behavior across evolving contexts of medical care. In this Perspective,\nwe outline a vision for context-switching in medical AI: models that\ndynamically adapt their reasoning without retraining to new specialties,\npopulations, workflows, and clinical roles. We envision context-switching AI to\ndiagnose, manage, and treat a wide range of diseases across specialties and\nregions, and expand access to medical care.", "authors": ["Michelle M. Li", "Ben Y. Reis", "Adam Rodman", "Tianxi Cai", "Noa Dagan", "Ran D. Balicer", "Joseph Loscalzo", "Isaac S. Kohane", "Marinka Zitnik"], "published_date": "2025-06-11", "timestamp": "2025-06-14T09:29:05.219351", "title_zh": "一人多境：透過情境智能擴展醫療人工智慧", "summary_zh": "現今醫療AI模型，如臨床筆記語言模型、醫學影像視覺語言模型和電子病歷多模態模型，能總結病歷、回答醫學問題並輔助決策。然而，這些模型難以適應新的族群、專業或環境，常需微調或提示。本研究提出一個願景：開發能動態適應醫療情境的AI模型，無需重新訓練即可跨專業、族群、工作流程和臨床角色進行推理。這種情境切換AI有望診斷、管理和治療各種疾病，並擴大醫療服務的可及性，解決目前模型在處理特定患者或情境資訊時容易出錯的問題。", "applications": ["偏鄉醫療：在醫療資源匱乏的偏遠地區，AI能根據當地病患的常見疾病和醫療條件，快速調整診斷和治療方案，協助經驗不足的醫生做出更精確的判斷。", "個人化用藥建議：AI能根據病患的基因、生活習慣和過去的用藥反應，提供更精準的藥物劑量和種類建議，降低副作用風險，提高治療效果。", "遠距醫療：透過視訊問診，AI能分析病患的語氣、表情和肢體語言，結合病歷資料，判斷病患的心理狀態和潛在健康問題，提供更全面的照護。"], "pitch": "想像一下，一個AI醫生能夠像變色龍一樣，瞬間適應不同的醫療情境，從急診室到安寧病房，從兒科到老年醫學，都能提供精準的診斷和治療建議。這就是我們正在開發的情境切換醫療AI！它不僅能解決目前醫療AI在特定情境下容易出錯的問題，更能大幅降低醫療成本、提升醫療效率，並擴大醫療服務的可及性。未來，我們將把這項技術應用於遠距醫療、個人化健康管理等領域，打造一個更智慧、更普惠的醫療生態系統。現在投資，您將成為這場醫療革命的先驅，共同開創一個AI賦能的健康未來！", "audio": "docs/data/audios/2506.10157v1.wav"}
{"query": "Diffusion Model", "id": "2506.10639v1", "url": "http://arxiv.org/abs/2506.10639v1", "title": "GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning", "summary": "Recent progress in diffusion models has greatly enhanced video generation\nquality, yet these models still require fine-tuning to improve specific\ndimensions like instance preservation, motion rationality, composition, and\nphysical plausibility. Existing fine-tuning approaches often rely on human\nannotations and large-scale computational resources, limiting their\npracticality. In this work, we propose GigaVideo-1, an efficient fine-tuning\nframework that advances video generation without additional human supervision.\nRather than injecting large volumes of high-quality data from external sources,\nGigaVideo-1 unlocks the latent potential of pre-trained video diffusion models\nthrough automatic feedback. Specifically, we focus on two key aspects of the\nfine-tuning process: data and optimization. To improve fine-tuning data, we\ndesign a prompt-driven data engine that constructs diverse, weakness-oriented\ntraining samples. On the optimization side, we introduce a reward-guided\ntraining strategy, which adaptively weights samples using feedback from\npre-trained vision-language models with a realism constraint. We evaluate\nGigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17\nevaluation dimensions. Experiments show that GigaVideo-1 consistently improves\nperformance on almost all the dimensions with an average gain of about 4% using\nonly 4 GPU-hours. Requiring no manual annotations and minimal real data,\nGigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and\ndata will be publicly available.", "authors": ["Xiaoyi Bao", "Jindi Lv", "Xiaofeng Wang", "Zheng Zhu", "Xinze Chen", "YuKun Zhou", "Jiancheng Lv", "Xingang Wang", "Guan Huang"], "published_date": "2025-06-12", "timestamp": "2025-06-14T09:30:56.045277", "title_zh": "GigaVideo-1：透過自動回饋與4 GPU小時微調推進影片生成", "summary_zh": "GigaVideo-1提出一個高效的影片生成微調框架，無需額外的人工監督。它不依賴大量外部高品質數據，而是透過自動回饋釋放預訓練影片擴散模型的潛力。GigaVideo-1 側重於微調過程的兩個關鍵面向：數據和最佳化。透過提示驅動的數據引擎構建多樣化、針對弱點的訓練樣本，並引入獎勵引導的訓練策略，利用預訓練視覺語言模型的回饋自適應地權衡樣本，並加上真實性約束。實驗證明，GigaVideo-1 僅使用 4 GPU 小時，就能在 VBench-2.0 基準測試的幾乎所有維度上持續提升性能，平均增益約為 4%。GigaVideo-1 無需手動註釋和最少的真實數據，展現了有效性和效率。", "applications": ["想像一下，你可以用手機App，輸入幾個關鍵詞，比如「貓咪」、「太空漫步」，App就能自動生成一段貓咪在太空漫步的影片！這項技術讓影片創作變得超簡單。", "遊戲開發者可以用這個技術快速生成遊戲場景的過場動畫，再也不用花大錢請動畫師了！而且，動畫的品質還能不斷提升。", "行銷人員可以用更低的成本製作吸睛的廣告影片。只要輸入產品特色和想要的風格，就能快速生成符合需求的影片，搶佔市場先機。"], "pitch": "各位投資人，我們相信影片生成是下一個AI的殺手級應用！GigaVideo-1技術，以極低的成本（僅需4 GPU小時），就能顯著提升現有影片生成模型的品質，無需依賴大量人工標註數據。這意味著更快的開發週期、更低的運營成本，以及更廣泛的應用場景。想像一下，未來每個人都能輕鬆製作出專業級的影片，內容創作的門檻將大幅降低，市場規模將呈指數級增長。我們的技術不僅能應用於娛樂、遊戲、行銷等領域，還能拓展到教育、醫療、工業設計等更多行業。我們預計，GigaVideo-1將成為影片生成領域的基礎設施，引領下一代內容創作革命。現在加入我們，共同打造這個千億美元級別的市場！", "audio": "docs/data/audios/2506.10639v1.wav"}
{"query": "AI", "id": "2506.10825v1", "url": "http://arxiv.org/abs/2506.10825v1", "title": "Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches", "summary": "Following the successful paradigm shift of large language models, leveraging\npre-training on a massive corpus of data and fine-tuning on different\ndownstream tasks, generalist models have made their foray into computer vision.\nThe introduction of Segment Anything Model (SAM) set a milestone on\nsegmentation of natural images, inspiring the design of a multitude of\narchitectures for medical image segmentation. In this survey we offer a\ncomprehensive and in-depth investigation on generalist models for medical image\nsegmentation. We start with an introduction on the fundamentals concepts\nunderpinning their development. Then, we provide a taxonomy on the different\ndeclinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on\nthe recent SAM 2, on other innovative models trained on images alone, and\nothers trained on both text and images. We thoroughly analyze their\nperformances at the level of both primary research and best-in-literature,\nfollowed by a rigorous comparison with the state-of-the-art task-specific\nmodels. We emphasize the need to address challenges in terms of compliance with\nregulatory frameworks, privacy and security laws, budget, and trustworthy\nartificial intelligence (AI). Finally, we share our perspective on future\ndirections concerning synthetic data, early fusion, lessons learnt from\ngeneralist models in natural language processing, agentic AI and physical AI,\nand clinical translation.", "authors": ["Andrea Moglia", "Matteo Leccardi", "Matteo Cavicchioli", "Alice Maccarini", "Marco Marcon", "Luca Mainardi", "Pietro Cerveri"], "published_date": "2025-06-12", "timestamp": "2025-06-14T12:47:38.975729", "title_zh": "醫學影像分割中的通用模型：與任務特定方法的調查與效能比較", "summary_zh": "大型語言模型成功後，通用模型也開始進入電腦視覺領域。Segment Anything Model (SAM) 在自然圖像分割上樹立了里程碑，啟發了眾多醫學影像分割架構的設計。本研究深入調查醫學影像分割的通用模型，介紹其基本概念，並對SAM的各種變體（零樣本、少樣本、微調、適配器、SAM 2）以及其他創新模型進行分類。我們分析了它們在研究和文獻中的效能，並與最先進的任務特定模型進行比較。同時，我們強調了法規遵循、隱私安全、預算和可信賴AI等挑戰。最後，我們展望了合成數據、早期融合、自然語言處理通用模型的經驗、代理AI和物理AI以及臨床轉化等未來方向。", "applications": ["想像一下，未來醫生可以使用AI眼鏡，在手術過程中即時看到腫瘤的精確邊界，就像擁有了透視眼一樣，大大提高手術的精準度和成功率。", "透過手機App，一般民眾可以上傳皮膚照片，AI立即分析是否有潛在的皮膚癌風險，並提供專業的醫療建議，實現早期發現、早期治療。", "在偏遠地區，醫療資源匱乏，AI可以輔助放射科醫師判讀X光片或CT掃描，即使是經驗不足的醫生也能做出更準確的診斷，提升整體醫療水平。"], "pitch": "各位創投先進，我們正處於AI醫療革命的風口浪尖！我們的技術基於最新的通用模型，能徹底改變醫學影像分割的方式。想像一下，一個能處理任何醫學影像、無需針對特定疾病或器官進行訓練的AI系統，這意味著無限的應用可能！從癌症檢測、心血管疾病診斷到神經退化性疾病的早期預測，我們的技術都能提供更準確、更快速、更經濟的解決方案。更重要的是，我們正在開發的AI代理，能像一位資深的放射科醫師一樣，主動分析影像、提出建議，甚至協助制定治療計畫。這不僅能大幅提升醫療效率，更能釋放醫生們的時間，讓他們專注於更複雜的病例和患者關懷。我們相信，這項技術將成為未來醫療AI的核心引擎，並創造數十億美元的市場價值。現在加入我們，共同打造一個更健康、更智慧的醫療未來！", "audio": "docs/data/audios/2506.10825v1.wav"}
{"query": "Foundation Model", "id": "2506.10055v1", "url": "http://arxiv.org/abs/2506.10055v1", "title": "TaskCraft: Automated Generation of Agentic Tasks", "summary": "Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce \\textsc{TaskCraft}, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.", "authors": ["Dingfeng Shi", "Jingyi Cao", "Qianben Chen", "Weichen Sun", "Weizhen Li", "Hongxuan Lu", "Fangchen Dong", "Tianrui Qin", "King Zhu", "Minghao Yang", "Jian Yang", "Ge Zhang", "Jiaheng Liu", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "published_date": "2025-06-11", "timestamp": "2025-06-14T12:49:28.191392", "title_zh": "TaskCraft：自動化生成具自主性的任務", "summary_zh": "TaskCraft是一個自動化流程，用於生成難度可調整、涉及多種工具且可驗證的自主性任務。它透過深度和廣度擴展來擴展基本任務，創造出結構複雜且具層次性的挑戰。實驗結果表明，這些任務能改善生成流程中的提示優化，並增強自主性基礎模型的監督式微調。我們提供了一個包含約36,000個不同難度任務的大型合成數據集，以支持未來對代理調整和評估的研究。簡單來說，TaskCraft就像一個AI任務工廠，能自動產生各種複雜的AI挑戰，加速AI自主解決問題能力的研究。", "applications": ["**智慧家庭管家：** 想像一下，你的智慧家庭管家不僅能聽懂指令，還能根據你的需求，自己規劃並執行一系列任務，例如：『準備一個浪漫的晚餐』，它會自動搜尋食譜、預約餐廳、調整燈光、播放音樂，甚至提醒你重要的紀念日。", "**個人化學習助理：** 學習不再是單向的知識灌輸，而是AI根據你的學習進度和弱點，自動生成客製化的練習題和學習計畫。它會像一位耐心的私人教練，引導你一步步掌握知識，讓你學習更有效率。", "**企業流程自動化：** 繁瑣的行政流程，例如報帳、請假、文件歸檔，都可以交給AI自動處理。它能根據公司規定，自動審核、追蹤進度，甚至在出現問題時主動尋求解決方案，大幅提升企業效率。"], "pitch": "各位投資人，我們正處於AI自主時代的開端！TaskCraft就像是AI界的『任務發電機』，它能低成本、高效率地產生大量複雜的AI任務，解決目前AI訓練數據不足的瓶頸。想像一下，未來AI不僅能回答問題，更能主動解決問題，從自動駕駛、醫療診斷到金融分析，各行各業都將迎來顛覆性的變革。TaskCraft不僅能加速AI的發展，更能催生出無數的商業機會。我們相信，TaskCraft將成為AI自主時代的基礎建設，而現在正是加入我們，共同打造AI未來的最佳時機！我們預期TaskCraft的技術能廣泛應用於各個領域，例如：客製化教育、智慧城市管理、以及更高效的AI藥物研發，創造出數十億美元的市場價值。現在投資TaskCraft，您投資的不僅僅是一項技術，更是AI自主時代的未來！", "audio": "docs/data/audios/2506.10055v1.wav"}
{"query": "Diffusion Model", "id": "2506.10633v1", "url": "http://arxiv.org/abs/2506.10633v1", "title": "Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models", "summary": "Latent Diffusion Models have shown remarkable results in text-guided image\nsynthesis in recent years. In the domain of natural (RGB) images, recent works\nhave shown that such models can be adapted to various vision-language\ndownstream tasks with little to no supervision involved. On the contrary,\ntext-to-image Latent Diffusion Models remain relatively underexplored in the\nfield of medical imaging, primarily due to limited data availability (e.g., due\nto privacy concerns). In this work, focusing on the chest X-ray modality, we\nfirst demonstrate that a standard text-conditioned Latent Diffusion Model has\nnot learned to align clinically relevant information in free-text radiology\nreports with the corresponding areas of the given scan. Then, to alleviate this\nissue, we propose a fine-tuning framework to improve multi-modal alignment in a\npre-trained model such that it can be efficiently repurposed for downstream\ntasks such as phrase grounding. Our method sets a new state-of-the-art on a\nstandard benchmark dataset (MS-CXR), while also exhibiting robust performance\non out-of-distribution data (VinDr-CXR). Our code will be made publicly\navailable.", "authors": ["Konstantinos Vilouras", "Ilias Stogiannidis", "Junyu Yan", "Alison Q. O'Neil", "Sotirios A. Tsaftaris"], "published_date": "2025-06-12", "timestamp": "2025-06-14T12:51:05.742897", "title_zh": "基於解剖結構的弱監督提示調整，用於胸腔X光潛在擴散模型", "summary_zh": "近年來，潛在擴散模型在文本引導的圖像合成方面表現出色。在自然圖像領域，這些模型已被應用於各種視覺語言下游任務，且幾乎不需要監督。然而，由於數據隱私等因素，文本到圖像的潛在擴散模型在醫學影像領域的應用仍有待探索。本研究針對胸腔X光片，首先證明標準的文本條件潛在擴散模型未能將放射科報告中的臨床相關信息與掃描圖像的相應區域對齊。為了解決這個問題，我們提出了一種微調框架，以改善預訓練模型中的多模態對齊，使其能有效地用於短語定位等下游任務。我們的模型在MS-CXR基準數據集上取得了新的最佳結果，並在VinDr-CXR數據集上表現出穩健的性能。", "applications": ["**智能輔助診斷：** 醫生在看X光片時，系統能自動標記出可疑區域，並提供相關的醫學報告片段，就像一個永遠不會疲勞的資深助手，幫助醫生更快更準確地做出診斷。", "**個性化健康報告：** 根據你的X光片，系統可以生成一份更易懂的健康報告，用更直觀的方式告訴你身體的狀況，例如：「您的肺部有一個小陰影，可能需要進一步檢查」。", "**遠程醫療與健康監測：** 在偏遠地區，專家資源有限，透過這項技術，即使是基層醫生也能獲得專家的診斷建議，提升醫療服務的覆蓋範圍和質量。"], "pitch": "各位投資人，想像一下，AI不只能畫貓，還能救命！我們的技術，正是利用AI來解讀胸腔X光片，這是一個每年數十億美元的市場。現有的AI診斷方案往往缺乏精準度，我們的模型能將醫學報告與X光圖像精準對應，大幅提升診斷準確率，減少誤診率。更重要的是，我們的方法只需要少量標註數據，降低了開發成本。未來，我們可以將這項技術擴展到其他醫學影像領域，例如CT、MRI等，甚至可以結合基因數據，實現更精準的個性化醫療。這不僅是一項技術突破，更是一項具有巨大社會價值的投資，我們有信心在三年內成為醫學影像AI領域的領導者，為投資者帶來豐厚的回報！", "audio": "docs/data/audios/2506.10633v1.wav"}
{"query": "AI", "id": "2506.10785v1", "url": "http://arxiv.org/abs/2506.10785v1", "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps", "summary": "Artificial Intelligence (AI)-powered features have rapidly proliferated\nacross mobile apps in various domains, including productivity, education,\nentertainment, and creativity. However, how users perceive, evaluate, and\ncritique these AI features remains largely unexplored, primarily due to the\noverwhelming volume of user feedback. In this work, we present the first\ncomprehensive, large-scale study of user feedback on AI-powered mobile apps,\nleveraging a curated dataset of 292 AI-driven apps across 14 categories with\n894K AI-specific reviews from Google Play. We develop and validate a\nmulti-stage analysis pipeline that begins with a human-labeled benchmark and\nsystematically evaluates large language models (LLMs) and prompting strategies.\nEach stage, including review classification, aspect-sentiment extraction, and\nclustering, is validated for accuracy and consistency. Our pipeline enables\nscalable, high-precision analysis of user feedback, extracting over one million\naspect-sentiment pairs clustered into 18 positive and 15 negative user topics.\nOur analysis reveals that users consistently focus on a narrow set of themes:\npositive comments emphasize productivity, reliability, and personalized\nassistance, while negative feedback highlights technical failures (e.g.,\nscanning and recognition), pricing concerns, and limitations in language\nsupport. Our pipeline surfaces both satisfaction with one feature and\nfrustration with another within the same review. These fine-grained,\nco-occurring sentiments are often missed by traditional approaches that treat\npositive and negative feedback in isolation or rely on coarse-grained analysis.\nTo this end, our approach provides a more faithful reflection of the real-world\nuser experiences with AI-powered apps. Category-aware analysis further uncovers\nboth universal drivers of satisfaction and domain-specific frustrations.", "authors": ["Vinaik Chhetri", "Krishna Upadhyay", "A. B. Siddique", "Umar Farooq"], "published_date": "2025-06-12", "timestamp": "2025-06-14T15:24:56.088468", "title_zh": "使用者價值與評論：AI驅動行動應用程式使用者回饋的大規模分析", "summary_zh": "本研究針對Google Play上292款AI行動App，分析近90萬則使用者評論，旨在了解使用者對AI功能的評價。我們開發了一套多階段分析流程，運用大型語言模型，精準萃取使用者回饋中的觀點與情感。研究發現，使用者正面評價多集中在AI的生產力、可靠性和個人化協助，負面評價則主要圍繞技術故障（如掃描和辨識問題）、價格疑慮和語言支援不足。此分析能同時捕捉同一評論中對不同功能的滿意與不滿，提供更細緻的使用者體驗洞察，揭示跨類別的普遍需求與特定領域的痛點。", "applications": ["**語音助理智慧提醒：** AI能分析你的行程、郵件和習慣，在出門前提醒你帶雨傘或提早出門避開塞車，就像有個貼心的AI秘書。", "**AI健身教練：** App能透過鏡頭追蹤你的動作，即時給予姿勢建議，就像一位24小時隨時待命的私人教練，讓你運動更有效率且避免受傷。", "**AI旅遊規劃師：** 根據你的喜好和預算，AI能自動生成客製化的旅遊行程，包含景點推薦、交通方式和住宿選擇，省去你花大量時間做功課的麻煩。"], "pitch": "各位投資人，想像一下，我們正處於AI應用爆發的前夜！這項技術不僅僅是學術研究，而是能精準挖掘使用者需求的金礦。我們的研究成果，能讓App開發者不再盲人摸象，而是能根據真實的使用者回饋，快速迭代產品，提升使用者體驗。試想，如果每個App都能更懂使用者，使用者黏著度將大幅提升，App商店的評分也會水漲船高，進而帶動下載量與營收。更進一步，我們可以將這套分析工具，授權給App開發商，甚至直接提供客製化的使用者體驗優化方案。未來，無論是醫療、金融、教育，各行各業的App都能因此受益，而我們，將成為這波AI浪潮中最關鍵的推手！現在加入我們，共同打造更智慧、更人性化的App生態系！", "audio": "docs/data/audios/2506.10785v1.wav"}
{"query": "Foundation Model", "id": "2506.09638v1", "url": "http://arxiv.org/abs/2506.09638v1", "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.", "authors": ["Weiying Zheng", "Ziyue Lin", "Pengxin Guo", "Yuyin Zhou", "Feifei Wang", "Liangqiong Qu"], "published_date": "2025-06-11", "timestamp": "2025-06-14T15:26:55.036002", "title_zh": "FedVLMBench：視覺語言模型之聯邦式微調基準測試", "summary_zh": "本研究提出FedVLMBench，首個針對視覺語言模型（VLM）聯邦式微調的系統性基準。VLM整合視覺和文本信息，在跨模態理解和生成方面表現出色。為解決隱私問題，研究人員將聯邦學習（FL）引入VLM微調。FedVLMBench整合了兩種主流VLM架構、四種微調策略、五種FL算法和六個多模態數據集，涵蓋多種跨領域單任務和多任務場景。實驗結果揭示了VLM架構、微調策略、數據異質性和多任務聯邦優化之間的相互作用。研究發現，對於基於編碼器的VLM，具有並行連接器和LLM調整的雙層多層感知器（MLP）連接器是FL的最佳配置。此外，當前的FL方法在以視覺為中心的任務中，比以文本為中心的任務對數據異質性更敏感。FedVLMBench為研究社群提供了必要的工具、數據集和經驗指導，為推進多模態基礎模型的隱私保護聯邦式訓練提供了一個標準化平台。", "applications": ["**醫療影像分析：** 想像一下，各醫院的X光片、CT掃描等影像資料，因為病人隱私無法直接共享，但透過FedVLMBench技術，可以在不洩漏病人資料的前提下，共同訓練出更精準的疾病診斷模型，讓醫生能更快更準確地判讀影像，提升醫療品質。", "**自動駕駛訓練：** 不同地區的自駕車收集到的道路影像、交通標誌等資料，因為數據所有權和隱私問題難以整合。利用FedVLMBench，可以在各車廠或地區間進行聯邦式學習，讓自駕車模型學習到更全面的駕駛情境，提升安全性與可靠性。", "**產品圖像搜尋與推薦：** 各電商平台擁有大量商品圖片和用戶行為數據，但難以跨平台整合。透過FedVLMBench，可以在保護用戶隱私和商家數據的前提下，建立更強大的圖像搜尋和商品推薦引擎，提升用戶購物體驗和平台銷售額。"], "pitch": "各位創投先進，我們團隊帶來的是FedVLMBench，一個劃時代的視覺語言模型聯邦式微調基準測試平台。在數據隱私日益重要的今天，傳統的集中式模型訓練方式已難以滿足需求。FedVLMBench 讓醫療、金融、自駕車等敏感數據產業，可以在不洩漏原始數據的前提下，共同訓練出更強大的AI模型，解鎖海量數據的潛力。這不僅符合法規要求，更能大幅降低數據獲取成本，加速AI應用落地。想像一下，未來各家醫院可以共同訓練出全球領先的癌症診斷模型，各家車廠可以合作打造最安全的自動駕駛系統，而這一切都建立在嚴格的數據隱私保護之上。FedVLMBench 不僅是一個技術平台，更是一個數據價值的賦能者，一個隱私保護的守護者。我們相信，FedVLMBench 將引領下一代AI發展方向，成為聯邦學習領域的黃金標準，為早期投資者帶來豐厚的回報。現在加入我們，一起開創AI的隱私新紀元！", "audio": "docs/data/audios/2506.09638v1.wav"}
{"query": "Diffusion Model", "id": "2506.10632v1", "url": "http://arxiv.org/abs/2506.10632v1", "title": "Hessian Geometry of Latent Space in Generative Models", "summary": "This paper presents a novel method for analyzing the latent space geometry of\ngenerative models, including statistical physics models and diffusion models,\nby reconstructing the Fisher information metric. The method approximates the\nposterior distribution of latent variables given generated samples and uses\nthis to learn the log-partition function, which defines the Fisher metric for\nexponential families. Theoretical convergence guarantees are provided, and the\nmethod is validated on the Ising and TASEP models, outperforming existing\nbaselines in reconstructing thermodynamic quantities. Applied to diffusion\nmodels, the method reveals a fractal structure of phase transitions in the\nlatent space, characterized by abrupt changes in the Fisher metric. We\ndemonstrate that while geodesic interpolations are approximately linear within\nindividual phases, this linearity breaks down at phase boundaries, where the\ndiffusion model exhibits a divergent Lipschitz constant with respect to the\nlatent space. These findings provide new insights into the complex structure of\ndiffusion model latent spaces and their connection to phenomena like phase\ntransitions. Our source code is available at\nhttps://github.com/alobashev/hessian-geometry-of-diffusion-models.", "authors": ["Alexander Lobashev", "Dmitry Guskov", "Maria Larchenko", "Mikhail Tamm"], "published_date": "2025-06-12", "timestamp": "2025-06-14T15:28:07.626124", "title_zh": "生成模型潛在空間的黑森幾何", "summary_zh": "本研究提出一種分析生成模型（包含統計物理模型與擴散模型）潛在空間幾何的新方法。透過重建費雪資訊度量，近似給定生成樣本的潛在變數後驗分佈，進而學習對數分割函數。此方法已在Ising和TASEP模型上驗證，優於現有基準，並揭示擴散模型潛在空間中相變的分形結構。研究顯示，測地線插值在各個相內近似線性，但在相邊界處，線性關係會崩潰，擴散模型對潛在空間展現出發散的Lipschitz常數。這些發現為擴散模型潛在空間的複雜結構及其與相變等現象的聯繫提供了新的見解。", "applications": ["AI藝術創作：讓AI更精準地控制圖像生成，例如，指定畫作風格從印象派平滑過渡到超現實主義，避免突兀變化，提升創作品質。", "藥物設計：協助AI預測藥物分子結構的微小變化如何影響藥效，加速新藥開發，並能更精確地調控藥物活性。", "材料科學：用於設計具有特定物理性質的新材料，例如，讓AI控制材料在不同溫度下的相變過程，製造出更堅固耐用或更具彈性的材料。"], "pitch": "各位投資人，我們帶來的是生成式AI領域的革命性突破！想像一下，我們不僅能生成圖像、音樂，還能精準控制生成過程中的每一個細節。透過分析生成模型潛在空間的黑森幾何，我們掌握了AI創造力的底層密碼。這項技術能讓AI藝術創作更具表現力，藥物設計更高效，新材料開發更智能。未來，我們將把這項技術應用於自動駕駛、金融建模等更廣闊的領域，讓AI真正成為各行各業的超級助手。這不僅是一項技術，更是一把開啟無限可能的鑰匙，現在加入我們，共同塑造AI的未來！", "audio": "docs/data/audios/2506.10632v1.wav"}
{"query": "AI", "id": "2506.10751v1", "url": "http://arxiv.org/abs/2506.10751v1", "title": "Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering", "summary": "Automated question answering (QA) over electronic health records (EHRs) can\nbridge critical information gaps for clinicians and patients, yet it demands\nboth precise evidence retrieval and faithful answer generation under limited\nsupervision. In this work, we present Neural, the runner-up in the BioNLP 2025\nArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method\ndecouples the task into (1) sentence-level evidence identification and (2)\nanswer synthesis with explicit citations. For each stage, we automatically\nexplore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning\ninstructions and few-shot demonstrations on the development set. A\nself-consistency voting scheme further improves evidence recall without\nsacrificing precision. On the hidden test set, our method attains an overall\nscore of 51.5, placing second stage while outperforming standard zero-shot and\nfew-shot prompting by over 20 and 10 points, respectively. These results\nindicate that data-driven prompt optimization is a cost-effective alternative\nto model fine-tuning for high-stakes clinical QA, advancing the reliability of\nAI assistants in healthcare.", "authors": ["Sai Prasanna Teja Reddy Bogireddy", "Abrar Majeedi", "Viswanatha Reddy Gajjala", "Zhuoyan Xu", "Siddhant Rai", "Vaishnav Potlapalli"], "published_date": "2025-06-12", "timestamp": "2025-06-14T18:32:56.425796", "title_zh": "Neural於ArchEHR-QA 2025：用於證據基礎臨床問答的代理式提示優化", "summary_zh": "本研究提出名為Neural的系統，在BioNLP 2025 ArchEHR-QA競賽中獲得亞軍，專注於電子病歷的自動問答。Neural將任務拆解為句子級證據識別和帶有明確引用的答案生成，並利用DSPy的MIPROv2優化器自動探索提示空間，調整指令和少量範例。此外，採用自我一致性投票機制，提升證據檢索的召回率，而不犧牲精確度。實驗結果顯示，Neural在隱藏測試集上表現出色，證明數據驅動的提示優化是高風險臨床問答中，一種具成本效益的替代方案，能提升醫療保健中AI助手的可靠性。", "applications": ["**個人化健康建議：**想像一下，你只要問AI『我的血糖有點高，最近應該吃什麼比較好？』，AI就能根據你的病歷資料，給你客製化的飲食建議，就像有個隨時待命的專業營養師。", "**用藥提醒與警示：**長輩常常忘記吃藥，或是搞不清楚藥物的交互作用。有了這個技術，AI可以根據病歷資料，提醒用藥時間和劑量，甚至在發現潛在的藥物不良反應時，立即發出警示。", "**輔助醫師診斷：**當醫師面對複雜的病例時，AI可以快速分析病歷資料，提供相關的醫學文獻和診斷建議，幫助醫師做出更明智的決策，減少誤診的風險。"], "pitch": "各位投資人，我們正處於醫療AI革命的風口浪尖！Neural不僅僅是一個問答系統，它代表著醫療知識獲取的未來。試想一下，一個能夠精準理解病歷、快速提供證據支持的AI助手，將如何徹底改變醫療服務的效率和品質？Neural的數據驅動提示優化技術，成本效益遠超傳統模型微調，意味著更快的部署和更廣泛的應用。我們預見，Neural將成為醫療機構的標配，從診所到醫院，甚至遠程醫療平台，都能看到它的身影。更進一步，我們將整合基因數據、穿戴裝置數據，打造真正個人化的健康管理平台，讓每個人都能掌握自己的健康密碼。這是一個千億美元級別的市場，而Neural有潛力成為這個市場的領導者。現在加入我們，一起塑造醫療AI的未來！", "audio": "docs/data/audios/2506.10751v1.wav"}
{"query": "Foundation Model", "id": "2506.09623v1", "url": "http://arxiv.org/abs/2506.09623v1", "title": "Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models", "summary": "Embodied foundation models are crucial for Artificial Intelligence (AI)\ninteracting with the physical world by integrating multi-modal inputs, such as\nproprioception, vision and language, to understand human intentions and\ngenerate actions to control robots. While these models demonstrate strong\ngeneralization and few-shot learning capabilities, they face significant\nchallenges in continually acquiring new skills without forgetting previously\nlearned skills, a problem known as catastrophic forgetting. To address this\nissue, we propose the Analytic Task Scheduler (ATS), a novel framework for\ncontinual learning in embodied foundation models. ATS consists of a\ntask-specific model library, where each model is fine-tuned independently on a\nsingle task, and an analytic scheduler trained using recursive least squares\n(RLS) to learn the mapping between language instructions and task-specific\nmodels. This architecture enables accurate task recognition and dynamic model\nselection while fundamentally avoiding parameter interference across tasks. The\nscheduler updates its parameters incrementally using only statistics\n(autocorrelation and cross-correlation matrices), enabling forgetting-resistant\nlearning without the need to revisit historical data. We validate ATS on a\nreal-world robot platform (RM65B), demonstrating superior resistance to\nforgetting and strong adaptability to task variations. The results highlight\nATS as an effective, scalable, and deployable solution for continual learning\nin embodied foundation models operating in complex, dynamic environments. Our\ncode will be available at\nhttps://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler", "authors": ["Lipei Xie", "Yingxin Li", "Huiping Zhuang"], "published_date": "2025-06-11", "timestamp": "2025-06-14T18:34:23.561398", "title_zh": "解析任務排程器：基於遞迴最小平方法的具體化基礎模型持續學習方法", "summary_zh": "本研究提出解析任務排程器(ATS)，一個專為具體化基礎模型設計的持續學習框架。ATS包含一個任務特定模型庫，每個模型針對單一任務進行微調。核心是一個解析排程器，利用遞迴最小平方法(RLS)學習語言指令與任務模型的映射關係。這種架構能精準識別任務、動態選擇模型，並避免任務間的參數干擾。排程器僅使用統計數據增量更新參數，無需回溯歷史數據，有效抵抗災難性遺忘。在真實機器人平台(RM65B)上的驗證顯示，ATS具有卓越的抗遺忘能力和對任務變化的適應性，是複雜動態環境下具體化基礎模型持續學習的可擴展、可部署的有效方案。", "applications": ["智慧家電控制：想像一下，你只要對掃地機器人說「幫我把客廳掃乾淨，特別是沙發底下」，它就能立刻理解並執行，即使你之後又教它「擦亮窗戶」，它也不會忘記怎麼掃地。", "客製化教學機器人：針對不同學習階段的孩子，機器人能根據他們的學習進度調整教學內容，而且不會因為教了新的單元就忘記舊的知識點，真正做到因材施教。", "工業機器人協作：在工廠裡，機器人可以同時執行多項任務，例如組裝零件、品質檢測等，並且能快速適應新的任務流程，不會因為學習新的技能而降低原有的工作效率。"], "pitch": "各位投資人，我們正處於AI機器人革命的風口浪尖！想像一下，一個能像人類一樣不斷學習、適應環境的機器人，它的潛力是無限的。我們的解析任務排程器(ATS)正是實現這一願景的關鍵技術。它解決了機器人學習過程中的「災難性遺忘」問題，讓機器人能夠像人類一樣，在不斷學習新技能的同時，牢記舊的知識。這意味著，我們可以打造出真正智能的服務型機器人、工業協作機器人，甚至是在醫療、教育等領域提供客製化服務的專家機器人。市場規模將是數十億美元級別的！更重要的是，ATS架構的可擴展性，讓我們能輕鬆整合更多感測器和任務模型，打造出更強大的AI系統。我們相信，ATS將成為未來AI機器人的核心技術，引領下一波AI浪潮。現在投資，您將成為這場革命的先驅！", "audio": "docs/data/audios/2506.09623v1.wav"}
{"query": "Diffusion Model", "id": "2506.10612v1", "url": "http://arxiv.org/abs/2506.10612v1", "title": "TexTailor: Customized Text-aligned Texturing via Effective Resampling", "summary": "We present TexTailor, a novel method for generating consistent object\ntextures from textual descriptions. Existing text-to-texture synthesis\napproaches utilize depth-aware diffusion models to progressively generate\nimages and synthesize textures across predefined multiple viewpoints. However,\nthese approaches lead to a gradual shift in texture properties across\nviewpoints due to (1) insufficient integration of previously synthesized\ntextures at each viewpoint during the diffusion process and (2) the\nautoregressive nature of the texture synthesis process. Moreover, the\npredefined selection of camera positions, which does not account for the\nobject's geometry, limits the effective use of texture information synthesized\nfrom different viewpoints, ultimately degrading overall texture consistency. In\nTexTailor, we address these issues by (1) applying a resampling scheme that\nrepeatedly integrates information from previously synthesized textures within\nthe diffusion process, and (2) fine-tuning a depth-aware diffusion model on\nthese resampled textures. During this process, we observed that using only a\nfew training images restricts the model's original ability to generate\nhigh-fidelity images aligned with the conditioning, and therefore propose an\nperformance preservation loss to mitigate this issue. Additionally, we improve\nthe synthesis of view-consistent textures by adaptively adjusting camera\npositions based on the object's geometry. Experiments on a subset of the\nObjaverse dataset and the ShapeNet car dataset demonstrate that TexTailor\noutperforms state-of-the-art methods in synthesizing view-consistent textures.\nThe source code for TexTailor is available at\nhttps://github.com/Adios42/Textailor", "authors": ["Suin Lee", "Dae-Shik Kim"], "published_date": "2025-06-12", "timestamp": "2025-06-14T18:35:30.714083", "title_zh": "TexTailor：透過有效重採樣實現客製化、文本對齊的材質貼圖", "summary_zh": "TexTailor 是一種創新的方法，能根據文字描述生成一致的物體材質貼圖。現有的技術常因視角間材質屬性偏移和相機位置預設等問題，導致材質不夠連貫。TexTailor 透過在擴散過程中重複整合先前合成的材質資訊，並根據物體幾何形狀調整相機位置來解決這些問題。此外，我們還提出了一種性能保持損失，以減輕訓練圖像數量不足對模型產生高保真圖像能力的限制。實驗證明，TexTailor 在合成視角一致的材質貼圖方面優於現有技術。程式碼已開源。", "applications": ["**遊戲開發：** 想像一下，遊戲設計師只要輸入『生鏽的金屬盔甲』，TexTailor就能自動生成各種角度都一致的盔甲材質，省下大量手動調整的時間。", "**虛擬試穿：** 在網路上買衣服，最怕材質跟想像不一樣。TexTailor 可以根據『柔軟的羊毛衫』生成逼真的材質，讓你在家就能體驗到真實的穿著感受。", "**建築設計：** 建築師可以輸入『粗糙的石牆』，TexTailor 就能快速生成適用於建築模型的材質，讓客戶更直觀地看到建築完成後的樣貌。"], "pitch": "各位投資人，我們正站在AI賦能3D內容創作的風口浪尖！TexTailor 解決了長期以來困擾業界的材質一致性問題，大幅降低了3D模型製作的門檻和成本。試想一下，未來電商平台可以根據消費者的文字描述，即時生成客製化的商品3D模型，提供前所未有的購物體驗；遊戲公司可以更快、更低成本地打造精美的遊戲世界；元宇宙的內容創作者可以更自由地揮灑創意。TexTailor不僅僅是一個技術，更是一個通往無限可能的入口。我們預計，TexTailor將在遊戲、電商、建築、教育等領域引發革命性的變革，市場潛力巨大。現在加入我們，共同打造3D內容創作的未來！", "audio": "docs/data/audios/2506.10612v1.wav"}
{"query": "AI", "id": "2506.10674v1", "url": "http://arxiv.org/abs/2506.10674v1", "title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving", "summary": "The increasing adoption of artificial intelligence in telecommunications has\nraised interest in the capability of Large Language Models (LLMs) to address\ndomain-specific, mathematically intensive tasks. Although recent advancements\nhave improved the performance of LLMs in general mathematical reasoning, their\neffectiveness within specialized domains, such as signal processing, network\noptimization, and performance analysis, remains largely unexplored. To address\nthis gap, we introduce TeleMath, the first benchmark dataset specifically\ndesigned to evaluate LLM performance in solving mathematical problems with\nnumerical solutions in the telecommunications domain. Comprising 500\nquestion-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the\ntelecommunications field. This paper outlines the proposed QnAs generation\npipeline, starting from a selected seed of problems crafted by Subject Matter\nExperts. The evaluation of a wide range of open-source LLMs reveals that best\nperformance on TeleMath is achieved by recent models explicitly designed for\nmathematical or logical reasoning. In contrast, general-purpose models, even\nthose with a large number of parameters, often struggle with these challenges.\nWe have released the dataset and the evaluation code to ease result\nreproducibility and support future research.", "authors": ["Vincenzo Colle", "Mohamed Sana", "Nicola Piovesan", "Antonio De Domenico", "Fadhel Ayed", "Merouane Debbah"], "published_date": "2025-06-12", "timestamp": "2025-06-14T21:24:25.452726", "title_zh": "TeleMath：大型語言模型在電信數學問題解決中的基準測試", "summary_zh": "本研究提出TeleMath，一個專門評估大型語言模型（LLM）在電信領域數學問題解決能力的基準數據集。該數據集包含500個問答對，涵蓋電信領域的多個主題，如訊號處理、網路優化和性能分析。研究結果顯示，專為數學或邏輯推理設計的LLM在TeleMath上表現最佳，而通用型LLM即使參數龐大，也難以應對這些挑戰。TeleMath的發布將有助於促進LLM在電信領域的應用研究，並推動相關技術的發展。我們釋出了數據集和評估程式碼，以方便結果重現並支持未來的研究。", "applications": ["智慧型手機訊號不佳時，自動診斷並提供解決方案：使用者只需描述問題，AI就能分析訊號數據，判斷是基地台擁塞、干擾還是其他原因，並給予建議，例如移動到空曠區域或重啟手機。", "網路管理員利用AI優化網路配置：AI可以分析網路流量數據，自動調整頻寬分配，確保重要服務（如視訊會議、線上遊戲）的穩定性，提升整體網路效能。", "電信公司利用AI預測設備故障：AI可以監控基地台等設備的運行數據，提前預測潛在故障，避免服務中斷，降低維護成本。"], "pitch": "各位投資人，我們正在打造電信產業的AI大腦！TeleMath不僅是一個數據集，更是一把開啟電信領域AI應用的鑰匙。想像一下，未來的電信網路將更加智慧、高效、可靠，而這一切都將基於我們所開發的TeleMath技術。\n\n市場規模巨大！5G、物聯網、智慧城市…這些熱門領域都離不開電信基礎建設。我們的技術能顯著提升電信網路的性能和效率，降低運營成本，為電信公司帶來巨大的商業價值。更重要的是，我們可以將這項技術授權給設備製造商、網路營運商、甚至是政府機構，打造一個龐大的AI生態系統。\n\n我們團隊擁有電信和AI領域的頂尖專家，已經完成了初步的驗證。現在，我們需要您的資金支持，加速數據集的擴充、模型的訓練和產品的商業化。讓我們一起引領電信產業的AI革命，共同分享這數十億美元的市場盛宴！未來，我們可以進一步將此技術應用於其他工程領域，例如航空航天、電力系統等，打造更廣闊的AI應用前景。", "audio": "docs/data/audios/2506.10674v1.wav"}
{"query": "Foundation Model", "id": "2506.09593v1", "url": "http://arxiv.org/abs/2506.09593v1", "title": "Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks", "summary": "Reliable uncertainty calibration is essential for safely deploying deep\nneural networks in high-stakes applications. Deep neural networks are known to\nexhibit systematic overconfidence, especially under distribution shifts.\nAlthough foundation models such as ConvNeXt, EVA and BEiT have demonstrated\nsignificant improvements in predictive performance, their calibration\nproperties remain underexplored. This paper presents a comprehensive\ninvestigation into the calibration behavior of foundation models, revealing\ninsights that challenge established paradigms. Our empirical analysis shows\nthat these models tend to be underconfident in in-distribution predictions,\nresulting in higher calibration errors, while demonstrating improved\ncalibration under distribution shifts. Furthermore, we demonstrate that\nfoundation models are highly responsive to post-hoc calibration techniques in\nthe in-distribution setting, enabling practitioners to effectively mitigate\nunderconfidence bias. However, these methods become progressively less reliable\nunder severe distribution shifts and can occasionally produce counterproductive\nresults. Our findings highlight the complex, non-monotonic effects of\narchitectural and training innovations on calibration, challenging established\nnarratives of continuous improvement.", "authors": ["Achim Hekler", "Lukas Kuhn", "Florian Buettner"], "published_date": "2025-06-11", "timestamp": "2025-06-14T21:26:12.749511", "title_zh": "超越過度自信：基礎模型重新定義深度神經網路中的校準", "summary_zh": "深度神經網路的可靠不確定性校準，對於在高風險應用中安全部署至關重要。過去研究顯示，深度神經網路常表現出過度自信，尤其是在資料分布發生變化時。本研究深入探討ConvNeXt、EVA和BEiT等基礎模型的校準特性，發現它們在原始資料分布中傾向於低度自信，導致校準誤差增加，但在資料分布變化時校準效果反而提升。此外，基礎模型對事後校準技術反應良好，可有效緩解低度自信偏差。然而，在嚴重資料分布變化下，這些方法的可靠性會降低，甚至產生反效果。這項研究挑戰了持續改進的傳統觀念，揭示了架構和訓練創新對校準的複雜且非單調影響。", "applications": ["**醫療診斷輔助：** 想像一下，醫生使用AI判讀X光片，這項技術可以幫助AI更準確地評估診斷結果的不確定性，避免過度自信導致的誤判，讓醫生更有信心做出正確的決策。", "**自動駕駛安全：** 自動駕駛系統需要準確判斷路況。這項技術可以讓AI更精確地知道自己判斷的可靠程度，在不確定情況下及時採取安全措施，例如減速或停止，降低事故風險。", "**金融風險評估：** 銀行在貸款審核時使用AI評估風險。如果AI能更準確地校準自己的預測，銀行就能更有效地識別高風險客戶，減少壞帳，並提供更客製化的金融服務。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它將重新定義AI的可靠性。過去的深度學習模型存在過度自信的問題，導致在關鍵決策時刻容易出錯。我們的研究發現，新一代的基礎模型雖然性能更強大，但也存在低度自信的問題。我們開發的校準技術，能有效解決這個問題，讓AI在各種應用場景中都更加可靠、值得信賴。想像一下，一個在醫療、金融、自動駕駛等領域都更安全、更精準的AI，這將釋放巨大的商業價值。我們的技術不僅能提升現有AI系統的性能，還能開創全新的AI應用，例如，在風險極高的金融衍生品交易中，提供更可靠的風險評估，或者在無人機送貨中，確保包裹安全準時送達。我們相信，隨著AI在各行各業的滲透，對可靠AI的需求將會爆炸性增長。現在投資我們，您將站在AI革命的最前沿，共同打造一個更安全、更智能的未來！", "audio": "docs/data/audios/2506.09593v1.wav"}
{"query": "Diffusion Model", "id": "2506.10605v1", "url": "http://arxiv.org/abs/2506.10605v1", "title": "High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model", "summary": "We present LatentCSI, a novel method for generating images of the physical\nenvironment from WiFi CSI measurements that leverages a pretrained latent\ndiffusion model (LDM). Unlike prior approaches that rely on complex and\ncomputationally intensive techniques such as GANs, our method employs a\nlightweight neural network to map CSI amplitudes directly into the latent space\nof an LDM. We then apply the LDM's denoising diffusion model to the latent\nrepresentation with text-based guidance before decoding using the LDM's\npretrained decoder to obtain a high-resolution image. This design bypasses the\nchallenges of pixel-space image generation and avoids the explicit image\nencoding stage typically required in conventional image-to-image pipelines,\nenabling efficient and high-quality image synthesis. We validate our approach\non two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi\ndevices and cameras; and a subset of the publicly available MM-Fi dataset. The\nresults demonstrate that LatentCSI outperforms baselines of comparable\ncomplexity trained directly on ground-truth images in both computational\nefficiency and perceptual quality, while additionally providing practical\nadvantages through its unique capacity for text-guided controllability.", "authors": ["Eshan Ramesh", "Nishio Takayuki"], "published_date": "2025-06-12", "timestamp": "2025-06-14T21:27:25.239791", "title_zh": "基於預訓練潛在擴散模型，從WiFi CSI高效生成高解析度圖像", "summary_zh": "本研究提出LatentCSI，一種利用預訓練潛在擴散模型（LDM）從WiFi CSI測量數據生成物理環境圖像的新方法。不同於以往依賴GAN等複雜技術，我們使用輕量級神經網路將CSI幅度直接映射到LDM的潛在空間。然後，我們應用LDM的去噪擴散模型，並使用文本引導，再透過LDM的預訓練解碼器解碼，獲得高解析度圖像。此設計繞過了像素空間圖像生成的挑戰，並避免了傳統圖像到圖像流程中所需的顯式圖像編碼階段，實現高效且高品質的圖像合成。實驗結果表明，LatentCSI在計算效率和感知品質方面優於直接在真實圖像上訓練的基線模型，並通過其獨特的文本引導可控性提供實用優勢。", "applications": ["**失物協尋：** 在人潮眾多的商場或車站，如果有人遺失物品，可以利用WiFi訊號變化快速生成物品周圍環境的圖像，協助尋找。", "**智慧安防：** 家中或辦公室安裝多個WiFi感測器，一旦偵測到異常訊號（例如有人闖入），就能即時生成現場圖像，提供更精確的警報資訊。", "**輔助導航：** 在GPS訊號不佳的室內環境，例如地下停車場，可以利用WiFi訊號生成環境圖像，結合AR技術提供更直觀的導航指引。"], "pitch": "各位投資人，想像一下，我們不再需要昂貴的攝影機和複雜的圖像辨識系統，就能『看見』周遭的物理世界！LatentCSI技術，利用無處不在的WiFi訊號，就能生成高解析度的環境圖像，這將徹底顛覆安防、導航、物聯網等領域。試想，未來每個智慧家庭、辦公室，甚至購物中心，都佈滿了我們的WiFi感測器，它們不僅提供網路，還能成為『眼睛』。我們可以將這些數據賣給保險公司，用於理賠評估；賣給零售商，用於顧客行為分析；甚至賣給政府，用於城市安全監控。這是一個百億美元級別的市場，而我們擁有領先的技術優勢。更令人興奮的是，LatentCSI還可以結合AI，實現更高級的功能，例如，透過文字描述生成特定場景的圖像，這將開啟無限的商業可能性。現在加入我們，一起打造『WiFi視覺』的未來吧！", "audio": "docs/data/audios/2506.10605v1.wav"}
{"query": "AI", "id": "2506.10673v1", "url": "http://arxiv.org/abs/2506.10673v1", "title": "Reaching the Ultimate Quantum Precision Limit at Colliders: Conditions and Case Studies", "summary": "We investigate whether collider experiments can reach the quantum limit of\nprecision, defined by the quantum Fisher information (QFI), using only\nclassical observables such as particle momenta. As a case study, we focus on\nthe $\\tau^+\\tau^-$ system and the decay channel $\\tau \\to \\pi \\nu$, which\noffers maximal spin-analyzing power and renders the decay a projective\nmeasurement. We develop a general framework to determine when collider\nmeasurements can, in principle, saturate the QFI in an entangled biparticle\nsystem, and this framework extends naturally to other such systems. Within this\nframework, QFI saturation occurs if and only if the symmetric logarithmic\nderivative (SLD) commutes with a complete set of orthonormal separable\nprojectors associated with collider-accessible measurements. This separability\ncondition, reflecting the independence of decay amplitudes, is highly\nnontrivial. To meet this condition, a key requirement is that the spin density\nmatrix be rank-deficient, allowing the SLD sufficient freedom. We show that the\nclassical Fisher information asymptotically saturates the QFI for magnetic\ndipole moments and CP-violating Higgs interactions in selected phase-space\nregions, but not for electric dipole moments. These results bridge quantum\nmetrology and collider physics, providing a systematic method to identify\nquantum-optimal sensitivity in collider experiments.", "authors": ["Tengyu Ai", "Qi Bi", "Yuxin He", "Jia Liu", "Xiao-Ping Wang"], "published_date": "2025-06-12", "timestamp": "2025-06-15T02:13:12.060025", "title_zh": "在對撞機中達到極限量子精度：條件與案例研究", "summary_zh": "本研究探討對撞機實驗是否能僅使用粒子動量等古典可觀測量，達到量子精度極限，即量子費雪信息（QFI）。我們以$\\,\\tau^+\\tau^-$系統和衰變通道$\\tau \\to \\pi \\nu$作為案例，後者提供最大的自旋分析能力，並使衰變成為投影測量。我們開發了一個通用框架，以確定對撞機測量在原理上何時能使糾纏雙粒子系統中的QFI飽和。QFI飽和發生的充分必要條件是對稱對數導數（SLD）與一組完整的、與對撞機可訪問測量相關的正交可分離投影算符交換。我們發現，在特定相空間區域，古典費雪信息可以漸近地達到磁偶極矩和違反CP對稱性的希格斯相互作用的QFI，但電偶極矩則不行。這些結果連接了量子計量學和對撞機物理學，提供了一種系統的方法來識別對撞機實驗中的量子最優靈敏度。", "applications": ["**精準醫療：** 這項技術能更精準地測量基本粒子的性質，就像更精密的尺可以更準確地測量藥物分子結構，加速新藥開發，實現個人化醫療。", "**更安全的核能：** 透過更深入了解基本粒子的交互作用，我們可以更有效地控制核反應，提升核能發電的安全性，減少意外風險。", "**先進材料開發：** 了解基本粒子的行為有助於我們設計出具有特殊性質的新材料，例如超導體或更堅固的複合材料，應用於各個領域。"], "pitch": "各位投資人，想像一下，我們正在解鎖宇宙最深層的秘密，而這項技術正是通往寶藏的鑰匙！我們開發的框架，能讓對撞機實驗達到前所未有的量子精度，這不僅是學術上的突破，更是潛力無限的商業機會。試想，如果我們能更精準地測量基本粒子的性質，就能夠徹底改變藥物開發、材料科學，甚至能源產業。舉例來說，透過精準測量希格斯粒子的性質，我們或許能找到通往新能源的道路，解決全球能源危機！更重要的是，這項技術具有極高的戰略價值，能提升國家在科技領域的競爭力。我們相信，在您的支持下，這項技術將引領下一波科技革命，為人類帶來巨大的福祉，並為您帶來豐厚的回報！", "audio": "docs/data/audios/2506.10673v1.wav"}
{"query": "Foundation Model", "id": "2506.09453v1", "url": "http://arxiv.org/abs/2506.09453v1", "title": "From Partial to Monadic: Combinatory Algebra with Effects", "summary": "Partial Combinatory Algebras (PCAs) provide a foundational model of the\nuntyped $\\lambda$-calculus and serve as the basis for many notions of\ncomputability, such as realizability theory. However, PCAs support a very\nlimited notion of computation by only incorporating non-termination as a\ncomputational effect. To provide a framework that better internalizes a wide\nrange of computational effects, this paper puts forward the notion of Monadic\nCombinatory Algebras (MCAs). MCAs generalize the notion of PCAs by structuring\nthe combinatory algebra over an underlying computational effect, embodied by a\nmonad. We show that MCAs can support various side effects through the\nunderlying monad, such as non-determinism, stateful computation and\ncontinuations. We further obtain a categorical characterization of MCAs within\nFreyd Categories, following a similar connection for PCAs. Moreover, we explore\nthe application of MCAs in realizability theory, presenting constructions of\neffectful realizability triposes and assemblies derived through evidenced\nframes, thereby generalizing traditional PCA-based realizability semantics. The\nmonadic generalization of the foundational notion of PCAs provides a\ncomprehensive and powerful framework for internally reasoning about effectful\ncomputations, paving the path to a more encompassing study of computation and\nits relationship with realizability models and programming languages.", "authors": ["Liron Cohen", "Ariel Grunfeld", "Dominik Kirst", "Étienne Miquey"], "published_date": "2025-06-11", "timestamp": "2025-06-15T02:15:00.579694", "title_zh": "從偏函數到單子：具備副作用的組合代數", "summary_zh": "偏函數組合代數(PCA)是無類型lambda演算的基礎模型，但僅支持非終止作為計算副作用。本研究提出單子組合代數(MCA)，透過單子將組合代數架構於底層計算副作用之上，廣泛地整合計算副作用。MCA能支持多種副作用，如非確定性、有狀態計算和續延。我們在Freyd範疇中獲得了MCA的範疇刻劃，並探討了MCA在可實現性理論中的應用，展示了透過證據框架導出的有效可實現性三一律和組合結構，推廣了傳統基於PCA的可實現性語義。這種單子泛化為有效計算的內部推理提供了一個全面而強大的框架，為更廣泛地研究計算及其與可實現性模型和程式語言的關係鋪平了道路。", "applications": ["智慧合約：在區塊鏈上，智慧合約需要處理各種複雜的副作用，例如交易失敗、狀態更新等。MCA可以幫助開發者更安全、可靠地編寫智慧合約，避免因副作用處理不當導致的漏洞或錯誤。", "AI訓練：在機器學習中，訓練模型通常需要大量的計算資源和時間。MCA可以幫助優化訓練過程，例如通過非確定性選擇更有效的訓練策略，或通過狀態ful計算追蹤訓練進度。", "遊戲開發：遊戲開發中，角色行為、場景變化等都需要處理各種副作用。MCA可以幫助遊戲開發者更好地管理這些副作用，創造更豐富、更具互動性的遊戲體驗。"], "pitch": "各位投資人，我們正處於一個軟體定義一切的時代，而軟體的複雜度正以前所未有的速度增長。傳統的程式設計方法在處理複雜的副作用時顯得力不從心，導致bug頻發、系統崩潰等問題。我們的Monadic Combinatory Algebras (MCAs)技術，正是為了解決這個痛點而生。它提供了一種全新的、更安全、更可靠的程式設計範式，能夠大幅降低軟體開發和維護的成本，並提高軟體的穩定性和可靠性。\n\n想像一下，未來所有的軟體系統，從金融交易系統到自動駕駛汽車，都將基於我們的MCA技術構建，從而避免因副作用處理不當而導致的災難性後果。這是一個數十億美元級別的市場，而我們正處於領先地位。我們已經證明了MCA在理論上的可行性，並正在開發實際的應用程式。我們需要您的資金，將這項技術推向市場，並與業界領先的程式語言和平台整合。投資我們，就是投資軟體的未來，投資一個更安全、更可靠的世界！我們不僅僅是在開發一種新的程式設計技術，我們正在構建一個新的軟體生態系統，一個將改變遊戲規則的生態系統。", "audio": "docs/data/audios/2506.09453v1.wav"}
{"query": "Diffusion Model", "id": "2506.10576v1", "url": "http://arxiv.org/abs/2506.10576v1", "title": "Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres", "summary": "Do contemporary diffusion models preserve the class geometry of\nhyperspherical data? Standard diffusion models rely on isotropic Gaussian noise\nin the forward process, inherently favoring Euclidean spaces. However, many\nreal-world problems involve non-Euclidean distributions, such as hyperspherical\nmanifolds, where class-specific patterns are governed by angular geometry\nwithin hypercones. When modeled in Euclidean space, these angular subtleties\nare lost, leading to suboptimal generative performance. To address this\nlimitation, we introduce HyperSphereDiff to align hyperspherical structures\nwith directional noise, preserving class geometry and effectively capturing\nangular uncertainty. We demonstrate both theoretically and empirically that\nthis approach aligns the generative process with the intrinsic geometry of\nhyperspherical data, resulting in more accurate and geometry-aware generative\nmodels. We evaluate our framework on four object datasets and two face\ndatasets, showing that incorporating angular uncertainty better preserves the\nunderlying hyperspherical manifold. Resources are available at:\n{https://github.com/IAB-IITJ/Harmonizing-Geometry-and-Uncertainty-Diffusion-with-Hyperspheres/}", "authors": ["Muskan Dosi", "Chiranjeev Chiranjeev", "Kartik Thakral", "Mayank Vatsa", "Richa Singh"], "published_date": "2025-06-12", "timestamp": "2025-06-15T02:16:32.312421", "title_zh": "調和幾何與不確定性：使用超球面的擴散模型", "summary_zh": "現有的擴散模型在處理超球面數據時，由於前向過程中使用各向同性高斯雜訊，傾向於歐幾里得空間，導致無法有效保留類別幾何結構。針對此問題，我們提出了 HyperSphereDiff，通過調整方向性雜訊，使擴散過程與超球面結構對齊，從而保留類別幾何結構並有效捕捉角度不確定性。理論與實驗證明，此方法能更精確地生成幾何感知模型。我們在多個物體和人臉數據集上驗證了該框架，結果表明，整合角度不確定性能夠更好地保留底層的超球面流形。", "applications": ["人臉辨識系統：提升在不同光照、角度下人臉辨識的準確性，因為它能更好處理人臉在超球面上的角度變化。", "醫學影像分析：更精準地分析醫學影像中器官或細胞的形狀，例如在癌症檢測中，可以更敏感地捕捉到不規則的細胞形狀變化。", "虛擬實境（VR）：創造更逼真的3D模型，尤其是在需要精確模擬物體角度和方向的應用中，例如模擬飛行或駕駛。"], "pitch": "各位投資人，我們帶來的是一項突破性的技術：HyperSphereDiff，它能讓AI更精準地理解和生成非歐幾里得空間的數據。想像一下，現今AI在處理3D視覺、機器人導航、甚至生物分子建模時，都受限於傳統歐幾里得空間的思維。HyperSphereDiff打破了這個限制，開啟了AI應用的新紀元！\n\n我們的技術不僅能提升現有人臉辨識、醫學影像分析的精準度，更能催生全新的應用：比如，基於超球面幾何的AI藝術創作，讓AI能模仿並創造出前所未有的藝術風格；又或者，在航空航天領域，HyperSphereDiff能幫助設計更穩定的飛行器，因為它能更準確地模擬空氣動力學中的複雜角度關係。更長遠來看，它將成為元宇宙創建精確、逼真3D環境的關鍵技術。我們相信，HyperSphereDiff將引領下一代AI革命，成為各行各業不可或缺的基礎設施。現在加入我們，一起塑造AI的未來！", "audio": "docs/data/audios/2506.10576v1.wav"}
{"query": "AI", "id": "2506.10627v1", "url": "http://arxiv.org/abs/2506.10627v1", "title": "NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors", "summary": "This paper presents our system for Track 1: Mistake Identification in the BEA\n2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The\ntask involves evaluating whether a tutor's response correctly identifies a\nmistake in a student's mathematical reasoning. We explore four approaches: (1)\nan ensemble of machine learning models over pooled token embeddings from\nmultiple pretrained language models (LMs); (2) a frozen sentence-transformer\nusing [CLS] embeddings with an MLP classifier; (3) a history-aware model with\nmulti-head attention between token-level history and response embeddings; and\n(4) a retrieval-augmented few-shot prompting system with a large language model\n(LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples,\nconstructs structured prompts, and uses schema-guided output parsing to produce\ninterpretable predictions. It outperforms all baselines, demonstrating the\neffectiveness of combining example-driven prompting with LLM reasoning for\npedagogical feedback assessment. Our code is available at\nhttps://github.com/NaumanNaeem/BEA_2025.", "authors": ["Numaan Naeem", "Sarfraz Ahmad", "Momina Ahsan", "Hasan Iqbal"], "published_date": "2025-06-12", "timestamp": "2025-06-15T06:35:05.300173", "title_zh": "NeuralNexus於BEA 2025共享任務：檢索增強提示在AI輔導中用於錯誤識別", "summary_zh": "本研究介紹NeuralNexus系統，用於評估AI輔導系統在數學推理中識別學生錯誤的能力。 我們探索了多種方法，最終採用了檢索增強的少量樣本提示系統，結合大型語言模型GPT-4o。 該系統通過檢索語義相似的範例，構建結構化提示，並使用模式引導的輸出解析來產生可解釋的預測。 實驗結果表明，該系統超越了所有基準模型，證明了將範例驅動的提示與LLM推理相結合，在教育反饋評估方面的有效性。 簡單來說，就是用AI幫AI家教抓錯，而且效果很好！", "applications": ["線上家教：AI家教透過這個技術，能更精準地找出學生數學解題步驟中的錯誤，並給予即時回饋，就像一位經驗豐富的老師在旁指導。", "程式碼偵錯：類似的概念也能應用在程式碼偵錯上，AI能分析程式碼邏輯，找出潛在的錯誤或漏洞，幫助工程師加速開發流程。", "醫療診斷輔助：醫生可以利用AI分析病患的病歷和檢查報告，找出可能的疾病徵兆，並提供診斷建議，提高醫療效率和準確性。"], "pitch": "各位投資人，我們NeuralNexus的AI輔導錯誤識別技術，不只是一個研究專案，更是一個顛覆教育產業的潛力股！想像一下，全球有多少學生需要課後輔導？我們的技術能讓AI家教更聰明、更有效率，大幅降低教育成本，讓高品質的教育資源普及到每個角落。更棒的是，這項技術的應用範圍遠不止於教育，從程式碼偵錯到醫療診斷，都能看到它的身影。我們擁有領先的技術和優秀的團隊，現在正尋求您的資金挹注，一起打造AI教育的新時代！未來，我們更計畫將此技術整合至元宇宙的教育平台，打造沉浸式學習體驗，讓學習不再枯燥乏味。這絕對是您不容錯過的投資機會！", "audio": "docs/data/audios/2506.10627v1.wav"}
{"query": "Foundation Model", "id": "2506.09448v1", "url": "http://arxiv.org/abs/2506.09448v1", "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary", "summary": "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models\n(OWSM), are trained on massive datasets to achieve accurate automatic speech\nrecognition. However, even SFMs struggle to accurately recognize rare and\nunseen words. While contextual biasing (CB) is a promising approach to improve\nrecognition of such words, most CB methods are trained from scratch, resulting\nin lower performance than SFMs due to the lack of pre-trained knowledge. This\npaper integrates an existing CB method with OWSM v3.1 while freezing its\npre-trained parameters. By leveraging the knowledge embedded in SFMs, the\nproposed method enables effective CB while preserving the advantages of SFMs,\neven with a small dataset. Experimental results show that the proposed method\nimproves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9\npoint improvement in the overall WER while reducing the real-time factor by\n7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean\nset.", "authors": ["Yui Sudo", "Yusuke Fujita", "Atsushi Kojima", "Tomoya Mizumoto", "Lianbo Liu"], "published_date": "2025-06-11", "timestamp": "2025-06-15T06:36:23.713839", "title_zh": "OWSM偏置：利用動態詞彙將Open Whisper風格語音模型置於自動語音辨識的上下文中", "summary_zh": "本研究改良了Open Whisper風格的語音模型，使其能更準確地辨識罕見詞彙。傳統方法需要從頭訓練，效果不如大型語音模型。我們提出的方法將現有的上下文偏置技術整合到OWSM v3.1中，保留了模型原有的強大能力，即使在小數據集上也能有效提升辨識準確度。實驗證明，我們的技術能顯著降低錯誤率，並減少即時處理所需的時間。這項技術使得語音辨識系統在處理專業術語、人名地名等特殊詞彙時更加精準。", "applications": ["醫生在診斷時，語音輸入病歷，系統能準確辨識罕見疾病名稱或藥物名稱，減少手動輸入錯誤。", "律師在準備法律文件時，語音轉文字功能可以精準辨識法律術語，提高工作效率。", "遊戲玩家使用語音指令控制角色，系統能辨識特殊的遊戲術語或玩家自定義的暱稱，提升遊戲體驗。"], "pitch": "想像一下，一個能聽懂所有專業術語、行話和新創詞彙的語音助理！我們的OWSM偏置技術，就像為現有語音辨識引擎裝上了一個超強大腦，讓它能精準辨識過去無法辨識的詞彙。這不僅僅是技術升級，更是開創了全新的應用場景。試想，在醫療、法律、金融等專業領域，我們的技術能大幅提升工作效率，降低錯誤率，甚至催生全新的語音服務。更進一步，我們可以將這項技術應用於客製化語音助理、智能客服、甚至打造一個能聽懂各種方言和口音的全球通用語音平台。這是一個潛力無限的市場，現在投資，您將站在語音辨識技術革命的最前沿！", "audio": "docs/data/audios/2506.09448v1.wav"}
{"query": "Diffusion Model", "id": "2506.10532v1", "url": "http://arxiv.org/abs/2506.10532v1", "title": "Equivariant Neural Diffusion for Molecule Generation", "summary": "We introduce Equivariant Neural Diffusion (END), a novel diffusion model for\nmolecule generation in 3D that is equivariant to Euclidean transformations.\nCompared to current state-of-the-art equivariant diffusion models, the key\ninnovation in END lies in its learnable forward process for enhanced generative\nmodelling. Rather than pre-specified, the forward process is parameterized\nthrough a time- and data-dependent transformation that is equivariant to rigid\ntransformations. Through a series of experiments on standard molecule\ngeneration benchmarks, we demonstrate the competitive performance of END\ncompared to several strong baselines for both unconditional and conditional\ngeneration.", "authors": ["François Cornet", "Grigory Bartosh", "Mikkel N. Schmidt", "Christian A. Naesseth"], "published_date": "2025-06-12", "timestamp": "2025-06-15T06:37:35.600021", "title_zh": "分子生成的等變神經擴散模型", "summary_zh": "本研究提出一種名為「等變神經擴散」（END）的新型擴散模型，專為3D分子生成設計，並具備對歐幾里德變換的等變性。END的關鍵創新在於其可學習的前向過程，這有助於增強生成模型的性能。與目前最先進的等變擴散模型相比，END的前向過程並非預先設定，而是通過一個隨時間和數據變化的變換來參數化，且該變換對剛性變換具有等變性。在標準分子生成基準測試中，實驗結果表明，無論是無條件生成還是條件生成，END的性能都優於幾個強大的基線模型。", "applications": ["藥物設計：想像一下，我們可以利用這項技術，快速生成大量具有特定藥效的分子結構，加速新藥開發，甚至針對個人基因客製化藥物。", "材料科學：透過生成具有特定物理或化學性質的分子，我們可以設計出更耐用、更輕便、或更高效能的新材料，應用於航空、電子產品等領域。", "化學合成：這項技術可以幫助化學家預測和設計新的化學反應路徑，簡化複雜分子的合成過程，降低實驗成本和時間。"], "pitch": "各位創投先進，我們正站在新藥開發和材料科學革命的浪潮之上！我們的「等變神經擴散」技術，就像一個分子世界的AI設計師，能以前所未有的速度和精度，創造出具有特定功能的分子。想像一下，我們可以利用AI設計出治療癌症的新藥，或是開發出超導材料，徹底改變能源產業。這不僅僅是一項技術，更是一個潛力無限的平台，未來可以授權給各大藥廠、材料公司，甚至建立一個AI驅動的分子設計平台，成為產業標準。現在投資，您將成為這場分子革命的領頭羊，共同開創一個充滿無限可能的未來！", "audio": "docs/data/audios/2506.10532v1.wav"}
{"query": "AI", "id": "2506.10624v1", "url": "http://arxiv.org/abs/2506.10624v1", "title": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization", "summary": "The ever-increasing complexity of HW/SW systems presents a persistent\nchallenge, particularly in safety-critical domains like automotive, where\nextensive testing is imperative. However, the availability of hardware often\nlags behind, hindering early-stage software development. To address this,\nVirtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a\npivotal solution, enabling pre-silicon execution and testing of unmodified\ntarget software. In this study, we propose an approach leveraging\ncontainerization to encapsulate VPs in order to reduce environment dependencies\nand enable cloud deployment for fast, parallelized test execution, as well as\nopen-source VP technologies such as QEMU and VCML to obviate the need for seat\nlicenses. To demonstrate the efficacy of our approach, we present an Artificial\nIntelligence (AI) accelerator VP case study. Through our research, we offer a\nrobust solution to address the challenges posed by the complexity of HW/SW\nsystems, with practical implications for accelerating HW/SW co-development.", "authors": ["Lukas Jünger", "Jan Henrik Weinstock", "Tim Kraus"], "published_date": "2025-06-12", "timestamp": "2025-06-15T09:26:08.669559", "title_zh": "快速虛擬平台中的可擴展軟體測試：利用 SystemC、QEMU 與容器化技術", "summary_zh": "現今硬軟體系統日益複雜，尤其在汽車等安全攸關領域，測試至關重要。然而，硬體往往供不應求，阻礙了早期軟體開發。本研究提出一種方法，利用容器化技術封裝基於SystemC TLM-2.0標準的虛擬平台(VP)，以減少環境依賴性，並支援雲端部署，實現快速並行測試。同時採用QEMU和VCML等開源VP技術，避免了授權費用。我們透過一個人工智慧(AI)加速器VP案例研究，展示了此方法的有效性，為加速硬軟體協同開發提供了一個穩健的解決方案。", "applications": ["想像一下，汽車廠商在實際晶片生產前，就能透過雲端模擬測試自動駕駛系統的軟體，大幅縮短開發時間，降低風險，讓我們更快用上更安全的自駕車。", "如果你是遊戲開發者，可以在遊戲上市前，利用這種技術在多種虛擬平台上測試遊戲的相容性和穩定性，確保玩家在不同裝置上都能獲得最佳體驗，減少bug和客訴。", "智慧家電製造商可以利用虛擬平台，在雲端大規模測試家電的韌體更新，確保更新過程安全穩定，避免使用者因為更新失敗而導致家電無法使用。"], "pitch": "各位投資人，我們正在打造一個革命性的硬軟體協同開發平台。隨著物聯網、AI和自動駕駛等技術的快速發展，硬軟體系統的複雜度呈指數級增長，傳統的開發和測試方法已經難以應付。我們的解決方案，基於SystemC、QEMU和容器化等先進技術，提供了一個可擴展、高效且低成本的虛擬平台，讓開發者可以在雲端進行大規模的並行測試，大幅縮短產品上市時間，降低開發成本，並提高產品的品質和可靠性。我們相信，在不久的將來，所有的硬軟體開發都將基於虛擬平台進行，而我們的技術將成為這個領域的領導者。這不僅僅是一個工具，而是一個全新的開發模式，一個價值數十億美元的市場。現在加入我們，一起開創硬軟體協同開發的新紀元！未來，我們甚至可以將此技術應用於太空探索，在發射火箭前，模擬各種極端環境，確保任務的成功！", "audio": "docs/data/audios/2506.10624v1.wav"}
{"query": "Foundation Model", "id": "2506.09440v1", "url": "http://arxiv.org/abs/2506.09440v1", "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "published_date": "2025-06-11", "timestamp": "2025-06-15T09:27:14.846756", "title_zh": "GigaChat家族：透過混合專家架構實現高效的俄語語言建模", "summary_zh": "本研究介紹了GigaChat，一系列專為俄語設計的大型語言模型。考量到訓練大型語言模型所需的大量計算資源，GigaChat家族提供了不同規模的模型，包含基礎模型和指令調整版本。論文詳細說明了模型架構、預訓練過程和實驗設計，並在俄語和英語基準測試中評估了其性能，與其他多語言模型進行比較。頂尖模型已透過API、Telegram機器人和Web介面提供試用。此外，三個開放原始碼的GigaChat模型已發布，旨在擴大俄語自然語言處理的研究機會和支持產業解決方案的開發。", "applications": ["俄語學習App：GigaChat可以作為俄語學習App的智慧助手，提供即時翻譯、語法檢查和口語練習，讓學習者更有效率地掌握俄語。", "智能客服：企業可以使用GigaChat來建立俄語智能客服，快速解答客戶問題、處理訂單，提升客戶滿意度並降低人力成本。", "新聞摘要：GigaChat可以自動生成俄語新聞的簡短摘要，讓使用者在短時間內掌握重要資訊，節省閱讀時間。"], "pitch": "各位投資人，想像一下，一個能精準理解並流暢使用俄語的人工智慧，它不僅僅是一個翻譯工具，更是一個能深刻理解俄羅斯文化、商業邏輯的智能夥伴。GigaChat家族的出現，填補了俄語大型語言模型的市場空白。俄語是全球重要的語言之一，擁有龐大的使用人口和市場潛力。GigaChat不僅能應用於智能客服、教育等領域，更能深入影響俄語地區的商業、文化交流。我們預見，GigaChat將成為俄語市場的AI基礎設施，驅動新一代的應用程式和服務。投資GigaChat，就是投資俄語AI的未來，搶佔先機，共同開創一個充滿可能性的市場！", "audio": "docs/data/audios/2506.09440v1.wav"}
{"query": "Diffusion Model", "id": "2506.10507v1", "url": "http://arxiv.org/abs/2506.10507v1", "title": "Edit360: 2D Image Edits to 3D Assets from Any Angle", "summary": "Recent advances in diffusion models have significantly improved image\ngeneration and editing, but extending these capabilities to 3D assets remains\nchallenging, especially for fine-grained edits that require multi-view\nconsistency. Existing methods typically restrict editing to predetermined\nviewing angles, severely limiting their flexibility and practical applications.\nWe introduce Edit360, a tuning-free framework that extends 2D modifications to\nmulti-view consistent 3D editing. Built upon video diffusion models, Edit360\nenables user-specific editing from arbitrary viewpoints while ensuring\nstructural coherence across all views. The framework selects anchor views for\n2D modifications and propagates edits across the entire 360-degree range. To\nachieve this, Edit360 introduces a novel Anchor-View Editing Propagation\nmechanism, which effectively aligns and merges multi-view information within\nthe latent and attention spaces of diffusion models. The resulting edited\nmulti-view sequences facilitate the reconstruction of high-quality 3D assets,\nenabling customizable 3D content creation.", "authors": ["Junchao Huang", "Xinting Hu", "Zhuotao Tian", "Shaoshuai Shi", "Li Jiang"], "published_date": "2025-06-12", "timestamp": "2025-06-15T09:28:38.652565", "title_zh": "Edit360：從任意角度將2D圖像編輯應用於3D資產", "summary_zh": "Edit360 是一個免微調的框架，能將2D圖像編輯擴展到多視角一致的3D編輯。它基於影片擴散模型，讓使用者能從任何角度進行編輯，同時確保所有視角的結構一致性。Edit360 透過創新的「錨點視角編輯傳播」機制，在擴散模型的潛在和注意力空間中對齊並融合多視角信息，從而生成高品質的3D資產。這項技術實現了可定制的3D內容創建，極大地提升了3D編輯的靈活性和實用性。", "applications": ["想像一下，你可以用手機拍一張家裡客廳的照片，然後輕鬆地把沙發換成不同的顏色或款式，甚至直接換成另一種風格的沙發，並且從任何角度看都非常真實。裝修設計再也不用大費周章，手機就能搞定。", "線上購物時，你可以把喜歡的鞋子或家具「放」到自己家裡，調整大小和角度，從各個角度看看是否搭配。省去退換貨的麻煩，購物體驗更直觀。", "遊戲開發者可以快速創建和修改3D角色和場景，不再需要耗時費力地手動建模。大幅縮短開發週期，降低開發成本。"], "pitch": "各位投資人，我們正處於3D內容爆炸性增長的時代，而Edit360正是開啟這扇大門的鑰匙。現有的3D編輯工具複雜且昂貴，Edit360則以其免微調、多視角一致的特性，將3D編輯的門檻降到最低。想像一下，一個擁有龐大用戶基礎的社交平台，讓用戶可以輕鬆創建、編輯和分享3D內容，這將帶來怎樣的流量和商業價值？從電商、遊戲、教育到工業設計，Edit360的應用場景無所不在。我們相信，Edit360將重新定義3D內容的創作和消費方式，成為元宇宙時代不可或缺的基礎設施。現在投資Edit360，就是投資3D內容的未來！", "audio": "docs/data/audios/2506.10507v1.wav"}
{"query": "AI", "id": "2506.10622v1", "url": "http://arxiv.org/abs/2506.10622v1", "title": "SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis", "summary": "The advancement of conversational AI systems relies on the availability of\nhigh-quality, flexible, and reproducible synthetic dialogues for training,\nevaluation, and benchmarking. SDialog is a modular, extensible Python toolkit\ndesigned to address the challenges of synthetic dialogue generation and\nanalysis. By leveraging instruction-tuned Large Language Models (LLMs), SDialog\nprovides abstractions for personas, orchestration, and scenario management,\nenabling the creation of realistic, diverse, and controllable conversational\ndata for research and development. SDialog supports workflows such as\nmulti-agent simulation and scenario-driven generation, and represents a step\nforward in the standardization of tools and frameworks for synthetic data\ngeneration, a crucial advancement for ensuring reproducibility in today's\nfast-evolving research landscape.", "authors": ["Sergio Burdisso", "Esaú Villatoro-Tello", "Petr Motlicek"], "published_date": "2025-06-12", "timestamp": "2025-06-15T12:48:53.976228", "title_zh": "SDialog：用於合成對話生成與分析的Python工具包", "summary_zh": "SDialog是一個Python工具包，旨在利用大型語言模型（LLMs）生成和分析合成對話。它提供角色、協調和情境管理的抽象化，能創建逼真、多樣且可控的對話數據，適用於研究和開發。SDialog支持多代理模擬和情境驅動生成等工作流程，有助於合成數據生成的工具和框架標準化，對於確保快速發展的研究領域中的可重複性至關重要。簡單來說，SDialog讓AI研究人員能更輕鬆地產生大量高品質的對話數據，加速對話式AI的發展。", "applications": ["想像一下，未來長輩在家可以跟AI健康管家聊天，AI會根據他們的飲食、運動習慣給予建議，就像一位貼心的家庭醫生，而且永遠不會不耐煩。", "如果你想學外語，不用再怕開口！AI語言學習夥伴可以24小時隨時和你練習，糾正你的發音，提供客製化的學習內容，讓你更快掌握外語。", "網路購物時，AI客服能更自然、更人性化地回答你的問題，甚至能預測你的需求，提供更精準的商品推薦，讓你享受更棒的購物體驗。"], "pitch": "各位投資人，SDialog是推動下一代對話式AI的關鍵引擎！我們正處於AI爆發性成長的時代，而高品質的訓練數據是AI發展的命脈。SDialog就像一座AI數據工廠，能源源不絕地生產各種情境的對話數據，大幅降低AI開發成本，加速產品上市時間。想像一下，未來AI客服、AI教育、AI醫療等領域，都將因為SDialog而產生革命性的變革！我們不僅僅是提供一個工具，更是打造一個AI數據生態系統，引領對話式AI的未來。現在投資SDialog，就是投資AI的未來，回報將超乎您的想像！", "audio": "docs/data/audios/2506.10622v1.wav"}
{"query": "Foundation Model", "id": "2506.09368v1", "url": "http://arxiv.org/abs/2506.09368v1", "title": "Anomaly Detection and Generation with Diffusion Models: A Survey", "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including\ncybersecurity, finance, healthcare, and industrial manufacturing, by\nidentifying unexpected patterns that deviate from established norms in\nreal-world data. Recent advancements in deep learning, specifically diffusion\nmodels (DMs), have sparked significant interest due to their ability to learn\ncomplex data distributions and generate high-fidelity samples, offering a\nrobust framework for unsupervised AD. In this survey, we comprehensively review\nanomaly detection and generation with diffusion models (ADGDM), presenting a\ntutorial-style analysis of the theoretical foundations and practical\nimplementations and spanning images, videos, time series, tabular, and\nmultimodal data. Crucially, unlike existing surveys that often treat anomaly\ndetection and generation as separate problems, we highlight their inherent\nsynergistic relationship. We reveal how DMs enable a reinforcing cycle where\ngeneration techniques directly address the fundamental challenge of anomaly\ndata scarcity, while detection methods provide critical feedback to improve\ngeneration fidelity and relevance, advancing both capabilities beyond their\nindividual potential. A detailed taxonomy categorizes ADGDM methods based on\nanomaly scoring mechanisms, conditioning strategies, and architectural designs,\nanalyzing their strengths and limitations. We final discuss key challenges\nincluding scalability and computational efficiency, and outline promising\nfuture directions such as efficient architectures, conditioning strategies, and\nintegration with foundation models (e.g., visual-language models and large\nlanguage models). By synthesizing recent advances and outlining open research\nquestions, this survey aims to guide researchers and practitioners in\nleveraging DMs for innovative AD solutions across diverse applications.", "authors": ["Yang Liu", "Jing Liu", "Chengfang Li", "Rui Xi", "Wenchao Li", "Liang Cao", "Jin Wang", "Laurence T. Yang", "Junsong Yuan", "Wei Zhou"], "published_date": "2025-06-11", "timestamp": "2025-06-15T12:49:54.753539", "title_zh": "基於擴散模型的異常檢測與生成：一篇綜述", "summary_zh": "本研究綜述了利用擴散模型進行異常檢測與生成的最新進展。異常檢測在網路安全、金融、醫療和工業製造等領域至關重要。擴散模型因其學習複雜數據分佈和生成高品質樣本的能力，為非監督式異常檢測提供了一個強大的框架。本綜述深入探討了擴散模型在圖像、影片、時間序列等多種數據類型上的應用，強調了異常檢測與生成之間內在的協同關係。通過詳細的分類，分析了不同方法的優缺點，並討論了可擴展性和計算效率等關鍵挑戰，最後展望了高效架構、條件策略以及與基礎模型的整合等未來方向。", "applications": ["想像一下，醫院可以利用這項技術，透過分析病人的生理數據，及早發現潛在的疾病風險，例如心律不整或癌症徵兆，讓我們能及時就醫，遠離病痛。", "工廠可以利用它來監控生產線上的設備，預測機器故障，減少停機時間，保證生產效率，這樣我們買到的產品就不會缺貨，品質也更有保障。", "銀行可以利用它來偵測異常的金融交易，防止信用卡盜刷或洗錢等犯罪行為，保護我們的財產安全，讓我們可以安心地使用金融服務。"], "pitch": "各位投資人，我們正在開發一項革命性的異常檢測技術，它基於最先進的擴散模型，能以前所未有的精度和效率，在海量數據中發現隱藏的異常模式。這項技術的應用潛力巨大，從預防金融詐欺、提升醫療診斷，到優化工業生產，無所不能。更重要的是，它能自我學習，不斷進化，適應不斷變化的數據環境。想像一下，未來每個智慧城市、每個醫療機構、每個金融機構，都需要這樣一個智慧的『守護神』。我們相信，這項技術將引領下一代人工智慧的發展，創造數十億美元的市場價值。現在加入我們，一起打造這個改變世界的技術吧！", "audio": "docs/data/audios/2506.09368v1.wav"}
{"query": "Diffusion Model", "id": "2506.10502v1", "url": "http://arxiv.org/abs/2506.10502v1", "title": "A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks", "summary": "We present a novel attack specifically designed against Tree-Ring, a\nwatermarking technique for diffusion models known for its high imperceptibility\nand robustness against removal attacks. Unlike previous removal attacks, which\nrely on strong assumptions about attacker capabilities, our attack only\nrequires access to the variational autoencoder that was used to train the\ntarget diffusion model, a component that is often publicly available. By\nleveraging this variational autoencoder, the attacker can approximate the\nmodel's intermediate latent space, enabling more effective surrogate-based\nattacks. Our evaluation shows that this approach leads to a dramatic reduction\nin the AUC of Tree-Ring detector's ROC and PR curves, decreasing from 0.993 to\n0.153 and from 0.994 to 0.385, respectively, while maintaining high image\nquality. Notably, our attacks outperform existing methods that assume full\naccess to the diffusion model. These findings highlight the risk of reusing\npublic autoencoders to train diffusion models -- a threat not considered by\ncurrent industry practices. Furthermore, the results suggest that the Tree-Ring\ndetector's precision, a metric that has been overlooked by previous\nevaluations, falls short of the requirements for real-world deployment.", "authors": ["Junhua Lin", "Marc Juarez"], "published_date": "2025-06-12", "timestamp": "2025-06-15T12:51:28.697900", "title_zh": "樹皮上的裂縫：利用公共知識移除樹環水印", "summary_zh": "這項研究揭露了一種針對擴散模型水印技術「樹環」的新型攻擊方式。「樹環」水印以其高度隱蔽性和抗移除攻擊的強韌性著稱。與以往依賴強大攻擊者假設的移除攻擊不同，此攻擊僅需存取用於訓練目標擴散模型的變分自編碼器，而這通常是公開可用的。透過利用這個變分自編碼器，攻擊者可以近似模型的中間潛在空間，從而實現更有效的代理基礎攻擊。評估結果顯示，此方法顯著降低了樹環偵測器ROC和PR曲線的AUC值，同時保持了高圖像品質。研究結果強調了重用公共自編碼器來訓練擴散模型的風險，這是一個當前業界實務未曾考慮的威脅。此外，結果表明樹環檢測器的精確度不足以滿足實際部署的要求。", "applications": ["數位藝術品防偽：藝術家可以利用這項研究的發現，更了解現有水印技術的弱點，並開發更強大的防偽機制，保護自己的數位創作不被盜用或未經授權的複製。", "照片來源追蹤：新聞媒體或攝影記者可以利用此研究，評估其照片水印技術的安全性，並確保照片的來源在未經授權使用時仍能被準確追蹤，維護新聞報導的真實性。", "機密文件保護：企業或政府機構可以利用這項研究來檢測並移除機密文件中可能隱藏的水印，防止敏感資訊洩露，並確保資訊安全。"], "pitch": "各位創投夥伴，想像一下，當AI生成的圖像越來越逼真，真假難辨，我們如何保護原創者的權益？這項研究揭示了現有水印技術的漏洞，為我們敲響了警鐘。但危機也是轉機！我們團隊將基於此研究，開發下一代更安全、更可靠的水印技術，不僅能有效抵抗現有的攻擊，更能預測和防禦未來的潛在威脅。我們的目標是打造一個堅不可摧的數位版權保護系統，應用範圍涵蓋數位藝術品、照片、影片，甚至機密文件。這不僅是一個技術問題，更是一個法律和商業問題。隨著AI內容創作的爆炸性增長，數位版權保護市場將迎來前所未有的機遇。我們相信，我們的技術將成為這個新興市場的基石，為原創者保駕護航，並創造巨大的商業價值。現在加入我們，共同打造一個更安全、更公平的數位世界！", "audio": "docs/data/audios/2506.10502v1.wav"}
{"query": "AI", "id": "2506.10600v1", "url": "http://arxiv.org/abs/2506.10600v1", "title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence", "summary": "Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and limited realism. These limitations\nsignificantly hinder the scalability of data driven approaches. We present\nEmbodiedGen, a foundational platform for interactive 3D world generation. It\nenables the scalable generation of high-quality, controllable and\nphotorealistic 3D assets with accurate physical properties and real-world scale\nin the Unified Robotics Description Format (URDF) at low cost. These assets can\nbe directly imported into various physics simulation engines for fine-grained\nphysical control, supporting downstream tasks in training and evaluation.\nEmbodiedGen is an easy-to-use, full-featured toolkit composed of six key\nmodules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object\nGeneration, Scene Generation and Layout Generation. EmbodiedGen generates\ndiverse and interactive 3D worlds composed of generative 3D assets, leveraging\ngenerative AI to address the challenges of generalization and evaluation to the\nneeds of embodied intelligence related research. Code is available at\nhttps://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.", "authors": ["Wang Xinjie", "Liu Liu", "Cao Yu", "Wu Ruiqi", "Qin Wenkang", "Wang Dehui", "Sui Wei", "Su Zhizhong"], "published_date": "2025-06-12", "timestamp": "2025-06-15T15:25:21.573110", "title_zh": "具身生成：邁向具身智慧的生成式3D世界引擎", "summary_zh": "EmbodiedGen是一個創新的3D世界生成平台，旨在解決具身智慧研究中，高品質、可控、且具備真實物理特性的3D資產生成問題。它透過Image-to-3D、Text-to-3D等模組，以低成本生成符合URDF格式、具備真實比例和物理特性的3D模型，能直接導入物理模擬引擎進行精細控制。EmbodiedGen利用生成式AI，創建多樣化且可互動的3D世界，大幅降低3D資產的製作成本，並提升具身AI在泛化和評估方面的能力，加速相關研究的發展。", "applications": ["想像一下，你可以用手機拍張照片，EmbodiedGen就能立刻生成一個3D模型，讓你直接在家裡模擬擺放新家具，看看是否合適。", "遊戲開發者可以利用EmbodiedGen快速生成各種不同的3D場景和道具，大幅縮短遊戲開發時間，降低開發成本。", "教育機構可以利用EmbodiedGen創建各種互動式的3D學習環境，例如模擬手術室、工廠車間等，讓學生在安全且低成本的環境下進行實踐學習。"], "pitch": "各位投資人，我們正在打造一個革命性的3D世界生成引擎——EmbodiedGen。它不僅能大幅降低3D內容的生產成本，更將徹底改變具身智慧、遊戲開發、教育訓練等產業。想像一下，未來的機器人可以透過EmbodiedGen生成的無數虛擬環境進行學習，快速適應真實世界；AR/VR應用將擁有取之不盡、用之不竭的3D內容，帶來前所未有的沉浸式體驗；工業設計師可以利用EmbodiedGen快速迭代產品設計，大幅縮短上市時間。EmbodiedGen不僅是一個技術平台，更是一個連接虛擬與現實的橋樑，擁有巨大的商業潛力。我們相信，EmbodiedGen將引領下一代AI和3D技術的發展，成為元宇宙時代的基礎設施。現在加入我們，共同開創這個充滿想像力的未來！", "audio": "docs/data/audios/2506.10600v1.wav"}
{"query": "Foundation Model", "id": "2506.09349v1", "url": "http://arxiv.org/abs/2506.09349v1", "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "published_date": "2025-06-11", "timestamp": "2025-06-15T15:26:43.500410", "title_zh": "OmniDRCA：基於雙重解析度語音表示和對比對齊的平行語音-文本基礎模型", "summary_zh": "OmniDRCA是一種新型的平行語音-文本基礎模型，它能同時處理語音和文字，並透過對比對齊增強對語音的理解。不同於以往的模型，OmniDRCA採用聯合自迴歸建模，讓語音合成和文字生成彼此感知。實驗結果顯示，OmniDRCA在口語問答基準測試中，於平行聯合語音-文本建模方面達到最先進的性能，甚至可與交錯模型相媲美。未來，OmniDRCA有潛力擴展到全雙工對話情境，提供更自然、更流暢的人機互動體驗。", "applications": ["**智能客服：** OmniDRCA可以讓客服機器人更自然地理解客戶的語音提問，並即時生成語音回答，提供更高效、更人性化的服務。", "**語音助理：** OmniDRCA可以應用於語音助理，讓使用者可以用更自然的方式與助理互動，例如，同時語音輸入指令和文字補充說明，助理可以更精確地理解使用者的意圖。", "**即時翻譯：** OmniDRCA可以實現更快速、更準確的即時語音翻譯，讓不同語言的人們可以更流暢地進行交流，打破語言障礙。"], "pitch": "各位創投先進，我們帶來的是OmniDRCA，一個劃時代的平行語音-文本基礎模型，它不只是技術突破，更是人機互動的未來！想像一下，未來所有需要語音和文字協同工作的場景，OmniDRCA都能提供最強大的引擎。智能客服不再生硬，語音助理更懂你心，即時翻譯打破語言藩籬，這些都只是OmniDRCA的起點！我們預見，OmniDRCA將成為下一代人機交互的基石，催生出無數創新應用，從智能家居到遠程醫療，從教育到娛樂，市場潛力無可限量！現在加入我們，共同打造這個語音和文字無縫融合的未來，一起瓜分這塊巨大的市場大餅！", "audio": "docs/data/audios/2506.09349v1.wav"}
{"query": "Diffusion Model", "id": "2506.10433v1", "url": "http://arxiv.org/abs/2506.10433v1", "title": "Measuring Semantic Information Production in Generative Diffusion Models", "summary": "It is well known that semantic and structural features of the generated\nimages emerge at different times during the reverse dynamics of diffusion, a\nphenomenon that has been connected to physical phase transitions in magnets and\nother materials. In this paper, we introduce a general information-theoretic\napproach to measure when these class-semantic \"decisions\" are made during the\ngenerative process. By using an online formula for the optimal Bayesian\nclassifier, we estimate the conditional entropy of the class label given the\nnoisy state. We then determine the time intervals corresponding to the highest\ninformation transfer between noisy states and class labels using the time\nderivative of the conditional entropy. We demonstrate our method on\none-dimensional Gaussian mixture models and on DDPM models trained on the\nCIFAR10 dataset. As expected, we find that the semantic information transfer is\nhighest in the intermediate stages of diffusion while vanishing during the\nfinal stages. However, we found sizable differences between the entropy rate\nprofiles of different classes, suggesting that different \"semantic decisions\"\nare located at different intermediate times.", "authors": ["Florian Handke", "Félix Koulischer", "Gabriel Raya", "Luca Ambrogioni"], "published_date": "2025-06-12", "timestamp": "2025-06-15T15:28:00.545580", "title_zh": "生成式擴散模型中語義信息產生的測量", "summary_zh": "本研究提出一種資訊理論方法，用以衡量生成式擴散模型在生成圖像過程中，不同類別語義「決策」產生的時間點。透過估算帶噪狀態下類別標籤的條件熵，我們能確定噪聲狀態與類別標籤之間信息傳輸量最大的時間區間。實驗結果顯示，語義信息傳輸在擴散過程的中間階段達到高峰，並在最後階段消失。不同類別的熵率曲線存在顯著差異，表明不同的「語義決策」發生在不同的中間時間點。這項技術有助於更深入地理解擴散模型的運作機制，並為改進圖像生成品質提供方向。", "applications": ["AI藝術品生成：想讓AI畫一隻貓，但希望牠的毛色、姿勢更符合你的想像？這項技術能精準控制AI在繪圖過程中，何時決定貓咪的特徵，讓你客製化獨一無二的藝術品。", "醫療影像分析：醫生想利用AI診斷X光片，判斷是否有腫瘤？這項技術能幫助AI更有效率地學習不同疾病的特徵，加速診斷速度，提高準確性。", "產品設計：設計師想用AI設計一款新手機，但希望在特定階段加入某些設計元素？這項技術能讓AI在設計過程中，於特定時間點「思考」並整合這些元素，創造出更符合需求的產品。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，能精準控制生成式AI的思考過程！想像一下，我們不再只是被動地接受AI生成的結果，而是能主動引導AI的創作方向。這項技術能應用於無數領域：從個人化的AI藝術創作、到精準的醫療影像分析、再到客製化的產品設計，市場潛力無可限量！更令人興奮的是，這項技術是通往「可解釋AI」的關鍵一步，讓我們更了解AI的運作方式，進而開發出更安全、更可靠的AI系統。我們相信，這項技術將引領下一波AI浪潮，成為AI領域的基礎建設。現在加入我們，一起打造AI的未來！", "audio": "docs/data/audios/2506.10433v1.wav"}
{"query": "AI", "id": "2506.10585v1", "url": "http://arxiv.org/abs/2506.10585v1", "title": "Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning", "summary": "This paper introduces the Primender sequence, a novel integer sequence\ndefined by a hybrid rule that combines classical primality with modular\ndigit-based conditions. Specifically, a number n is included in the sequence if\nit is prime or ends with a prime number of unit digit or any length. In other\nwords, numbers which are primes or have at least one prime suffix. The\nresulting sequence exhibits a deterministic yet non-trivial structure, blending\nnumber-theoretic properties with symbolic patterning. We propose the Primender\nsequence as a benchmark for evaluating the symbolic reasoning capabilities of\nLarge Language Models (LLMs). The study is motivated by the need for\ninterpretable, rule-based testbeds that can assess an LLM's ability to infer\nhidden rules, validate mathematical hypotheses, and generalize symbolic logic\nat scale. A key hypothesis explored is: Whenever a number in the Primender\nsequence is exactly one more than the largest prime less than or equal to it,\nthe difference between it and the previous number in the sequence is also 1. We\ndesign a structured prompt and evaluation framework to test this hypothesis\nacross multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,\nGemini, Grok, and LLaMA. The models are tasked with identifying the underlying\nrule, validating the hypothesis, and generating the next 100,000 terms of the\nsequence. Comparative metrics such as rule inference accuracy, hypothesis\nevaluation, sequence validity, and symbolic explanation quality are used to\nassess model performance. This work contributes a novel mathematical construct\nand a reproducible methodology for benchmarking LLMs in symbolic reasoning,\nhypothesis testing, and scalable pattern generalization - bridging the domains\nof number theory, artificial intelligence, and software engineering.", "authors": ["Mohd Anwar Jamal Faiz"], "published_date": "2025-06-12", "timestamp": "2025-06-15T18:34:10.794188", "title_zh": "質數尾數序列：一種用於測試符號推論與AI推理的新穎數學結構", "summary_zh": "本研究提出一種名為「質數尾數序列」的新穎整數序列，其定義結合了傳統質數判斷和基於模數的數字條件。一個數字若為質數，或其尾數包含質數，則被納入此序列。此序列展現出確定性但又非顯而易見的結構，融合了數論性質與符號模式。我們建議將此序列作為評估大型語言模型（LLM）符號推理能力的基準。研究旨在建立可解釋、基於規則的測試平台，以評估LLM推斷隱藏規則、驗證數學假設以及大規模推廣符號邏輯的能力。我們設計結構化提示和評估框架，測試多個LLM，並使用多項指標評估模型性能。本研究貢獻了一種新穎的數學結構和可重現的方法，用於基準測試LLM在符號推理、假設檢驗和可擴展模式泛化方面的能力。", "applications": ["**密碼學應用：** 質數尾數序列的獨特性質，可用於產生更複雜、難以破解的密碼。就像我們在玩「密碼鎖」，如果鎖的規則不是單純的數字順序，而是結合了質數和尾數的規則，那破解難度就大大提升了！", "**資料壓縮：** 序列的特殊結構或許能用於資料壓縮演算法，找出資料中的隱藏模式，進而更有效地壓縮檔案。想像一下，就像把一堆雜亂無章的衣服整理成整齊的衣櫃，找到衣服之間的關聯性，就能節省更多空間。", "**AI藝術創作：** 將質數尾數序列作為生成藝術的基礎，創造出獨特且具有數學美感的視覺或聽覺作品。就像音樂家利用數學公式創作樂曲，AI可以利用這個序列創造出獨一無二的藝術品。"], "pitch": "各位投資人，我們帶來一個劃時代的數學概念——「質數尾數序列」。這不僅僅是一個序列，它是AI推理能力的試金石，是未來人工智慧發展的關鍵！想像一下，如果我們能讓AI像數學家一樣思考，像藝術家一樣創造，那將釋放出多麼巨大的潛力？\n\n我們的技術能精準評估AI的邏輯推理能力，讓AI不再只是「背誦」知識，而是真正理解並運用知識。這將徹底改變AI在金融建模、醫療診斷、科學研究等領域的應用。更重要的是，這個序列的獨特性，為密碼學和資料壓縮帶來了革命性的機會，潛在市場價值數十億美元！\n\n我們正在打造一個AI推理的黃金標準，一個AI創新的加速器。現在加入我們，您將站在AI革命的最前沿，共同開創一個由智慧驅動的未來！", "audio": "docs/data/audios/2506.10585v1.wav"}
{"query": "Foundation Model", "id": "2506.09284v1", "url": "http://arxiv.org/abs/2506.09284v1", "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation", "summary": "Understanding fine-grained object affordances is imperative for robots to\nmanipulate objects in unstructured environments given open-ended task\ninstructions. However, existing methods of visual affordance predictions often\nrely on manually annotated data or conditions only on a predefined set of\ntasks. We introduce UAD (Unsupervised Affordance Distillation), a method for\ndistilling affordance knowledge from foundation models into a task-conditioned\naffordance model without any manual annotations. By leveraging the\ncomplementary strengths of large vision models and vision-language models, UAD\nautomatically annotates a large-scale dataset with detailed $<$instruction,\nvisual affordance$>$ pairs. Training only a lightweight task-conditioned\ndecoder atop frozen features, UAD exhibits notable generalization to\nin-the-wild robotic scenes and to various human activities, despite only being\ntrained on rendered objects in simulation. Using affordance provided by UAD as\nthe observation space, we show an imitation learning policy that demonstrates\npromising generalization to unseen object instances, object categories, and\neven variations in task instructions after training on as few as 10\ndemonstrations. Project website: https://unsup-affordance.github.io/", "authors": ["Yihe Tang", "Wenlong Huang", "Yingke Wang", "Chengshu Li", "Roy Yuan", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei"], "published_date": "2025-06-10", "timestamp": "2025-06-15T18:35:52.924369", "title_zh": "UAD：用於機器人操作泛化的無監督可供性蒸餾", "summary_zh": "這項研究提出了一種名為UAD的無監督可供性蒸餾方法，旨在提升機器人在非結構化環境中操作物體的泛化能力。UAD利用大型視覺模型和視覺語言模型的優勢，自動生成大規模的<指令, 視覺可供性>配對數據集，無需人工標註。透過在凍結特徵之上訓練一個輕量級的任務條件解碼器，UAD展現了對真實世界機器人場景和各種人類活動的顯著泛化能力，即使僅在模擬渲染對象上進行訓練。研究表明，使用UAD提供的可供性作為觀察空間，模仿學習策略可以在僅經過少量演示（10次）的訓練後，對未見過的物體實例、物體類別，甚至任務指令的變化展現出良好的泛化能力。", "applications": ["**智慧家庭助手：** 想像一下，你的機器人管家能理解「把蘋果放進冰箱」這樣的指令，並自動找到蘋果、打開冰箱、並將蘋果放入。UAD技術讓機器人能理解不同物體的可操作性，不再只是執行預先設定好的動作。", "**自動化倉儲管理：** 在大型倉庫中，機器人可以根據指令揀選不同種類的貨物。UAD技術能讓機器人辨識貨物的最佳抓取方式，提高揀選效率，降低損壞率。", "**手術輔助機器人：** 手術機器人可以透過UAD技術，更精準地理解手術器械與人體組織的互動方式，輔助醫生進行更複雜、更精細的手術操作，減少手術風險。"], "pitch": "各位投資人，我們正在打造下一代機器人操作系統，核心技術是UAD——一種無監督可供性蒸餾方法。這項技術讓機器人具備了前所未有的泛化能力，能夠像人類一樣理解物體的用途和操作方式。想想看，現在的工廠、倉庫、甚至是醫院，都需要大量的人力來處理重複性的操作任務。我們的UAD技術，可以讓機器人接管這些工作，大幅降低成本，提升效率。更重要的是，UAD技術的潛力遠不止於此。未來，我們將把UAD整合到各種消費級機器人產品中，例如智慧家庭助手、陪伴型機器人等等。想像一下，一個能夠理解你的指令、並能自主完成各種家務的機器人，市場潛力將是巨大的。我們相信，UAD技術將引領機器人產業進入一個全新的時代，而現在就是加入我們的最佳時機！", "audio": "docs/data/audios/2506.09284v1.wav"}
{"query": "Diffusion Model", "id": "2506.10391v1", "url": "http://arxiv.org/abs/2506.10391v1", "title": "ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion", "summary": "Accurate reconstruction of ocean is essential for reflecting global climate\ndynamics and supporting marine meteorological research. Conventional methods\nface challenges due to sparse data, algorithmic complexity, and high\ncomputational costs, while increasing usage of machine learning (ML) method\nremains limited to reconstruction problems at the sea surface and local\nregions, struggling with issues like cloud occlusion. To address these\nlimitations, this paper proposes ReconMOST, a data-driven guided diffusion\nmodel framework for multi-layer sea temperature reconstruction. Specifically,\nwe first pre-train an unconditional diffusion model using a large collection of\nhistorical numerical simulation data, enabling the model to attain physically\nconsistent distribution patterns of ocean temperature fields. During the\ngeneration phase, sparse yet high-accuracy in-situ observational data are\nutilized as guidance points for the reverse diffusion process, generating\naccurate reconstruction results. Importantly, in regions lacking direct\nobservational data, the physically consistent spatial distribution patterns\nlearned during pre-training enable implicitly guided and physically plausible\nreconstructions. Our method extends ML-based SST reconstruction to a global,\nmulti-layer setting, handling over 92.5% missing data while maintaining\nreconstruction accuracy, spatial resolution, and superior generalization\ncapability. We pre-train our model on CMIP6 numerical simulation data and\nconduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The\nresults of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on\nreconstruction, and 0.633 on total, respectively, demonstrating the\neffectiveness and robustness of the proposed framework. Our source code is\navailable at https://github.com/norsheep/ReconMOST.", "authors": ["Yuanyi Song", "Pumeng Lyu", "Ben Fei", "Fenghua Ling", "Wanli Ouyang", "Lei Bai"], "published_date": "2025-06-12", "timestamp": "2025-06-15T18:37:30.263459", "title_zh": "ReconMOST：基於觀測引導擴散的多層海洋溫度重建", "summary_zh": "ReconMOST 是一個數據驅動的擴散模型框架，用於多層海洋溫度重建。它利用大量歷史數值模擬數據預訓練一個無條件擴散模型，使其學習海洋溫度場的物理一致性分佈模式。在生成階段，使用稀疏但高精度的實地觀測數據作為引導點，進行反向擴散過程，從而生成準確的重建結果。即使在缺乏直接觀測數據的區域，模型也能依賴預訓練學習到的物理一致性空間分佈模式，進行隱式引導和物理上合理的重建。該方法將基於機器學習的海面溫度重建擴展到全球、多層的環境，處理超過 92.5% 的缺失數據，同時保持重建精度、空間分辨率和卓越的泛化能力。實驗結果表明，該框架有效且穩健。", "applications": ["漁業資源管理：漁民可以利用更精確的海洋溫度數據，預測魚群的遷徙路線，提高捕撈效率，同時避免過度捕撈。", "氣候變遷研究：科學家可以利用重建的歷史海洋溫度數據，更準確地分析氣候變遷的影響，並預測未來的氣候變化趨勢。", "航運安全：船隻可以利用實時的海洋溫度信息，避開寒流或高溫區域，優化航線，節省燃料，並提高航行安全。"], "pitch": "各位投資人，我們正處於一個氣候變遷日益嚴峻的時代，精準的海洋數據至關重要。ReconMOST 不僅僅是一個海洋溫度重建模型，它是一個解鎖海洋數據價值的鑰匙！想像一下，一個能夠精確預測全球漁業資源分佈、優化航運路線，甚至協助預測極端氣候事件的平台，它的價值何止數十億美元？\n\n我們的 ReconMOST 技術，透過創新的觀測引導擴散模型，克服了傳統方法的局限性，能夠在全球範圍內，以驚人的精度和效率重建多層海洋溫度。這意味著我們能填補現有數據的巨大空白，提供前所未有的海洋洞察力。更重要的是，ReconMOST 的預訓練模型具備極強的泛化能力，能夠適應不同的數據源和環境變化，確保長期穩定可靠的數據服務。\n\n我們不僅僅提供數據，更提供基於數據的解決方案。未來，我們可以將 ReconMOST 技術應用於海洋資源管理、氣候風險評估、甚至是海洋碳封存等領域，打造一個龐大的海洋數據生態系統。這是一個具有巨大潛力的市場，而 ReconMOST 將成為這個市場的領導者。現在加入我們，一起開創海洋數據的新時代，共同實現經濟效益和環境保護的雙贏！", "audio": "docs/data/audios/2506.10391v1.wav"}
{"query": "AI", "id": "2506.10570v1", "url": "http://arxiv.org/abs/2506.10570v1", "title": "6G Infrastructures for Edge AI: An Analytical Perspective", "summary": "The convergence of Artificial Intelligence (AI) and the Internet of Things\nhas accelerated the development of distributed, network-sensitive applications,\nnecessitating ultra-low latency, high throughput, and real-time processing\ncapabilities. While 5G networks represent a significant technological\nmilestone, their ability to support AI-driven edge applications remains\nconstrained by performance gaps observed in real-world deployments. This paper\naddresses these limitations and highlights critical advancements needed to\nrealize a robust and scalable 6G ecosystem optimized for AI applications.\nFurthermore, we conduct an empirical evaluation of 5G network infrastructure in\ncentral Europe, with latency measurements ranging from 61 ms to 110 ms across\ndifferent close geographical areas. These values exceed the requirements of\nlatency-critical AI applications by approximately 270%, revealing significant\nshortcomings in current deployments. Building on these findings, we propose a\nset of recommendations to bridge the gap between existing 5G performance and\nthe requirements of next-generation AI applications.", "authors": ["Kurt Horvath", "Shpresa Tuda", "Blerta Idrizi", "Stojan Kitanov", "Fisnik Doko", "Dragi Kimovski"], "published_date": "2025-06-12", "timestamp": "2025-06-15T21:23:33.025395", "title_zh": "邊緣人工智慧的6G基礎設施：一個分析性的觀點", "summary_zh": "隨著人工智慧和物聯網的融合，對超低延遲、高吞吐量和即時處理的需求日益增加。雖然5G網絡取得了顯著進展，但在實際部署中，其對AI驅動的邊緣應用程式的支援仍存在性能瓶頸。本研究探討了這些限制，並強調了實現為AI應用程式優化的強大且可擴展的6G生態系統所需的關鍵進展。我們在歐洲中部對5G網絡基礎設施進行了實證評估，發現延遲遠遠超出延遲敏感型AI應用程式的需求。基於這些發現，我們提出了一系列建議，以彌合現有5G性能與下一代AI應用程式需求之間的差距。", "applications": ["**智慧交通：**想像一下，未來的自駕車可以透過6G即時分析路況、預測行人動向，避免事故發生，讓交通更安全、更順暢。這就像擁有一位超級聰明的駕駛員，隨時保護你的安全。", "**遠程醫療：**醫生可以利用6G進行遠程手術，即使身在偏遠地區，也能獲得頂尖專家的協助。高解析度的影像和零延遲的操作，讓手術如同親臨現場，拯救更多生命。", "**智慧工廠：**工廠裡的機器人透過6G協同工作，即時調整生產流程，大幅提高生產效率和產品品質。這就像擁有一個超級智能的生產線，可以根據市場需求靈活調整。"], "pitch": "各位投資人，我們正在打造的是未來AI的基石！現有的5G網路已無法滿足AI爆炸性成長的需求，尤其是在需要極低延遲的應用上。我們的6G基礎設施方案，將徹底解放AI的潛力，讓AI真正落地到各行各業。試想一下，一個由AI驅動的智慧世界：自駕車減少事故、遠程醫療拯救生命、智慧工廠提高效率，這一切都建立在我們提供的超高速、超低延遲的6G網路上。這不僅僅是技術升級，更是一場產業革命！我們預計，隨著AI應用的普及，對6G的需求將呈現指數級增長，而我們將成為這場革命的領跑者，坐擁巨大的市場潛力。現在投資我們，就是投資未來！", "audio": "docs/data/audios/2506.10570v1.wav"}
{"query": "Foundation Model", "id": "2506.09167v1", "url": "http://arxiv.org/abs/2506.09167v1", "title": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry", "summary": "Visceral adipose tissue (VAT) is a key marker of both metabolic health and\nhabitual physical activity (PA). Excess VAT is highly correlated with type 2\ndiabetes and insulin resistance. The mechanistic basis for this pathophysiology\nrelates to overloading the liver with fatty acids. VAT is also a highly labile\nfat depot, with increased turnover stimulated by catecholamines during\nexercise. VAT can be measured with sophisticated imaging technologies, but can\nalso be inferred directly from PA. We tested this relationship using National\nHealth and Nutrition Examination Survey (NHANES) data from 2011-2014, for\nindividuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;\n2,427 women) [1]. Two approaches were used for estimating VAT from activity.\nThe first used engineered features based on movements during gait and sleep,\nand then ridge regression to map summary statistics of these features into a\nVAT estimate. The second approach used deep neural networks trained on 24 hours\nof continuous accelerometry. A foundation model first mapped each 10s frame\ninto a high-dimensional feature vector. A transformer model then mapped each\nday's feature vector time series into a VAT estimate, which were averaged over\nmultiple days. For both approaches, the most accurate estimates were obtained\nwith the addition of covariate information about subject demographics and body\nmeasurements. The best performance was obtained by combining the two\napproaches, resulting in VAT estimates with correlations of r=0.86. These\nfindings demonstrate a strong relationship between PA and VAT and, by\nextension, between PA and metabolic health risks.", "authors": ["James R. Williamson", "Andrew Alini", "Brian A. Telfer", "Adam W. Potter", "Karl E. Friedl"], "published_date": "2025-06-10", "timestamp": "2025-06-15T21:24:37.133766", "title_zh": "從腕戴式加速計估算內臟脂肪", "summary_zh": "本研究利用腕戴式加速計數據，結合機器學習模型，成功估算內臟脂肪(VAT)含量。VAT是評估代謝健康的重要指標，過高與第二型糖尿病和胰島素阻抗密切相關。研究團隊使用NHANES數據，開發了兩種方法：一是基於步態和睡眠的特徵工程，二是基於深度神經網路直接分析24小時的加速計數據。結合人口統計學和身體測量數據後，兩種方法結合可達到r=0.86的相關性。這項研究證明了身體活動與內臟脂肪之間存在強烈的關聯性，進而反映了身體活動與代謝健康風險之間的關係。未來可應用於個人化健康管理和疾病預防。", "applications": ["減肥健身App：結合手環數據，更精準地顯示燃燒的內臟脂肪量，激勵使用者運動。", "企業員工健康計畫：公司提供手環，追蹤員工的活動量，並估算內臟脂肪，提供個人化的健康建議。", "遠距醫療：醫生透過患者的手環數據，遠端監測內臟脂肪變化，調整治療方案，特別適合慢性病患者。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，能透過簡單的腕戴式裝置，精準估算內臟脂肪含量。想像一下，不再需要昂貴的醫療掃描，就能隨時掌握自己的代謝健康狀況。這項技術的潛力無窮，能應用於個人健康管理、運動健身、遠距醫療等領域。隨著人們對健康意識的提高，以及預防醫學的興起，市場對這類產品的需求將會爆發性成長。我們已經建立了穩固的技術基礎，並擁有一支經驗豐富的團隊。我們相信，這項技術將能為投資者帶來豐厚的回報，並為人類的健康做出巨大貢獻。現在加入我們，一起打造更健康、更美好的未來！", "audio": "docs/data/audios/2506.09167v1.wav"}
{"query": "Diffusion Model", "id": "2506.10233v1", "url": "http://arxiv.org/abs/2506.10233v1", "title": "Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization", "summary": "Supervised machine learning has enabled accurate pathology detection in brain\nMRI, but requires training data from diseased subjects that may not be readily\navailable in some scenarios, for example, in the case of rare diseases.\nReconstruction-based unsupervised anomaly detection, in particular using\ndiffusion models, has gained popularity in the medical field as it allows for\ntraining on healthy images alone, eliminating the need for large\ndisease-specific cohorts. These methods assume that a model trained on normal\ndata cannot accurately represent or reconstruct anomalies. However, this\nassumption often fails with models failing to reconstruct healthy tissue or\naccurately reconstruct abnormal regions i.e., failing to remove anomalies. In\nthis work, we introduce a novel conditional diffusion model framework for\nanomaly detection and healthy image reconstruction in brain MRI. Our weakly\nsupervised approach integrates synthetically generated pseudo-pathology images\ninto the modeling process to better guide the reconstruction of healthy images.\nTo generate these pseudo-pathologies, we apply fluid-driven anomaly\nrandomization to augment real pathology segmentation maps from an auxiliary\ndataset, ensuring that the synthetic anomalies are both realistic and\nanatomically coherent. We evaluate our model's ability to detect pathology,\nusing both synthetic anomaly datasets and real pathology from the ATLAS\ndataset. In our extensive experiments, our model: (i) consistently outperforms\nvariational autoencoders, and conditional and unconditional latent diffusion;\nand (ii) surpasses on most datasets, the performance of supervised inpainting\nmethods with access to paired diseased/healthy images.", "authors": ["Ana Lawry Aguila", "Peirong Liu", "Oula Puonti", "Juan Eugenio Iglesias"], "published_date": "2025-06-11", "timestamp": "2025-06-15T21:26:01.160981", "title_zh": "使用流體驅動異常隨機化的條件擴散模型，用於引導腦部影像中的異常檢測", "summary_zh": "本研究提出一種新穎的條件擴散模型框架，用於腦部MRI的異常檢測與健康影像重建。有別於傳統方法僅使用健康影像訓練，我們將合成產生的偽病理影像融入建模過程，以更好地引導健康影像的重建。我們利用流體驅動異常隨機化技術，擴增輔助數據集中的真實病理分割圖，確保合成的異常既真實又符合解剖學結構。實驗結果顯示，我們的模型在檢測病理方面，不僅優於變分自編碼器和條件/非條件潛在擴散模型，在多數數據集上，也超越了能取得配對病變/健康影像的監督式影像修復方法。這項技術有效提升腦部MRI異常檢測的準確性，尤其在罕見疾病數據不足的情況下更顯價值。", "applications": ["**早期阿茲海默症篩檢：** 透過分析腦部MRI影像，及早發現細微的腦部異常，幫助高風險族群進行早期診斷與介入，延緩病情惡化。", "**腦部腫瘤術前規劃：** 精準定位腫瘤位置與範圍，協助醫生擬定更完善的手術計畫，降低手術風險，提高切除成功率，並保護周圍健康腦組織。", "**罕見腦部疾病診斷：** 針對數據匱乏的罕見腦部疾病，即使只有少量健康腦部影像，也能有效檢測出異常，協助醫生進行更精確的診斷，縮短確診時間。"], "pitch": "想像一下，我們正在打造一個AI腦部醫生，它能像福爾摩斯一樣，從複雜的腦部MRI影像中，找出隱藏的病灶，甚至在疾病初期、症狀還不明顯時就能預警！我們的條件擴散模型，就像是賦予這位AI腦部醫生更敏銳的觀察力，即使面對罕見疾病、數據不足的情況，也能精準判斷。這不僅能大幅提升醫療效率，更重要的是，能為患者爭取寶貴的治療時間。未來，我們可以將這項技術應用於遠程醫療、智慧醫院等場景，甚至可以開發出個人化的腦部健康監測APP，讓每個人都能隨時掌握自己的腦部健康狀況。這是一個潛力無限的市場，我們有信心能引領醫療AI的新革命，創造巨大的商業價值！", "audio": "docs/data/audios/2506.10233v1.wav"}
{"query": "AI", "id": "2506.10559v1", "url": "http://arxiv.org/abs/2506.10559v1", "title": "From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations", "summary": "Explaining why the species lives at a particular location is important for\nunderstanding ecological systems and conserving biodiversity. However, existing\necological workflows are fragmented and often inaccessible to non-specialists.\nWe propose an end-to-end visual-to-causal framework that transforms a species\nimage into interpretable causal insights about its habitat preference. The\nsystem integrates species recognition, global occurrence retrieval,\npseudo-absence sampling, and climate data extraction. We then discover causal\nstructures among environmental features and estimate their influence on species\noccurrence using modern causal inference methods. Finally, we generate\nstatistically grounded, human-readable causal explanations from structured\ntemplates and large language models. We demonstrate the framework on a bee and\na flower species and report early results as part of an ongoing project,\nshowing the potential of the multimodal AI assistant backed up by a recommended\necological modeling practice for describing species habitat in\nhuman-understandable language.", "authors": ["Yutong Zhou", "Masahiro Ryo"], "published_date": "2025-06-12", "timestamp": "2025-06-16T02:07:22.893107", "title_zh": "從圖像到洞察：利用平實語言的棲地解釋實現可解釋的生物多樣性監測", "summary_zh": "本研究提出一個端到端的視覺到因果框架，將物種圖像轉化為對其棲息地偏好的可解釋因果洞察。系統整合了物種辨識、全球發生地檢索、偽缺失抽樣和氣候數據提取。接著，利用現代因果推論方法，發現環境特徵之間的因果結構，並評估它們對物種發生的影響。最後，系統從結構化模板和大型語言模型中生成基於統計的、人類可讀的因果解釋。我們以蜜蜂和花卉物種為例，展示了該框架的潛力，這是一個由推薦的生態建模實踐支持的多模態AI助手，可以用人類易於理解的語言描述物種棲息地。", "applications": ["農民可以利用手機拍攝農田周圍的昆蟲照片，AI系統就能分析出哪些昆蟲對農作物有益，哪些有害，並提供相應的防治建議，減少農藥使用。", "登山愛好者在野外發現不認識的植物，拍照上傳後，系統不僅能辨識植物種類，還能告訴你這種植物喜歡生長在什麼樣的環境，例如海拔、濕度等，讓你更了解大自然。", "政府或保育團體可以利用無人機拍攝大範圍的棲地照片，快速分析出不同物種的分布情況，以及棲地環境的變化，以便制定更有效的保育政策。"], "pitch": "各位投資人，我們正在開發一個革命性的生物多樣性監測平台，它能將複雜的生態數據轉化為人人都能理解的資訊。想像一下，未來我們可以利用手機App，隨時隨地了解周圍環境的生態狀況，預測氣候變遷對物種的影響，甚至發現新的藥用植物！這個平台不僅能幫助政府和企業制定更明智的環境政策，還能為生態旅遊、農業等產業帶來巨大的商業價值。我們利用先進的AI技術，結合圖像辨識、因果推論和自然語言處理，打造了一個獨一無二的解決方案，具有極高的技術壁壘。我們相信，這個平台將成為未來生態保護和永續發展的重要工具，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.10559v1.wav"}
{"query": "Foundation Model", "id": "2506.09114v1", "url": "http://arxiv.org/abs/2506.09114v1", "title": "TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval", "summary": "The ubiquity of dynamic data in domains such as weather, healthcare, and\nenergy underscores a growing need for effective interpretation and retrieval of\ntime-series data. These data are inherently tied to domain-specific contexts,\nsuch as clinical notes or weather narratives, making cross-modal retrieval\nessential not only for downstream tasks but also for developing robust\ntime-series foundation models by retrieval-augmented generation (RAG). Despite\nthe increasing demand, time-series retrieval remains largely underexplored.\nExisting methods often lack semantic grounding, struggle to align heterogeneous\nmodalities, and have limited capacity for handling multi-channel signals. To\naddress this gap, we propose TRACE, a generic multimodal retriever that grounds\ntime-series embeddings in aligned textual context. TRACE enables fine-grained\nchannel-level alignment and employs hard negative mining to facilitate\nsemantically meaningful retrieval. It supports flexible cross-modal retrieval\nmodes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking\nlinguistic descriptions with complex temporal patterns. By retrieving\nsemantically relevant pairs, TRACE enriches downstream models with informative\ncontext, leading to improved predictive accuracy and interpretability. Beyond a\nstatic retrieval engine, TRACE also serves as a powerful standalone encoder,\nwith lightweight task-specific tuning that refines context-aware\nrepresentations while maintaining strong cross-modal alignment. These\nrepresentations achieve state-of-the-art performance on downstream forecasting\nand classification tasks. Extensive experiments across multiple domains\nhighlight its dual utility, as both an effective encoder for downstream\napplications and a general-purpose retriever to enhance time-series models.", "authors": ["Jialin Chen", "Ziyu Zhao", "Gaukhar Nurbek", "Aosong Feng", "Ali Maatouk", "Leandros Tassiulas", "Yifeng Gao", "Rex Ying"], "published_date": "2025-06-10", "timestamp": "2025-06-16T02:08:43.612741", "title_zh": "TRACE：基於情境的時間序列多模態嵌入與檢索", "summary_zh": "TRACE是一個創新的多模態檢索系統，它將時間序列數據與相關的文字情境連結起來，例如醫療紀錄或天氣描述。TRACE能精準對齊不同資料通道，並透過負樣本挖掘來提升檢索的語義準確性。它支援文本到時間序列以及時間序列到文本的雙向檢索，讓複雜的時間模式與語言描述能有效連結。TRACE不僅能作為獨立的編碼器，輕量調整後即可增強情境感知表示，還能提升下游預測模型的準確性和可解釋性。實驗證明，TRACE在多個領域中都表現出色，既是有效的編碼器，也是通用的檢索器，能強化時間序列模型。", "applications": ["**智慧醫療：** 想像一下，醫生可以透過病人的心電圖，快速檢索出與其相似病例的文字病歷描述，從而更準確地診斷病情並制定治療方案。這就像擁有一個龐大的、即時更新的病例資料庫，隨時提供參考。", "**精準農業：** 農民可以根據過去的天氣數據和農作物生長情況，預測未來的收成。TRACE可以將天氣數據與農作物生長報告連結起來，幫助農民更好地管理灌溉、施肥等措施，提高農作物產量。", "**智慧城市：** 城市管理者可以利用TRACE分析交通流量數據，並將其與新聞報導、社交媒體上的事件描述連結起來，從而更好地理解交通擁堵的原因，並採取相應的措施，例如調整紅綠燈時間、增加公共交通班次等。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，TRACE，它將徹底改變時間序列數據的應用方式。試想一下，在金融市場，TRACE能將股價走勢與新聞事件、財報數據關聯，提前預測市場波動，讓交易者搶佔先機。在能源產業，TRACE能將電網數據與天氣預報、設備運行狀態結合，優化能源分配，減少浪費。更令人興奮的是，TRACE作為一個基礎模型，具有極高的可擴展性，可以應用於任何涉及時間序列數據的領域。我們相信，TRACE將成為未來人工智慧發展的關鍵基礎設施，其潛在商業價值將達到數十億美元。現在加入我們，共同開創時間序列數據應用的新紀元！", "audio": "docs/data/audios/2506.09114v1.wav"}
{"query": "Diffusion Model", "id": "2506.10963v2", "url": "http://arxiv.org/abs/2506.10963v2", "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "summary": "In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning -- a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits -- low entity fidelity,\nweak relations, and clutter -- with GPT-4o achieving an MMMG-Score of only\n50.20, underscoring the benchmark's difficulty. To spur further progress, we\nrelease FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that\ncombines a reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.", "authors": ["Yuxuan Luo", "Yuhui Yuan", "Junwen Chen", "Haonan Cai", "Ziyi Yue", "Yuwei Yang", "Fatima Zohra Daha", "Ji Li", "Zhouhui Lian"], "published_date": "2025-06-12", "timestamp": "2025-06-16T02:10:00.399459", "title_zh": "MMMG：一個大規模、跨領域、多層級的文本到圖像生成推理基準", "summary_zh": "本研究提出知識圖像生成作為一項新任務，並創建了大規模、跨領域、多層級知識圖像生成基準（MMMG），以評估圖像生成模型的推理能力。知識圖像在人類文明和學習中扮演重要角色。生成這些圖像需要多模態推理，將世界知識與像素級基礎融合為清晰的解釋性視覺效果。MMMG提供涵蓋10個學科、6個教育程度和多種知識格式的4456個專家驗證的圖像-提示對。研究採用統一的知識圖譜（KG）表示，並引入MMMG-Score來評估生成的知識圖像，結合事實準確性（通過圖編輯距離測量）和視覺清晰度評估。評估結果顯示，現有模型在推理方面存在嚴重缺陷，為此，研究發布了FLUX-Reason作為基準模型。", "applications": ["教育輔助：學生可以輸入課本內容，自動生成概念圖或流程圖，幫助理解複雜知識。", "旅遊導覽：輸入景點描述，自動生成包含歷史背景、特色亮點的圖像，讓遊客更深入了解。", "食譜生成：輸入食材和料理方式，自動生成步驟圖，即使是新手也能輕鬆學會做菜。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，能將文字描述轉化為清晰易懂的知識圖像。想像一下，不再需要死記硬背，而是透過視覺化的方式快速掌握複雜概念！MMMG基準測試證明，現有AI模型在這方面仍有巨大進步空間，這正是我們的機會。我們的技術不僅能應用於教育、旅遊等領域，更能顛覆資訊傳播的方式。未來，我們可以將這項技術整合到各個產業，例如醫療、工程、金融，甚至軍事領域，創造出巨大的商業價值。我們相信，這項技術將成為AI領域的下一個風口，現在加入，就能搶佔先機，共同打造一個更智能、更高效的未來！", "audio": "docs/data/audios/2506.10963v2.wav"}
{"query": "AI", "id": "2506.12008v1", "url": "http://arxiv.org/abs/2506.12008v1", "title": "Reimagining Dance: Real-time Music Co-creation between Dancers and AI", "summary": "Dance performance traditionally follows a unidirectional relationship where\nmovement responds to music. While AI has advanced in various creative domains,\nits application in dance has primarily focused on generating choreography from\nmusical input. We present a system that enables dancers to dynamically shape\nmusical environments through their movements. Our multi-modal architecture\ncreates a coherent musical composition by intelligently combining pre-recorded\nmusical clips in response to dance movements, establishing a bidirectional\ncreative partnership where dancers function as both performers and composers.\nThrough correlation analysis of performance data, we demonstrate emergent\ncommunication patterns between movement qualities and audio features. This\napproach reconceptualizes the role of AI in performing arts as a responsive\ncollaborator that expands possibilities for both professional dance performance\nand improvisational artistic expression across broader populations.", "authors": ["Olga Vechtomova", "Jeff Bos"], "published_date": "2025-06-13", "timestamp": "2025-06-16T03:56:41.190627", "title_zh": "重塑舞蹈：舞者與人工智慧之間的即時音樂共創", "summary_zh": "本研究提出一套系統，讓舞者能透過肢體動作即時塑造音樂環境。系統結合多模態架構，根據舞者的動作，智慧地組合預錄音樂片段，創造連貫的音樂作品，建立舞者與AI之間的雙向創意合作關係，舞者同時扮演表演者和作曲家的角色。通過對演出數據的相關性分析，我們展示了動作質量和音頻特徵之間的新興溝通模式。此方法重新定義了AI在表演藝術中的角色，使其成為一個反應迅速的協作者，擴展了專業舞蹈表演和更廣泛人群的即興藝術表達的可能性。", "applications": ["想像一下，未來在家跳舞時，音樂會隨著你的動作、情緒自動調整，讓你彷彿置身於專屬的音樂舞蹈世界，創造獨一無二的健身體驗。", "在復健中心，AI可以根據病患的動作，即時生成合適的音樂，幫助他們更輕鬆、更投入地完成復健療程，提高治療效果。", "音樂治療師可以利用這項技術，讓自閉症兒童透過肢體動作來表達情感，AI即時將其轉化為音樂，幫助他們更好地與外界溝通。"], "pitch": "各位投資人，我們正在打造一個劃時代的音樂舞蹈互動平台。想像一下，一個App讓使用者在家就能創造出獨一無二的音樂舞蹈體驗，甚至能將自己的『舞曲』分享給全世界。這不僅僅是娛樂，更是一個全新的社交模式！我們的技術不僅能應用於健身、復健，更能拓展到遊戲、教育等領域。AI將成為每個人的音樂夥伴，讓創作不再是專業人士的專利。我們預計在三年內，使用者將突破百萬，成為音樂舞蹈領域的獨角獸企業。這是一個顛覆傳統、引領未來的投資機會，讓我們一起重新定義音樂與舞蹈的界限！", "audio": "docs/data/audios/2506.12008v1.wav"}
{"query": "Foundation Model", "id": "2506.12015v1", "url": "http://arxiv.org/abs/2506.12015v1", "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction", "summary": "Open-source foundation models have seen rapid adoption and development,\nenabling powerful general-purpose capabilities across diverse domains. However,\nfine-tuning large foundation models for domain-specific or personalized tasks\nremains prohibitively expensive for most users due to the significant memory\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\nmodel fine-tuning within the same memory budget required for inference. EMLoC\nconstructs a task-specific light-weight emulator using activation-aware\nsingular value decomposition (SVD) on a small downstream calibration set.\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\nthe misalignment between the original model and the compressed emulator, we\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\nwhich thus can be merged into the original model for inference. EMLoC supports\nflexible compression ratios and standard training pipelines, making it\nadaptable to a wide range of applications. Extensive experiments demonstrate\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\nindividual users.", "authors": ["Hsi-Che Lin", "Yu-Chu Yu", "Kai-Po Chang", "Yu-Chiang Frank Wang"], "published_date": "2025-06-13", "timestamp": "2025-06-16T03:58:18.190316", "title_zh": "EMLoC：基於模擬器的記憶體效率型微調，具備LoRA校正功能", "summary_zh": "EMLoC是一個創新的微調框架，它讓使用者能在與模型推論相同的記憶體預算下，進行大型基礎模型的微調。它利用激活感知奇異值分解（SVD）在小型校準集上構建一個輕量級的任務特定模擬器，然後透過LoRA在這個模擬器上進行微調。為了解決原始模型和壓縮後的模擬器之間的不一致問題，EMLoC提出了一種新穎的補償算法來校正微調後的LoRA模組，使其能夠合併回原始模型以進行推論。EMLoC支援靈活的壓縮比率和標準訓練流程，適用於各種應用。實驗證明，EMLoC在多個數據集和模態上優於其他基準模型，且無需量化，即可在單個24GB消費級GPU上微調一個380億參數的模型，為個人用戶帶來高效且實用的模型適應能力。", "applications": ["**個人化AI助理：** 你可以想像一下，未來每個人的手機裡都有一個高度客製化的AI助理，它能根據你的閱讀習慣、聊天風格，甚至情緒反應，提供更貼心、更懂你的服務。EMLoC讓你在自己的手機上就能微調大型語言模型，打造獨一無二的AI夥伴。", "**專業領域知識庫：** 醫生、律師或工程師可以利用EMLoC，針對特定領域的文獻和案例，快速微調大型語言模型，建立一個專業知識庫。這個知識庫不僅能回答問題，還能提供專業建議，大幅提升工作效率。", "**低資源語言翻譯：** 對於一些使用人口較少、數位資源匱乏的語言，我們可以利用EMLoC，在有限的數據上微調大型語言模型，開發出高品質的翻譯工具，促進不同文化之間的交流。"], "pitch": "各位投資人，我們正處於AI爆發的時代，但大型模型的微調成本卻是阻礙其廣泛應用的關鍵瓶頸。EMLoC技術徹底顛覆了這一現狀！它讓微調成本降至與推論成本相同，這意味著什麼？意味著每個人都能擁有客製化的AI模型，各行各業都能輕鬆導入AI解決方案。試想一下，醫療領域可以利用EMLoC快速訓練針對罕見疾病的診斷模型；金融領域可以打造預測更精準的風險評估模型；教育領域可以開發個性化的學習輔導系統。EMLoC不僅僅是一項技術，更是一個全新的AI生態系統！我們預計，EMLoC將引領下一波AI創新浪潮，市場潛力無可限量。現在加入我們，共同開創AI普惠的新時代！", "audio": "docs/data/audios/2506.12015v1.wav"}
{"query": "Diffusion Model", "id": "2506.11893v1", "url": "http://arxiv.org/abs/2506.11893v1", "title": "Measurement-aligned Flow for Inverse Problem", "summary": "Diffusion models provide a powerful way to incorporate complex prior\ninformation for solving inverse problems. However, existing methods struggle to\ncorrectly incorporate guidance from conflicting signals in the prior and\nmeasurement, especially in the challenging setting of non-Gaussian or unknown\nnoise. To bridge these gaps, we propose Measurement-Aligned Sampling (MAS), a\nnovel framework for linear inverse problem solving that can more flexibly\nbalance prior and measurement information. MAS unifies and extends existing\napproaches like DDNM and DAPS, and offers a new optimization perspective. MAS\ncan generalize to handle known Gaussian noise, unknown or non-Gaussian noise\ntypes. Extensive experiments show that MAS consistently outperforms\nstate-of-the-art methods across a range of tasks.", "authors": ["Shaorong Zhang", "Rob Brekelmans", "Yunshu Wu", "Greg Ver Steeg"], "published_date": "2025-06-13", "timestamp": "2025-06-16T03:59:29.012654", "title_zh": "對齊量測的反問題流程", "summary_zh": "本研究提出一種名為「量測對齊採樣 (MAS)」的新框架，用於解決線性反問題。現有方法在處理非高斯或未知雜訊等複雜情況時，難以正確整合先驗知識和量測信號。MAS 能夠更靈活地平衡這兩者，統一並擴展了現有方法，並提供新的優化視角。它不僅能處理已知的高斯雜訊，還能應對未知或非高斯雜訊。實驗結果顯示，MAS 在多項任務中均優於現有最先進的方法，為反問題提供更強大的解決方案。", "applications": ["**更清晰的醫學影像：** 想像一下，醫生可以使用這項技術，即使在X光或核磁共振影像受到雜訊干擾的情況下，也能更清楚地看到你體內的狀況，幫助他們做出更準確的診斷。", "**更可靠的犯罪現場重建：** 警察可以利用這項技術，從模糊或不完整的監視器畫面中，重建出清晰的影像，幫助他們找到罪犯。", "**更精準的氣象預報：** 氣象學家可以利用這項技術，從有雜訊的氣象數據中，更準確地預測天氣，讓我們提前做好準備。"], "pitch": "各位創投先進，我們團隊正在開發一項革命性的技術，名為「量測對齊採樣 (MAS)」。這項技術能大幅提升反問題的解決能力，其應用範圍極廣，從醫學影像、安全監控到氣象預測，無所不在。想像一下，未來自動駕駛汽車能更準確地感知周遭環境，即使在惡劣天氣下也能安全行駛；工廠能更精準地檢測產品瑕疵，大幅降低生產成本；甚至軍事偵察也能突破重重干擾，獲得清晰情報。MAS 不僅僅是一項技術，更是一個平台，能賦能各行各業，創造巨大的商業價值。我們相信，MAS 將引領下一波AI浪潮，成為解決現實世界複雜問題的關鍵引擎。現在投資，您將站在技術的最前沿，共享無限商機！", "audio": "docs/data/audios/2506.11893v1.wav"}
