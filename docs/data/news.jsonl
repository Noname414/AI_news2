{"query": "AI", "id": "2506.06242v1", "url": "http://arxiv.org/abs/2506.06242v1", "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models", "summary": "Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}", "authors": ["Zahra Babaiee", "Peyman M. Kiasari", "Daniela Rus", "Radu Grosu"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:19:31.539877", "title_zh": "視覺圖形競技場：評估視覺和多模態大型語言模型的視覺概念化能力", "summary_zh": "近年來，多模態大型語言模型在視覺問答方面取得了顯著進展。然而，AI在「概念化」能力上仍存在差距，也就是辨識和推理相同概念，不受視覺形式變化的影響。為了解決這個問題，我們推出了視覺圖形競技場（VGA），它是一個包含六個基於圖形的任務的數據集，旨在評估和提升AI系統的視覺抽象能力。VGA使用不同的圖形佈局來測試獨立於視覺形式的推理。實驗結果顯示，人類在各項任務中幾乎達到完美準確度，而模型在同構檢測方面完全失敗，在路徑/循環任務方面表現有限，這突顯了當前AI模型在視覺理解方面的根本局限性。VGA提供了一個框架，旨在推動AI視覺模型在概念化方面取得類似人類的進展。", "applications": ["**自動駕駛：** 讓汽車能辨識不同角度或光線下的交通標誌，確保行車安全。例如，即使交通標誌被樹葉遮蔽一部分，或因光線反射而變形，汽車也能正確判斷其意義。", "**醫療影像分析：** 協助醫生辨識X光片或斷層掃描中不同形態的腫瘤，提高診斷準確性。例如，即使腫瘤形狀不規則或與周圍組織融合，AI也能準確辨識並標記。", "**智慧零售：** 讓機器人能辨識貨架上不同包裝或擺放方式的商品，提升倉儲和物流效率。例如，即使商品條碼被遮蓋，或商品被隨意堆放，機器人也能準確辨識商品種類和數量。"], "pitch": "各位投資人，我們正處於AI革命的風口浪尖！視覺圖形競技場（VGA）不僅僅是一個數據集，它是解鎖AI真正視覺理解能力的鑰匙。試想一下，一個能像人類一樣理解世界，不受視覺表象干擾的AI，它將顛覆自動駕駛、醫療診斷、智慧製造等各個領域。目前AI在概念化方面的不足，正是我們VGA的機會。我們正在打造下一代AI視覺引擎，它將超越簡單的模式匹配，真正理解圖像背後的概念。這意味著更安全可靠的自動駕駛、更精準高效的醫療診斷、以及更智能化的生產流程。我們的團隊由頂尖的AI專家組成，我們有信心將VGA打造成AI視覺領域的黃金標準。現在投資VGA，您不僅僅是投資一個數據集，更是投資一個充滿無限可能的未來！讓我們一起引領這場視覺智能的革命，共同創造一個更智能、更美好的世界！", "audio": "docs/data/audios/2506.06242v1.wav"}
{"query": "Foundation Model", "id": "2506.06281v1", "url": "http://arxiv.org/abs/2506.06281v1", "title": "TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation", "summary": "Modern Earth observation (EO) increasingly leverages deep learning to harness\nthe scale and diversity of satellite imagery across sensors and regions. While\nrecent foundation models have demonstrated promising generalization across EO\ntasks, many remain limited by the scale, geographical coverage, and spectral\ndiversity of their training data, factors critical for learning globally\ntransferable representations. In this work, we introduce TerraFM, a scalable\nself-supervised learning model that leverages globally distributed Sentinel-1\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\nsampling to enrich spatial and semantic coverage. By treating sensing\nmodalities as natural augmentations in our self-supervised approach, we unify\nradar and optical inputs via modality-specific patch embeddings and adaptive\ncross-attention fusion. Our training strategy integrates local-global\ncontrastive learning and introduces a dual-centering mechanism that\nincorporates class-frequency-aware regularization to address long-tailed\ndistributions in land cover.TerraFM achieves strong generalization on both\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\nhttps://github.com/mbzuai-oryx/TerraFM .", "authors": ["Muhammad Sohail Danish", "Muhammad Akhtar Munir", "Syed Roshaan Ali Shah", "Muhammad Haris Khan", "Rao Muhammad Anwer", "Jorma Laaksonen", "Fahad Shahbaz Khan", "Salman Khan"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:20:54.025845", "title_zh": "TerraFM：適用於統一多感測器地球觀測的可擴展基礎模型", "summary_zh": "TerraFM是一個利用深度學習技術，結合Sentinel-1和Sentinel-2衛星影像的可擴展自監督學習模型。它透過獨特的空間瓦片和土地覆蓋感知採樣方法，豐富了空間和語義覆蓋範圍。TerraFM將雷達和光學輸入視為自然增強，透過模態特定的patch嵌入和自適應交叉注意力融合來統一處理。其訓練策略結合了局部-全局對比學習，並引入雙中心機制，以解決土地覆蓋中長尾分佈的問題。TerraFM在分類和分割任務上表現出色，優於先前的模型，為地球觀測領域帶來了更強大的通用性和準確性。", "applications": ["農作物監測：農民可以利用TerraFM分析衛星影像，了解農作物的生長狀況、預測產量，及早發現病蟲害，提高農業生產效率。", "災害評估：在地震、洪水等災害發生後，TerraFM可以快速分析災區的受損情況，協助救援人員制定更有效的救援計畫，並進行災後重建。", "環境保護：環保機構可以利用TerraFM監測森林砍伐、水污染等環境問題，及時採取措施保護地球資源。"], "pitch": "各位創投先進，想像一下，我們正站在一個前所未有的數據金礦之上：地球觀測數據！TerraFM，我們的殺手級應用，正是開啟這座寶藏的鑰匙。它不僅能整合不同衛星感測器的數據，更具備強大的泛化能力，能應用於農業、災害管理、環境監測等各個領域。這意味著什麼？更精準的作物預測，減少糧食浪費；更快速的災害評估，拯救更多生命；更有效的環境監測，守護我們的地球。但這還不是全部！TerraFM的自監督學習能力，使其能不斷從海量數據中自我提升，就像一個永動機，不斷產生價值。未來，我們甚至可以將TerraFM應用於城市規劃、基礎設施建設、甚至是國防安全等更廣闊的領域。現在投資TerraFM，就是投資地球的未來，回報將遠超您的想像！", "audio": "docs/data/audios/2506.06281v1.wav"}
{"query": "Diffusion Model", "id": "2506.06276v1", "url": "http://arxiv.org/abs/2506.06276v1", "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis", "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.", "authors": ["Jiatao Gu", "Tianrong Chen", "David Berthelot", "Huangjie Zheng", "Yuyang Wang", "Ruixiang Zhang", "Laurent Dinh", "Miguel Angel Bautista", "Josh Susskind", "Shuangfei Zhai"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:22:30.194724", "title_zh": "STARFlow：擴展潛在歸一化流以實現高解析度圖像合成", "summary_zh": "STARFlow是一種基於歸一化流的可擴展生成模型，在高解析度圖像合成方面表現出色。其核心是Transformer自迴歸流（TARFlow），結合了歸一化流的表達能力和自迴歸Transformer的結構化建模能力。STARFlow通過深度-淺層設計、在預訓練自編碼器的潛在空間中建模以及創新的引導算法，顯著提高了可擴展性。該模型保持端到端的歸一化流，無需離散化即可在連續空間中進行精確的最大似然訓練。STARFlow在類條件和文本條件圖像生成任務中表現出色，其樣本品質接近最先進的擴散模型。這是首次成功展示歸一化流在此規模和解析度下有效運作。", "applications": ["想像一下，你想要一張獨一無二的寵物照片，但你沒有專業攝影師。STARFlow可以根據你的文字描述，例如「一隻戴著皇冠的可愛貓咪」，自動生成一張高解析度的照片。", "假設你是遊戲開發者，需要大量不同的遊戲角色和場景。STARFlow可以幫助你快速生成各種風格的遊戲素材，節省大量美術設計的時間和成本。", "如果你是室內設計師，想向客戶展示不同裝修風格的效果圖。STARFlow可以根據客戶的描述，快速生成逼真的室內設計圖，方便客戶選擇。"], "pitch": "各位投資人，我們帶來的是STARFlow，一項革命性的圖像生成技術，它將徹底改變圖像內容創作的遊戲規則！想像一下，一個可以根據簡單的文字描述，就能生成照片級別真實圖像的世界。STARFlow不僅僅是一個技術突破，它是一座金礦！在廣告行銷領域，它可以創造出高度個性化的廣告素材，大幅提升點擊率和轉換率。在娛樂產業，它可以賦予遊戲開發者和電影製作人前所未有的創作自由。在電商領域，它可以自動生成商品圖片，降低運營成本。更重要的是，隨著元宇宙的興起，對虛擬內容的需求將呈現爆炸式增長，而STARFlow正是滿足這一需求的完美解決方案。我們的團隊擁有世界一流的AI專家，我們已經成功驗證了STARFlow的技術可行性和商業潛力。現在，我們需要您的投資，共同將STARFlow推向市場，搶佔先機，打造一個全新的圖像內容生態系統。我們相信，STARFlow將成為下一代圖像生成技術的領導者，為您帶來豐厚的回報！", "audio": "docs/data/audios/2506.06276v1.wav"}
{"query": "AI", "id": "2506.06232v1", "url": "http://arxiv.org/abs/2506.06232v1", "title": "Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study", "summary": "While traditional computer vision models have historically struggled to\ngeneralize to endoscopic domains, the emergence of foundation models has shown\npromising cross-domain performance. In this work, we present the first\nlarge-scale study assessing the capabilities of Vision Language Models (VLMs)\nfor endoscopic tasks with a specific focus on laparoscopic surgery. Using a\ndiverse set of state-of-the-art models, multiple surgical datasets, and\nextensive human reference annotations, we address three key research questions:\n(1) Can current VLMs solve basic perception tasks on surgical images? (2) Can\nthey handle advanced frame-based endoscopic scene understanding tasks? and (3)\nHow do specialized medical VLMs compare to generalist models in this context?\nOur results reveal that VLMs can effectively perform basic surgical perception\ntasks, such as object counting and localization, with performance levels\ncomparable to general domain tasks. However, their performance deteriorates\nsignificantly when the tasks require medical knowledge. Notably, we find that\nspecialized medical VLMs currently underperform compared to generalist models\nacross both basic and advanced surgical tasks, suggesting that they are not yet\noptimized for the complexity of surgical environments. These findings highlight\nthe need for further advancements to enable VLMs to handle the unique\nchallenges posed by surgery. Overall, our work provides important insights for\nthe development of next-generation endoscopic AI systems and identifies key\nareas for improvement in medical visual language models.", "authors": ["Leon Mayer", "Tim Rädsch", "Dominik Michael", "Lucas Luttner", "Amine Yamlahi", "Evangelia Christodoulou", "Patrick Godau", "Marcel Knopp", "Annika Reinke", "Fiona Kolbinger", "Lena Maier-Hein"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:31:58.254264", "title_zh": "以外科手術數據挑戰視覺語言模型：一個新數據集與廣泛的基準測試研究", "summary_zh": "本研究首次大規模評估視覺語言模型（VLMs）在腹腔鏡手術等內視鏡任務中的能力。我們使用多種先進模型、手術數據集和人工標註，探討VLMs能否勝任手術圖像的基本感知任務和進階的內視鏡場景理解任務，以及專用醫療VLMs與通用模型的比較。結果顯示，VLMs在物體計數和定位等基本任務上表現出色，但處理需要醫學知識的任務時性能顯著下降。令人驚訝的是，專用醫療VLMs的表現不如通用模型，表明它們尚未針對手術環境的複雜性進行優化。這項研究突顯了未來開發內視鏡AI系統的需求，並為改進醫療視覺語言模型指明了方向。", "applications": ["想像一下，未來醫生在做腹腔鏡手術時，AI能即時辨識手術視野中的器官、血管，甚至提醒醫生注意潛在風險，就像有個經驗豐富的助手在旁邊一樣。", "以後醫學院學生可以利用這個AI系統來模擬手術，AI會根據學生的操作給予即時反饋，讓他們在真實手術前就能累積經驗。", "開發一套居家健康監測系統，透過內視鏡影像分析，早期發現腸胃道疾病，讓民眾在家就能進行初步的健康檢查。"], "pitch": "各位投資人，我們正在打造醫療AI的未來！這項技術不僅僅是個研究項目，它將徹底改變外科手術的面貌。想像一下，AI能輔助醫生進行更精準、更安全的手術，降低醫療事故的發生率，並大幅縮短手術時間。更重要的是，我們發現專用醫療模型的表現不如通用模型，這代表著巨大的市場機會！我們將開發針對手術環境優化的VLMs，解決現有模型的瓶頸，打造出真正能夠理解手術場景的AI。這不僅能應用於手術室，還能拓展到遠程醫療、醫學教育等領域，潛在市場規模數十億美元！現在加入我們，一起開創醫療AI的新紀元！", "audio": "docs/data/audios/2506.06232v1.wav"}
{"query": "Foundation Model", "id": "2506.06270v1", "url": "http://arxiv.org/abs/2506.06270v1", "title": "RecGPT: A Foundation Model for Sequential Recommendation", "summary": "This work addresses a fundamental barrier in recommender systems: the\ninability to generalize across domains without extensive retraining.\nTraditional ID-based approaches fail entirely in cold-start and cross-domain\nscenarios where new users or items lack sufficient interaction history.\nInspired by foundation models' cross-domain success, we develop a foundation\nmodel for sequential recommendation that achieves genuine zero-shot\ngeneralization capabilities. Our approach fundamentally departs from existing\nID-based methods by deriving item representations exclusively from textual\nfeatures. This enables immediate embedding of any new item without model\nretraining. We introduce unified item tokenization with Finite Scalar\nQuantization that transforms heterogeneous textual descriptions into\nstandardized discrete tokens. This eliminates domain barriers that plague\nexisting systems. Additionally, the framework features hybrid\nbidirectional-causal attention that captures both intra-item token coherence\nand inter-item sequential dependencies. An efficient catalog-aware beam search\ndecoder enables real-time token-to-item mapping. Unlike conventional approaches\nconfined to their training domains, RecGPT naturally bridges diverse\nrecommendation contexts through its domain-invariant tokenization mechanism.\nComprehensive evaluations across six datasets and industrial scenarios\ndemonstrate consistent performance advantages.", "authors": ["Yangqin Jiang", "Xubin Ren", "Lianghao Xia", "Da Luo", "Kangyi Lin", "Chao Huang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:33:25.131072", "title_zh": "RecGPT：用於序列推薦的基礎模型", "summary_zh": "RecGPT 是一個突破性的推薦系統，它像大型語言模型一樣，具備跨領域的泛化能力，不需要針對新領域重新訓練。它捨棄了傳統基於ID的方法，改為完全從文字特徵提取商品資訊，讓新商品能立即加入推薦，無需重新訓練模型。RecGPT 使用統一的商品符號化方法，將各種文字描述轉換為標準化的離散符號，消除了領域之間的障礙。此外，它還採用混合雙向因果注意力機制，捕捉商品內部的關聯和商品之間的順序關係。這種方法在六個數據集和工業場景中都展現了優越的性能，為推薦系統帶來了革命性的改變。", "applications": ["**個人化新聞推薦：** 不再只推薦你看過的新聞，而是根據你讀過的文章內容，推薦其他領域但主題相關的新聞，擴展你的知識視野。", "**跨平台商品推薦：** 假設你在A電商平台買了咖啡豆，RecGPT可以立刻在B平台上推薦你適合的咖啡濾杯或磨豆機，即使你在B平台沒有任何購買紀錄。", "**冷啟動影視推薦：** 新上映的冷門獨立電影，即使觀看人數不多，RecGPT也能透過電影簡介的文字內容，推薦給可能感興趣的觀眾，讓小眾佳作也能被發掘。"], "pitch": "各位投資人，想像一下，一個能理解所有商品和使用者喜好的超級推薦引擎，它不需要大量數據訓練，就能精準推薦，這就是RecGPT的潛力！傳統推薦系統就像個別的孤島，RecGPT則是一座連接所有島嶼的橋樑。它不僅解決了冷啟動和跨領域推薦的難題，更開創了全新的商業模式。我們可以將RecGPT授權給各個電商平台、內容平台，甚至線下零售商，讓他們輕鬆實現個性化推薦，提升銷售額和使用者滿意度。更進一步，RecGPT可以應用於智慧城市、智慧醫療等領域，例如根據病患的病歷和生活習慣，推薦個性化的健康管理方案。未來，RecGPT將成為AI推薦領域的領導者，引領下一代推薦技術的發展。現在投資RecGPT，就是投資未來！", "audio": "docs/data/audios/2506.06270v1.wav"}
{"query": "Diffusion Model", "id": "2506.06185v1", "url": "http://arxiv.org/abs/2506.06185v1", "title": "Antithetic Noise in Diffusion Models", "summary": "We initiate a systematic study of antithetic initial noise in diffusion\nmodels. Across unconditional models trained on diverse datasets,\ntext-conditioned latent-diffusion models, and diffusion-posterior samplers, we\nfind that pairing each initial noise with its negation consistently yields\nstrongly negatively correlated samples. To explain this phenomenon, we combine\nexperiments and theoretical analysis, leading to a symmetry conjecture that the\nlearned score function is approximately affine antisymmetric (odd symmetry up\nto a constant shift), and provide evidence supporting it. Leveraging this\nnegative correlation, we enable two applications: (1) enhancing image diversity\nin models like Stable Diffusion without quality loss, and (2) sharpening\nuncertainty quantification (e.g., up to 90% narrower confidence intervals) when\nestimating downstream statistics. Building on these gains, we extend the\ntwo-point pairing to a randomized quasi-Monte Carlo estimator, which further\nimproves estimation accuracy. Our framework is training-free, model-agnostic,\nand adds no runtime overhead.", "authors": ["Jing Jia", "Sifan Liu", "Bowen Song", "Wei Yuan", "Liyue Shen", "Guanyang Wang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:34:50.492212", "title_zh": "擴散模型中的反義噪音", "summary_zh": "本研究深入探討擴散模型中反義初始噪音的特性。我們發現，無論是無條件模型、文本條件潛在擴散模型還是擴散後驗採樣器，將每個初始噪音與其負值配對，都能產生強烈負相關的樣本。我們提出「對稱猜想」，認為模型學習到的分數函數近似為仿射反對稱（奇對稱加上常數偏移）。基於這種負相關性，我們實現了兩個應用：一是提高Stable Diffusion等模型的圖像多樣性，且不損失品質；二是銳化不確定性量化，例如縮小高達90%的置信區間。此外，我們將雙點配對擴展到隨機準蒙地卡羅估計器，進一步提高了估計準確性。此框架無需訓練、適用於各種模型，且不增加運行時開銷。", "applications": ["想像一下，你想要用AI繪圖產生風景照，但每次生成的結果都很類似。使用這項技術，你可以輕鬆產生更多樣化的風景，讓你的照片集更加豐富。", "醫生在分析X光片時，常常需要判斷是否有微小的病灶。這項技術可以幫助醫生更精準地評估診斷結果的不確定性，提供更可靠的醫療建議。", "科學家在模擬氣候變遷時，需要考慮各種不確定因素。這項技術可以幫助他們更準確地預測氣候變遷的影響，為政策制定提供更可靠的依據。"], "pitch": "各位創投，擴散模型是AI領域的明日之星，但其生成結果的多樣性和預測的準確性仍有提升空間。我們的「反義噪音」技術，無需額外訓練成本，就能顯著提高圖像生成的多樣性，並大幅提升不確定性量化的準確性。這意味著，我們可以打造更具創意、更可靠的AI應用。試想一下，將這項技術應用於自動駕駛，可以更精準地預測路況風險；應用於金融市場預測，可以更有效地管理投資組合風險；應用於新藥研發，可以更快速地篩選潛力藥物。這不僅是一項技術突破，更是一個巨大的商業機會。我們相信，透過您的投資，我們可以將這項技術推向更廣闊的應用領域，共同開創AI的新紀元！", "audio": "docs/data/audios/2506.06185v1.wav"}
{"query": "AI", "id": "2506.06225v1", "url": "http://arxiv.org/abs/2506.06225v1", "title": "\"We need to avail ourselves of GenAI to enhance knowledge distribution\": Empowering Older Adults through GenAI Literacy", "summary": "As generative AI (GenAI) becomes increasingly widespread, it is crucial to\nequip users, particularly vulnerable populations such as older adults (65 and\nolder), with the knowledge to understand its benefits and potential risks.\nOlder adults often exhibit greater reservations about adopting emerging\ntechnologies and require tailored literacy support. Using a mixed methods\napproach, this study examines strategies for delivering GenAI literacy to older\nadults through a chatbot named Litti, evaluating its impact on their AI\nliteracy (knowledge, safety, and ethical use). The quantitative data indicated\na trend toward improved AI literacy, though the results were not statistically\nsignificant. However, qualitative interviews revealed diverse levels of\nfamiliarity with generative AI and a strong desire to learn more. Findings also\nshow that while Litti provided a positive learning experience, it did not\nsignificantly enhance participants' trust or sense of safety regarding GenAI.\nThis exploratory case study highlights the challenges and opportunities in\ndesigning AI literacy education for the rapidly growing older adult population.", "authors": ["Eunhye Grace Ko", "Shaini Nanayakkara", "Earl W. Huff Jr"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:44:00.677565", "title_zh": "「我們需要善用生成式AI來強化知識傳播」：透過生成式AI素養賦能年長者", "summary_zh": "本研究探討如何提升年長者對生成式AI的素養，讓他們了解其益處與潛在風險。研究採用混合方法，透過名為Litti的聊天機器人，評估其對年長者AI素養（知識、安全和道德使用）的影響。定量數據顯示AI素養有改善趨勢，但未達統計顯著性。質性訪談則揭示年長者對生成式AI的熟悉程度各異，但都渴望學習更多。研究發現Litti提供了正面的學習體驗，但並未顯著提升參與者對生成式AI的信任感或安全感。本研究強調了為快速增長的年長者人口設計AI素養教育的挑戰與機會。", "applications": ["**長輩專屬的AI健康管家：** Litti可以變成一個24小時待命的健康顧問，提醒長輩服藥、提供飲食建議，甚至在緊急情況下聯絡家人或救護車。它能用長輩習慣的語言溝通，讓他們更安心。", "**AI陪伴聊天解悶神器：** 許多長輩獨居，Litti可以陪他們聊天、分享新聞、甚至一起玩簡單的遊戲。它能記住長輩的喜好，提供客製化的內容，減少孤獨感。", "**銀髮族數位學習好幫手：** Litti可以教長輩如何使用智慧型手機、平板電腦，讓他們輕鬆上手網路購物、視訊通話，甚至參與線上課程，享受數位生活的便利。"], "pitch": "各位投資人，高齡化社會是全球趨勢，而生成式AI是賦能銀髮族、提升他們生活品質的關鍵技術。想像一下，一個由AI驅動的銀髮族生態系，包含個人化的健康管理、社交互動、數位學習等服務，市場潛力無窮！我們的Litti聊天機器人正是這個生態系的起點。它不僅能提升長輩的AI素養，更能成為他們信任的數位夥伴。我們計劃將Litti整合到各種銀髮族產品和服務中，例如智慧居家設備、遠距醫療平台等，打造一個龐大的銀髮族AI市場。現在投資我們，您將搶佔先機，共同開創銀髮經濟的下一個藍海！未來，我們甚至可以將Litti發展成具有情感理解能力的AI，真正成為長輩們的心靈伴侶，這將是劃時代的創新！", "audio": "docs/data/audios/2506.06225v1.wav"}
{"query": "Foundation Model", "id": "2506.06211v1", "url": "http://arxiv.org/abs/2506.06211v1", "title": "PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts", "summary": "Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined\nproblem definitions. In contrast to conventional reasoning benchmarks\nconsisting of tasks with clear instructions, puzzlehunts require models to\ndiscover the underlying problem structure from multimodal evidence and\niterative reasoning, mirroring real-world domains such as scientific discovery,\nexploratory data analysis, or investigative problem-solving. Despite recent\nprogress in foundation models, their performance on such open-ended settings\nremains largely untested. In this paper, we introduce PuzzleWorld, a\nlarge-scale benchmark of 667 puzzlehunt-style problems designed to assess\nstep-by-step, open-ended, and creative multimodal reasoning. Each puzzle is\nannotated with the final solution, detailed reasoning traces, and cognitive\nskill labels, enabling holistic benchmarking and fine-grained diagnostic\nanalysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,\nwith the best model solving only 14% of puzzles and reaching 40% stepwise\naccuracy. To demonstrate the value of our reasoning annotations, we show that\nfine-tuning a small model on reasoning traces improves stepwise reasoning from\n4% to 11%, while training on final answers alone degrades performance to near\nzero. Our error analysis reveals that current models exhibit myopic reasoning,\nare bottlenecked by the limitations of language-based inference, and lack\nsketching capabilities crucial for visual and spatial reasoning. We release\nPuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on\nbuilding more general, open-ended, and creative reasoning systems.", "authors": ["Hengzhi Li", "Brendon Jiang", "Alexander Naehu", "Regan Song", "Justin Zhang", "Megan Tjandrasuwita", "Chanakya Ekbote", "Steven-Shine Chen", "Adithya Balachandran", "Wei Dai", "Rebecca Chang", "Paul Pu Liang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:45:24.929005", "title_zh": "謎題世界：謎題狩獵中多模態、開放式推理的基準測試", "summary_zh": "「謎題狩獵」是一種複雜、多步驟的謎題類型，缺乏明確的問題定義。PuzzleWorld是一個大型基準測試，包含667個謎題狩獵風格的問題，旨在評估逐步、開放式和創造性的多模態推理。現有模型在最終答案的準確率上僅達到1-2%，最佳模型也僅解決了14%的謎題。研究顯示，模型在推理過程中存在短視近利的問題，並受限於基於語言的推論能力，且缺乏視覺和空間推理所需的草圖能力。此基準測試將有助於開發更通用、開放式和創造性的推理系統，可用於科學發現、數據分析和調查性問題解決等領域。", "applications": ["設計逃脫遊戲：PuzzleWorld可以幫助遊戲設計師創建更具挑戰性、更有趣的逃脫遊戲，透過AI自動生成謎題和線索，讓玩家有更好的遊戲體驗。", "輔助兒童教育：將PuzzleWorld應用於兒童教育，可以開發出更具互動性的學習工具，培養孩子的邏輯思維、空間推理和創造力，讓學習過程更加生動有趣。", "提升企業問題解決能力：企業可以利用PuzzleWorld來訓練員工的解決問題能力，透過模擬真實世界的複雜情境，提升團隊合作和創新思維。"], "pitch": "各位投資人，我們正處於AI發展的關鍵時刻，生成式AI正快速改變世界。然而，現有的AI模型在處理需要多模態推理、開放式問題解決的複雜任務時，能力仍遠遠不足。PuzzleWorld的出現，正是為了填補這一空白。它不僅是一個基準測試，更是一個孕育新一代AI的搖籃。想像一下，未來的AI不僅能理解語言，還能看懂圖像、理解空間關係，甚至能像人類一樣進行創造性思考。這種AI將在科學研究、金融分析、甚至藝術創作等領域產生顛覆性的影響。我們相信，透過PuzzleWorld的持續發展，我們能打造出真正具有通用智能的AI，開創一個充滿無限可能的未來。現在投資PuzzleWorld，就是投資AI的未來，您將成為這場技術革命的先驅！", "audio": "docs/data/audios/2506.06211v1.wav"}
{"query": "Diffusion Model", "id": "2506.06085v1", "url": "http://arxiv.org/abs/2506.06085v1", "title": "Feedback Guidance of Diffusion Models", "summary": "While Classifier-Free Guidance (CFG) has become standard for improving sample\nfidelity in conditional diffusion models, it can harm diversity and induce\nmemorization by applying constant guidance regardless of whether a particular\nsample needs correction. We propose FeedBack Guidance (FBG), which uses a\nstate-dependent coefficient to self-regulate guidance amounts based on need.\nOur approach is derived from first principles by assuming the learned\nconditional distribution is linearly corrupted by the unconditional\ndistribution, contrasting with CFG's implicit multiplicative assumption. Our\nscheme relies on feedback of its own predictions about the conditional signal\ninformativeness to adapt guidance dynamically during inference, challenging the\nview of guidance as a fixed hyperparameter. The approach is benchmarked on\nImageNet512x512, where it significantly outperforms Classifier-Free Guidance\nand is competitive to Limited Interval Guidance (LIG) while benefitting from a\nstrong mathematical framework. On Text-To-Image generation, we demonstrate\nthat, as anticipated, our approach automatically applies higher guidance scales\nfor complex prompts than for simpler ones and that it can be easily combined\nwith existing guidance schemes such as CFG or LIG.", "authors": ["Koulischer Felix", "Handke Florian", "Deleu Johannes", "Demeester Thomas", "Ambrogioni Luca"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:47:05.009526", "title_zh": "擴散模型的反饋引導", "summary_zh": "現行的無分類器引導(CFG)雖然能提升條件式擴散模型的生成品質，但恆定的引導可能損害多樣性並導致記憶化。我們提出反饋引導(FBG)，它使用一個狀態相關係數，根據需求自我調節引導量。FBG基於第一原理推導，假設學習到的條件分佈被無條件分佈線性破壞。FBG利用自身對條件訊號資訊量的預測反饋，在推論過程中動態調整引導，挑戰了引導作為固定超參數的觀點。在ImageNet512x512基準測試中，FBG顯著優於CFG，並與LIG競爭，同時受益於強大的數學框架。在文本到圖像生成中，FBG能針對複雜提示自動應用更高的引導尺度，且易於與現有引導方案(如CFG或LIG)結合。", "applications": ["想像一下，你想要AI幫你畫一張生日派對的邀請函，但你只給了很簡單的描述，像是「生日快樂」。傳統的AI可能會畫出很普通的派對畫面。但用了反饋引導，AI會自動判斷這個提示太簡單，需要加強引導，於是它會加入更多細節，像是氣球、蛋糕、禮物等等，讓邀請函更豐富。", "假設你是服裝設計師，想用AI生成一些新的設計稿。你輸入一個比較模糊的概念，像是「未來感外套」。用了反饋引導的AI，會根據這個概念的複雜度，自動調整生成過程，確保生成的外套設計既有未來感，又不會過於抽象或難以理解，讓設計師能更容易的激發靈感。", "如果你在玩AI繪圖，想要生成一張特定風格的圖片，例如「梵谷風格的貓」。如果提示不夠明確，AI可能會畫出很普通的貓。但有了反饋引導，AI會自動加強梵谷風格的元素，像是用色、筆觸等等，讓生成的貓咪圖片更具藝術感，更符合你的期望。"], "pitch": "各位投資人，我們帶來的是擴散模型領域的革命性技術——反饋引導(FBG)。現有的生成式AI，如DALL-E、Midjourney等，都依賴於人工設定的引導參數，這不僅耗時，也限制了AI的創造力。FBG技術顛覆了這一模式，它讓AI能夠像一位經驗豐富的藝術家一樣，根據創作內容的複雜程度，自動調整引導的力度，從而生成更高品質、更具創意、更符合使用者需求的圖像。想像一下，未來，設計師、藝術家、甚至是普通使用者，都能夠輕鬆地利用AI創造出獨一無二的作品，而無需具備專業的AI知識。這將開啟一個全新的創意經濟時代，市場規模將是數百億美元級別的。更重要的是，FBG技術不僅僅局限於圖像生成，它還可以應用於音訊、影片、甚至3D模型的生成，潛力無限。我們相信，FBG技術將成為下一代生成式AI的核心引擎，而我們團隊將引領這場技術革命。現在加入我們，共同打造AI驅動的未來，您將獲得豐厚的回報！", "audio": "docs/data/audios/2506.06085v1.wav"}
{"query": "AI", "id": "2506.06220v1", "url": "http://arxiv.org/abs/2506.06220v1", "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval", "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind,\nthat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction.", "authors": ["Diji Yang", "Minghao Liu", "Chung-Hsiang Lo", "Yi Zhang", "James Davis"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:29:11.968645", "title_zh": "GenIR：用於心理圖像檢索的生成式視覺回饋", "summary_zh": "現有的視覺語言模型在文字到圖像檢索方面表現出色，但實際應用仍存在挑戰。GenIR針對「心理圖像檢索」任務，讓使用者能透過多輪互動，逐步逼近腦海中的圖像。GenIR的核心是利用擴散模型生成圖像，將AI系統的理解視覺化呈現，提供清晰且可操作的回饋。使用者能根據這些視覺回饋，更直觀有效地調整檢索條件。我們還建立了全自動流程，生成高品質的多輪心理圖像檢索數據集。實驗結果顯示，GenIR顯著優於現有的互動式方法，為未來研究奠定了基礎。", "applications": ["想像一下，你忘記了小時候最喜歡的玩具長什麼樣子，但還記得一些模糊的特徵。透過GenIR，你可以描述這些特徵，系統會生成可能的圖像，讓你逐步縮小範圍，最終找到你心心念念的玩具。", "假設你想在家裡重新裝潢，但腦海中只有一些零碎的想法。你可以用GenIR描述你想要的風格、顏色和家具，系統會生成不同的房間設計，幫助你找到最喜歡的方案，省去尋找靈感的時間。", "如果你正在尋找失散多年的親人，但只有一些模糊的記憶，例如臉部特徵或衣著風格。GenIR可以根據你的描述生成可能的圖像，幫助你擴大搜索範圍，增加找到親人的機會。"], "pitch": "各位投資人，我們相信GenIR將徹底改變圖像檢索的未來！現今的圖像檢索技術往往無法滿足人們腦海中模糊的需求。GenIR透過生成式視覺回饋，讓人機互動更加直觀高效，解決了這個痛點。想像一下，未來的電商平台，使用者只需描述想要的商品，AI就能生成商品圖像，甚至可以根據使用者的喜好客製化設計。在醫療領域，醫生可以透過GenIR，根據患者的描述生成病灶圖像，輔助診斷。在安全領域，警方可以根據目擊者的描述，生成嫌疑犯的模擬圖像，提高破案率。GenIR的應用場景無限廣闊，市場潛力巨大。我們已經建立了一個高品質的數據集，並開發了領先的生成式檢索方法。我們正在尋找有遠見的投資人，一起將GenIR推向市場，引領下一代圖像檢索革命！", "audio": "docs/data/audios/2506.06220v1.wav"}
{"query": "Foundation Model", "id": "2506.06105v1", "url": "http://arxiv.org/abs/2506.06105v1", "title": "Text-to-LoRA: Instant Transformer Adaption", "summary": "While Foundation Models provide a general tool for rapid content creation,\nthey regularly require task-specific adaptation. Traditionally, this exercise\ninvolves careful curation of datasets and repeated fine-tuning of the\nunderlying model. Fine-tuning techniques enable practitioners to adapt\nfoundation models for many new applications but require expensive and lengthy\ntraining while being notably sensitive to hyper-parameter choices. To overcome\nthese limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting\nLarge Language Models on the fly solely based on a natural language description\nof the target task. T2L is a hypernetwork trained to construct LoRAs in a\nsingle inexpensive forward pass. After training T2L on a suite of 9 pre-trained\nLoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA\ninstances match the performance of task-specific adapters across the\ncorresponding test sets. Furthermore, T2L can compress hundreds of LoRA\ninstances and zero-shot generalize to entirely unseen tasks. This approach\nprovides a significant step towards democratizing the specialization of\nfoundation models and enables language-based adaptation with minimal compute\nrequirements. Our code is available at https://github.com/SakanaAI/text-to-lora", "authors": ["Rujikorn Charakorn", "Edoardo Cetin", "Yujin Tang", "Robert Tjarko Lange"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:30:20.416876", "title_zh": "文字到LoRA：即時轉換器適應", "summary_zh": "本研究提出Text-to-LoRA (T2L)模型，能根據自然語言描述，即時調整大型語言模型以適應特定任務。T2L是一種超網路，只需一次前向傳遞就能建構LoRA。經過九個預訓練LoRA適配器訓練後，T2L重建的LoRA實例在對應測試集上表現與特定任務適配器相當。更重要的是，T2L能壓縮數百個LoRA實例，並零樣本泛化到全新任務。這項技術大幅降低了基礎模型專業化的門檻，並以極少的計算資源實現基於語言的適應，讓AI模型客製化變得更快速、更普及。", "applications": ["AI繪圖客製化：使用者只要用文字描述想要的圖片風格(例如：水墨畫、卡通風格)，AI就能快速調整模型，產生符合需求的圖片。", "個人化AI助理：針對不同使用者的需求，例如：商業寫作、程式碼除錯等，AI助理能根據文字指令即時調整模型，提供更精準的協助。", "遊戲AI角色客製化：遊戲開發者可以透過文字描述，快速調整AI角色的行為模式或對話風格，讓遊戲體驗更加豐富。"], "pitch": "各位投資人，想像一下，未來每個人都能輕鬆客製化AI模型，就像調整手機App一樣簡單！Text-to-LoRA技術，正是實現這個願景的關鍵。它能讓AI模型根據文字指令即時調整，無需耗時費力的重新訓練。這意味著，我們能以極低的成本，打造出無數個針對特定領域或個人需求的AI應用。從AI繪圖、個人化助理到遊戲AI，市場潛力無限。更重要的是，T2L技術還能壓縮模型，讓AI應用在各種裝置上都能流暢運行。我們相信，Text-to-LoRA將徹底顛覆AI產業，成為新一代AI應用的基礎設施。現在加入我們，一起打造AI客製化的未來！", "audio": "docs/data/audios/2506.06105v1.wav"}
{"query": "Diffusion Model", "id": "2506.06023v1", "url": "http://arxiv.org/abs/2506.06023v1", "title": "Restereo: Diffusion stereo video generation and restoration", "summary": "Stereo video generation has been gaining increasing attention with recent\nadvancements in video diffusion models. However, most existing methods focus on\ngenerating 3D stereoscopic videos from monocular 2D videos. These approaches\ntypically assume that the input monocular video is of high quality, making the\ntask primarily about inpainting occluded regions in the warped video while\npreserving disoccluded areas. In this paper, we introduce a new pipeline that\nnot only generates stereo videos but also enhances both left-view and\nright-view videos consistently with a single model. Our approach achieves this\nby fine-tuning the model on degraded data for restoration, as well as\nconditioning the model on warped masks for consistent stereo generation. As a\nresult, our method can be fine-tuned on a relatively small synthetic stereo\nvideo datasets and applied to low-quality real-world videos, performing both\nstereo video generation and restoration. Experiments demonstrate that our\nmethod outperforms existing approaches both qualitatively and quantitatively in\nstereo video generation from low-resolution inputs.", "authors": ["Xingchang Huang", "Ashish Kumar Singh", "Florian Dubost", "Cristina Nader Vasconcelos", "Sakar Khattar", "Liang Shi", "Christian Theobalt", "Cengiz Oztireli", "Gurprit Singh"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:32:05.943203", "title_zh": "Restereo：擴散立體影片生成與修復", "summary_zh": "本研究提出一個新穎的立體影片生成流程，不僅能從單眼2D影片生成3D立體影片，還能同時增強左右視角的影片品質。此方法透過在降質數據上微調模型進行修復，並以扭曲遮罩為條件進行一致的立體生成。因此，即使在相對較小的合成立體影片數據集上進行微調，也能應用於低品質的真實世界影片，同時實現立體影片的生成和修復。實驗結果表明，本方法在低解析度輸入的立體影片生成方面，在品質和數量上均優於現有方法。", "applications": ["在家用VR觀影時，即使影片來源畫質不佳，也能透過此技術即時提升畫質並轉換為立體3D，享受更沉浸式的觀影體驗。", "老舊照片或影片的數位修復：將舊照片或影片轉換為立體影像，讓回憶更加生動，並修復畫質，讓珍貴的影像資料得以保存。", "線上遊戲體驗優化：即時將2D遊戲畫面轉換為3D立體畫面，提升遊戲沉浸感，並修復遊戲畫面中可能存在的模糊或失真問題。"], "pitch": "各位創投先進，我們帶來的是Restereo，一項劃時代的立體影片生成與修復技術。想像一下，現今VR/AR內容的最大瓶頸是什麼？是高品質3D內容的匱乏！Restereo能將任何2D影片，甚至是低畫質的老舊影片，即時轉換為令人驚豔的3D立體影像，並同步提升畫質。這代表什麼？龐大的內容創作潛力！從個人用戶到大型影視公司，都能輕易創造出引人入勝的VR/AR體驗。更重要的是，我們能賦予歷史影像新的生命力，將塵封的記憶以更真實、更立體的方式呈現。未來，Restereo將成為元宇宙內容生態的基石，我們不只是在修復影片，我們是在打造一個全新的視覺世界！現在投資Restereo，就是投資元宇宙的未來！", "audio": "docs/data/audios/2506.06023v1.wav"}
{"query": "AI", "id": "2506.06214v1", "url": "http://arxiv.org/abs/2506.06214v1", "title": "Can Theoretical Physics Research Benefit from Language Agents?", "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.", "authors": ["Sirui Lu", "Zhijing Jin", "Terry Jingchen Zhang", "Pavel Kos", "J. Ignacio Cirac", "Bernhard Schölkopf"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:35:20.869608", "title_zh": "理論物理研究能否受益於語言智能體？", "summary_zh": "大型語言模型(LLM)在各領域快速發展，但在理論物理研究中的應用尚不成熟。本文認為，若將LLM智能體與領域知識和工具箱適當結合，有潛力加速理論、計算和應用物理學的發展。我們分析了LLM目前在物理學方面的能力，包括數學推理和程式碼生成，並指出了在物理直覺、約束滿足和可靠推理方面的關鍵差距。我們設想未來專門用於物理學的LLM能夠處理多模態數據、提出可驗證的假設並設計實驗。實現這一願景需要應對根本挑戰：確保物理一致性，並開發穩健的驗證方法。我們呼籲物理學界和人工智慧社群共同努力，以幫助推進物理學的科學發現。", "applications": ["**智慧教材：** LLM能根據學生的學習進度和理解程度，客製化物理教材和練習題，就像一位24小時隨時待命的私人物理家教。", "**科學玩具：** LLM可以嵌入到玩具中，讓孩子在玩樂中學習物理知識，例如，一個能回答物理問題的積木或一個能模擬物理現象的遊戲。", "**故障排除：** LLM可以協助工程師快速診斷複雜系統的故障，例如，分析感測器數據，找出發電廠或飛機引擎的潛在問題。"], "pitch": "各位投資人，我們正處於AI與物理學交匯的革命性時刻！想像一下，一個能自主設計實驗、推導新理論的AI科學家，這不再是科幻小說。我們的團隊正在開發專為物理學打造的LLM智能體，它能處理複雜的物理數據，提出創新的解決方案，並加速科學發現的進程。這項技術的潛在商業價值難以估量，從新材料的發現到能源效率的突破，再到太空探索的加速，都將受益於此。我們預計，未來物理學LLM將成為科研機構、工程公司和政府部門不可或缺的工具。現在投資我們，您將站在這場科學革命的最前沿，共同塑造未來！", "audio": "docs/data/audios/2506.06214v1.wav"}
{"query": "Foundation Model", "id": "2506.06076v1", "url": "http://arxiv.org/abs/2506.06076v1", "title": "Full Conformal Adaptation of Medical Vision-Language Models", "summary": "Vision-language models (VLMs) pre-trained at large scale have shown\nunprecedented transferability capabilities and are being progressively\nintegrated into medical image analysis. Although its discriminative potential\nhas been widely explored, its reliability aspect remains overlooked. This work\ninvestigates their behavior under the increasingly popular split conformal\nprediction (SCP) framework, which theoretically guarantees a given error level\non output sets by leveraging a labeled calibration set. However, the zero-shot\nperformance of VLMs is inherently limited, and common practice involves\nfew-shot transfer learning pipelines, which cannot absorb the rigid\nexchangeability assumptions of SCP. To alleviate this issue, we propose full\nconformal adaptation, a novel setting for jointly adapting and conformalizing\npre-trained foundation models, which operates transductively over each test\ndata point using a few-shot adaptation set. Moreover, we complement this\nframework with SS-Text, a novel training-free linear probe solver for VLMs that\nalleviates the computational cost of such a transductive approach. We provide\ncomprehensive experiments using 3 different modality-specialized medical VLMs\nand 9 adaptation tasks. Our framework requires exactly the same data as SCP,\nand provides consistent relative improvements of up to 27% on set efficiency\nwhile maintaining the same coverage guarantees.", "authors": ["Julio Silva-Rodríguez", "Leo Fillioux", "Paul-Henry Cournède", "Maria Vakalopoulou", "Stergios Christodoulidis", "Ismail Ben Ayed", "Jose Dolz"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:36:57.027212", "title_zh": "醫學視覺語言模型之完全適形調整", "summary_zh": "大型預訓練的視覺語言模型（VLMs）在醫學影像分析中展現了前所未有的遷移能力。然而，其可靠性卻被忽略。本研究探討了在split conformal prediction (SCP)框架下VLMs的行為，該框架藉由標記的校準集，在輸出集上保證給定的錯誤水平。為了解決VLMs的zero-shot性能限制以及few-shot遷移學習管道無法滿足SCP的嚴格可交換性假設的問題，我們提出了完全適形調整，這是一種新穎的設定，用於聯合調整和適形預訓練的基礎模型，並使用few-shot調整集對每個測試數據點進行轉導操作。此外，我們使用SS-Text來補充這個框架，這是一種用於VLMs的免訓練線性探測求解器，可減輕這種轉導方法的計算成本。實驗結果表明，我們的框架在保持相同覆蓋率保證的同時，在集合效率上提供了高達27%的相對改進。", "applications": ["**遠距醫療影像判讀：** 想像一下，偏鄉地區的醫生可以透過手機App，將X光片上傳，AI就能快速提供初步診斷結果，協助醫生做出更精確的判斷，提升醫療效率。", "**個人化健康管理：** 未來，我們可以將自己的醫療影像，例如心電圖、眼底照片等，上傳到一個安全平台，AI會分析這些數據，並提供個人化的健康建議，例如飲食調整、運動計畫等。", "**新藥開發加速：** 藥廠可以利用這項技術，快速分析大量的醫學影像資料，找出潛在的藥物靶點，加速新藥開發的進程，讓更多疾病得到及時治療。"], "pitch": "各位投資人，我們帶來的是醫學影像AI的革命性突破！傳統AI在醫學影像判讀上，準確度參差不齊，醫生往往不敢完全信任。我們的「完全適形調整」技術，能讓AI在判讀醫學影像時，不僅給出結果，還能提供信賴度評估，讓醫生更安心。想像一下，這項技術能大幅降低誤診率，提升醫療品質，減少醫療糾紛。更重要的是，它能解放醫生的時間，讓他們能更專注於病人護理。市場潛力巨大！從遠距醫療、個人化健康管理，到新藥開發，都有廣闊的應用前景。我們預期，在未來五年內，這項技術將成為醫學影像AI的產業標準，帶領我們在精準醫療時代搶佔先機。現在加入我們，您將成為這場醫療革命的領航者！", "audio": "docs/data/audios/2506.06076v1.wav"}
{"query": "Diffusion Model", "id": "2506.06018v1", "url": "http://arxiv.org/abs/2506.06018v1", "title": "Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models", "summary": "Watermarking becomes one of the pivotal solutions to trace and verify the\norigin of synthetic images generated by artificial intelligence models, but it\nis not free of risks. Recent studies demonstrate the capability to forge\nwatermarks from a target image onto cover images via adversarial optimization\nwithout knowledge of the target generative model and watermark schemes. In this\npaper, we uncover a greater risk of an optimization-free and universal\nwatermark forgery that harnesses existing regenerative diffusion models. Our\nproposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and\nintegrates the target watermark via regenerating the image, without needing any\nadditional optimization routine. It allows for universal watermark forgery that\nworks independently of the target image's origin or the watermarking model\nused. We explore the watermarked latent extracted from the target image and\nvisual-textual context of cover images as priors to guide sampling of the\nregenerative process. Extensive evaluation on 24 scenarios of\nmodel-data-watermark combinations demonstrates that PnP can successfully forge\nthe watermark (up to 100% detectability and user attribution), and maintain the\nbest visual perception. By bypassing model retraining and enabling adaptability\nto any image, our approach significantly broadens the scope of forgery attacks,\npresenting a greater challenge to the security of current watermarking\ntechniques for diffusion models and the authority of watermarking schemes in\nsynthetic data generation and governance.", "authors": ["Chaoyi Zhu", "Zaitang Li", "Renyi Yang", "Robert Birke", "Pin-Yu Chen", "Tsung-Yi Ho", "Lydia Y. Chen"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:38:37.267711", "title_zh": "基於再生擴散模型之免優化通用浮水印偽造", "summary_zh": "浮水印技術被廣泛應用於追蹤和驗證AI生成圖像的來源，但存在偽造風險。本研究揭示了一種更嚴重的免優化通用浮水印偽造方法，利用現有的再生擴散模型，名為PnP（Plug-and-Plant）。PnP無需額外優化，即可透過圖像再生無縫提取和整合目標浮水印。此方法獨立於目標圖像的來源或浮水印模型，實現通用浮水印偽造。實驗證明，PnP在多種情境下成功偽造浮水印，同時保持最佳視覺效果。這種繞過模型重新訓練並適應任何圖像的能力，擴大了偽造攻擊的範圍，對當前擴散模型的浮水印技術安全性和合成數據生成和治理中浮水印方案的權威性提出了更大的挑戰。", "applications": ["情境一：假設你是一位藝術家，想保護你的AI生成作品不被盜用。但有人利用這項技術，將你的浮水印複製到其他圖像上，讓你難以證明原創性，甚至可能被誤認為抄襲者。", "情境二：新聞媒體使用AI生成圖片來輔助報導。如果有人惡意將浮水印偽造到假新聞圖片上，並嫁禍給該媒體，可能嚴重損害其聲譽和公信力。", "情境三：在學術界，研究人員發表基於AI生成數據的論文。如果他人偽造浮水印，聲稱該數據來自不同的來源，可能導致學術欺詐和錯誤的研究結論。"], "pitch": "各位創投朋友們，想像一下，AI生成的內容正以前所未有的速度爆發，但信任危機也隨之而來。我們的技術揭示了現有浮水印系統的重大漏洞，同時也帶來了巨大的商機！PnP技術不僅能檢測偽造的浮水印，更能進一步開發出更強大、更安全的浮水印系統，保護原創內容，維護數據的真實性。未來，我們可以將這項技術應用於數位版權管理、內容溯源、甚至金融安全等領域。試想一下，每一張AI生成的圖片、每一份重要的數據報告，都擁有一個無法偽造的數位身份證，這將徹底改變我們對數位內容的信任方式。現在投資我們，您將站在AI安全的最前沿，共同打造一個更值得信賴的AI未來！", "audio": "docs/data/audios/2506.06018v1.wav"}
{"query": "AI", "id": "2506.06166v1", "url": "http://arxiv.org/abs/2506.06166v1", "title": "The Lock-in Hypothesis: Stagnation by Algorithm", "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com", "authors": ["Tianyi Alex Qiu", "Zhonghao He", "Tejasveer Chugh", "Max Kleiman-Weiner"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:25:05.955123", "title_zh": "鎖定假說：演算法造成的停滯", "summary_zh": "大型語言模型（LLM）的訓練和部署，會與使用者形成一種回饋迴路：模型從數據中學習人類的信念，透過生成內容強化這些信念，再吸收這些被強化的信念，然後反覆地回饋給使用者。這種動態類似於同溫層效應。我們假設這種回饋迴路會鞏固使用者現有的價值觀和信念，導致多樣性的喪失，並可能鎖定錯誤的信念。我們透過基於代理的LLM模擬和真實世界的GPT使用數據，對此假設進行了形式化並進行了實證檢驗。分析顯示，在新的GPT版本發布後，多樣性出現了突然但持續的下降，這與假設的人機回饋迴路一致。", "applications": ["新聞App總是推播你喜歡的新聞，讓你覺得世界就是你想的那樣，忽略了其他不同的聲音，長期下來，你可能變得更偏激。", "社群媒體的演算法只推薦你追蹤與你意見相似的人，讓你越來越難接觸到不同的觀點，導致同溫層效應越來越嚴重。", "孩子使用AI學習工具，但AI只根據過去的資料生成答案，可能讓孩子學到過時或有偏見的知識，阻礙他們的創新能力。"], "pitch": "各位創投先進，我們正處於AI革命的關鍵時刻，但一個潛在的危機正在浮現：AI正在將我們鎖死在過去的認知中。想像一下，如果未來的AI只能重複過去的觀點，創新將停滯，社會將分裂。我們的研究揭示了這個『鎖定假說』，並提供了應對方案。我們正在開發一種『AI多樣性引擎』，它能主動引入不同的觀點，打破同溫層效應，確保AI成為促進進步的力量，而不是阻礙。這不僅是一項技術，更是一項社會責任。投資我們，就是投資一個更開放、更具創新力的未來。我們預期在三年內，這項技術將成為所有大型語言模型的標準配置，並在教育、媒體、政策制定等領域產生深遠影響。未來的AI，不應該只是過去的鏡子，而應該是通往新世界的窗戶。加入我們，一起開啟這扇窗！", "audio": "docs/data/audios/2506.06166v1.wav"}
{"query": "Foundation Model", "id": "2506.06006v1", "url": "http://arxiv.org/abs/2506.06006v1", "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models", "summary": "To what extent do vision-and-language foundation models possess a realistic\nworld model (observation $\\times$ action $\\rightarrow$ observation) and a\ndynamics model (observation $\\times$ observation $\\rightarrow$ action), when\nactions are expressed through language? While open-source foundation models\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\nthrough supervision is significantly easier than acquiring a world model. In\nturn, dynamics models can be used to bootstrap world models through two main\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\ntime verification. Firstly, the dynamics model can annotate actions for\nunlabelled pairs of video frame observations to expand the training data. We\nfurther propose a new objective, where image tokens in observation pairs are\nweighted by their importance, as predicted by a recognition model. Secondly,\nthe dynamics models can assign rewards to multiple samples of the world model\nto score them, effectively guiding search at inference time. We evaluate the\nworld models resulting from both strategies through the task of action-centric\nimage editing on Aurora-Bench. Our best model achieves a performance\ncompetitive with state-of-the-art image editing models, improving on them by a\nmargin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and\nachieving the best average human evaluation across all subsets of Aurora-Bench.", "authors": ["Yifu Qiu", "Yftah Ziser", "Anna Korhonen", "Shay B. Cohen", "Edoardo M. Ponti"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:26:30.464573", "title_zh": "從多模態基礎模型中的動力學模型引導世界模型", "summary_zh": "本研究探討視覺與語言基礎模型是否具備真實的世界模型（觀察×行動→觀察）和動力學模型（觀察×觀察→行動）。研究發現，微調模型以獲得動力學模型比獲得世界模型更容易。進而，動力學模型可以透過合成數據的弱監督學習和推理時驗證來引導世界模型。首先，動力學模型可以為未標記的影片幀觀察對添加行動標籤，擴展訓練數據。其次，動力學模型可以為世界模型的多個樣本分配獎勵，對其進行評分，從而在推理時有效地引導搜尋。實驗結果顯示，該模型在Aurora-Bench上進行以行動為中心的圖像編輯任務時，性能與最先進的圖像編輯模型相媲美，在真實世界子集上的表現提高了15%。", "applications": ["想像一下，你可以用一句話，例如「把房間變成充滿陽光的沙灘」，然後這個AI就能自動幫你修改照片，讓你的房間看起來就像真的在沙灘上！這就像擁有了魔法PS高手。", "以後玩遊戲，AI能更聰明地理解你的指令。例如，你說「跳到最高的平台上」，AI就能預測你的角色需要如何移動和跳躍，讓遊戲體驗更流暢、更真實。", "在製造業，我們可以透過AI預測機器在不同操作下的反應。例如，輸入「提高機器速度」，AI就能預測機器零件的磨損情況，提前預防故障，降低維修成本。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能讓機器像人類一樣理解世界，並根據指令改變現實！我們的核心突破在於，我們發現了從動力學模型引導世界模型的有效方法，這讓AI能更準確地預測行動的後果。這項技術的潛力無窮，從圖像編輯、遊戲開發到工業自動化，都能帶來顛覆性的變革。想像一下，一個能根據你的想法創造圖像、控制機器人的AI，這將是一個數十億美元的市場！我們已經在Aurora-Bench基準測試中取得了令人矚目的成果，超越了現有的圖像編輯模型。現在，我們需要您的資金，將這項技術推向市場，成為AI領域的領導者。投資我們，就是投資未來！未來，每個人都可以是創作者，都可以用簡單的語言改變世界！", "audio": "docs/data/audios/2506.06006v1.wav"}
{"query": "Diffusion Model", "id": "2506.05960v1", "url": "http://arxiv.org/abs/2506.05960v1", "title": "AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models", "summary": "Significant investments have been made towards the commodification of\ndiffusion models for generation of diverse media. Their mass-market adoption is\nhowever still hobbled by the intense hardware resource requirements of\ndiffusion model inference. Model quantization strategies tailored specifically\ntowards diffusion models have been useful in easing this burden, yet have\ngenerally explored the Uniform Scalar Quantization (USQ) family of quantization\nmethods. In contrast, Vector Quantization (VQ) methods, which operate on groups\nof multiple related weights as the basic unit of compression, have seen\nsubstantial success in Large Language Model (LLM) quantization. In this work,\nwe apply codebook-based additive vector quantization to the problem of\ndiffusion model compression. Our resulting approach achieves a new Pareto\nfrontier for the extremely low-bit weight quantization on the standard\nclass-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.\nNotably, we report sFID 1.92 points lower than the full-precision model at W4A8\nand the best-reported results for FID, sFID and ISC at W2A8. We are also able\nto demonstrate FLOPs savings on arbitrary hardware via an efficient inference\nkernel, as opposed to savings resulting from small integer operations which may\nlack broad hardware support.", "authors": ["Adil Hasan", "Thomas Peyrin"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:27:56.686311", "title_zh": "AQUATIC-Diff：適用於極小壓縮擴散模型的加法量化", "summary_zh": "本研究針對擴散模型在硬體資源上的高需求問題，提出了一種名為AQUATIC-Diff的加法向量量化方法。不同於以往常用的均勻標量量化，此方法基於碼本，能更有效地壓縮模型，在極低位元量化下達到新的效能巔峰。在ImageNet的LDM-4基準測試中，W4A8設定下sFID值比全精度模型低1.92點，W2A8設定下FID、sFID和ISC指標均達到最佳。更重要的是，我們開發了高效的推論核心，能在各種硬體上實現FLOPs節省，擺脫了對特定硬體支援小整數運算的依賴。", "applications": ["**手機攝影美化：** 將這項技術應用於手機App中，即使是低階手機也能快速生成高品質、風格獨特的照片，讓每個人都能輕鬆成為攝影大師。", "**遊戲角色生成：** 遊戲開發者可以利用這項技術，快速生成大量獨一無二的遊戲角色，節省美術設計時間，並提供玩家更多樣化的選擇。", "**AI藝術創作：** 藝術家可以使用這項技術，在資源有限的設備上進行AI藝術創作，激發無限創意，並將藝術帶入更多人的生活。"], "pitch": "各位創投先進，我們正站在AI圖像生成革命的浪潮之巔！AQUATIC-Diff技術，如同為擴散模型裝上了火箭推進器，使其能在極低的硬體資源下運行，打破了過往高算力需求的瓶頸。想像一下，未來每一台手機都能運行複雜的AI圖像生成模型，人人都能隨時隨地創造獨一無二的內容。這不僅僅是技術突破，更是商業模式的巨大變革！我們可以將此技術授權給手機廠商、遊戲公司、甚至是元宇宙平台，收取授權費用；或者開發基於AQUATIC-Diff的雲端服務，提供更高效、更低成本的AI圖像生成解決方案。隨著元宇宙、NFT等領域的蓬勃發展，對AI圖像生成的需求將呈指數級增長，AQUATIC-Diff必將成為這場盛宴中最耀眼的明星，為各位帶來豐厚的回報！現在投資AQUATIC-Diff，就是投資AI圖像生成的未來！", "audio": "docs/data/audios/2506.05960v1.wav"}
{"query": "AI", "id": "2506.08006v1", "url": "http://arxiv.org/abs/2506.08006v1", "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:53:33.383616", "title_zh": "夢境樂園：結合模擬器與生成模型的可控世界創造", "summary_zh": "本研究提出「夢境樂園」，一個結合物理模擬器和生成模型的混合世界生成框架。它利用分層世界抽象，將像素級和物件級的語義與幾何資訊編碼為中間表示，連接模擬器和生成模型。這增強了可控性，透過與真實世界分佈的早期對齊，降低了適應成本，並支援現有和未來預訓練生成模型的直接使用。我們構建了D3Sim數據集，以促進混合生成管道的訓練和評估。實驗表明，「夢境樂園」在圖像質量上提升了50.8%，可控性增強了17.9%，並具有增強具身智能體訓練的巨大潛力。", "applications": ["遊戲開發者可以利用這項技術快速創建多樣且逼真的遊戲世界，並精確控制場景中的元素，例如調整物體的物理特性或改變環境光照，讓遊戲體驗更豐富。", "建築師和設計師可以創建虛擬的建築模型，並模擬不同天氣或光照條件下的效果，讓客戶在實際建造前就能身歷其境地體驗設計方案。", "電影製作人可以使用這項技術製作特效場景，例如創建逼真的自然災害或科幻世界，並精確控制場景中的每個細節，降低製作成本並提高效率。"], "pitch": "想像一下，我們正站在一個無限可能的起點。Dreamland不僅僅是一個技術突破，它是一個通往全新現實的鑰匙。它將徹底改變遊戲、娛樂、設計乃至AI訓練的未來。我們的混合框架，結合了物理模擬的精確控制與生成模型的逼真渲染，創造出前所未有的可控虛擬世界。這意味著更高效的遊戲開發、更具沉浸感的虛擬體驗，以及更強大的AI智能體。D3Sim數據集是我們的獨家優勢，能加速AI學習並提升性能。市場潜力巨大：遊戲產業對逼真場景的需求、建築設計對可視化效果的追求、AI訓練對大量數據的渴求，都將推動Dreamland的快速成長。我們正在打造的不僅是一個產品，而是一個平台，一個生態系統，一個全新的現實。現在加入我們，一起塑造這個未來，共享這份巨大的商業價值！", "audio": "docs/data/audios/2506.08006v1.wav"}
{"query": "Foundation Model", "id": "2506.07940v1", "url": "http://arxiv.org/abs/2506.07940v1", "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation", "summary": "Foundation model fine-tuning faces a fundamental challenge: existing AutoML\nplatforms rely on single optimisation strategies that explore only a fraction\nof viable hyperparameter configurations. In this white paper, We introduce\nGradients, a decentralised AutoML platform that transforms hyperparameter\noptimisation into a competitive marketplace where independent miners compete to\ndiscover optimal configurations. Economic incentives align individual\nexploration with collective optimisation goals, driving systematic\ninvestigation of hyperparameter regions that centralised methods miss. We\nevaluate our approach across 180 controlled experiments spanning diverse model\narchitectures (70M to 70B parameters) and task types. Gradients achieves an\n82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI,\nDatabricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\%\nrespectively. Complex reasoning and retrieval tasks show particularly strong\ngains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for\nperson-specific generation. These results demonstrate that competitive,\neconomically-driven approaches can systematically discover superior\nconfigurations that centralised AutoML consistently miss.", "authors": ["Christopher Subia-Waud"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:54:57.509488", "title_zh": "梯度：當市場遇上微調——一種模型優化的分散式方法", "summary_zh": "現有自動機器學習平台在微調大型模型時，往往受限於單一優化策略，無法充分探索所有可能的超參數組合。Gradients平台將超參數優化轉變為一個去中心化的競爭市場，讓獨立的「礦工」競相尋找最佳配置。經濟誘因驅動個人探索，並將其與集體優化目標對齊，從而系統性地挖掘中心化方法遺漏的超參數區域。實驗結果顯示，Gradients在多種模型架構和任務類型中，相較於其他平台，平均提升了11.8%至42.1%的性能，尤其在複雜推理和檢索任務以及個人化生成方面表現出色。這證明了基於經濟驅動的競爭方法，能有效發現卓越的配置。", "applications": ["想像一下，你是一位行銷人員，想為你的產品創建最吸引人的廣告文案。Gradients就像一個超級優化的廣告文案產生器，能自動找到最有效的詞語和風格，讓你的廣告點擊率飆升。", "如果你是一位醫生，想利用AI診斷罕見疾病。Gradients可以幫助你快速微調AI模型，使其能更準確地識別出疾病的細微特徵，提高診斷的準確性。", "假設你是一位遊戲開發者，想創造一個能根據玩家喜好自動調整難度的遊戲。Gradients可以幫助你優化遊戲AI，讓每個玩家都能享受到獨一無二、高度個人化的遊戲體驗。"], "pitch": "各位投資人，我們相信Gradients將徹底改變AI模型的微調方式。現今，微調過程耗時且昂貴，如同大海撈針。Gradients透過去中心化的市場機制，將這個過程轉變為高效、經濟的競賽。想像一下，一個能自我優化的AI生態系統，就像AI界的App Store，每天都在產生更強大、更精準的模型。這不僅能節省數百萬美元的成本，更能加速AI在各行各業的應用。我們預見，Gradients將成為AI基礎設施的關鍵組成部分，為各行各業提供更強大、更個人化的AI解決方案。投資Gradients，就是投資AI的未來，一個充滿無限可能的未來！", "audio": "docs/data/audios/2506.07940v1.wav"}
{"query": "Diffusion Model", "id": "2506.08013v1", "url": "http://arxiv.org/abs/2506.08013v1", "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:56:34.805569", "title_zh": "StableMTL：利用潛在擴散模型，從部分標註的合成數據集中進行多任務學習", "summary_zh": "這項研究提出StableMTL方法，利用擴散模型強大的泛化能力，在只有部分標註的合成數據集上訓練多任務模型，實現零樣本學習。StableMTL將圖像生成器用於潛在回歸，通過任務編碼、逐任務條件化和定制的訓練方案來調整去噪框架。它採用統一的潛在損失，無需仔細平衡各任務的損失，從而實現無縫擴展到更多任務。此外，引入了多流模型和任務注意力機制，將任務間的交互轉化為高效的單向注意力，促進跨任務共享。實驗證明，StableMTL在多個基準測試中優於其他方法。", "applications": ["智慧城市：利用路口監視器畫面，同時辨識車流量、行人數量、違規停車等，提升交通管理效率，並減少人力成本。", "醫療影像分析：從X光片或斷層掃描中，同時檢測多種疾病徵兆，例如腫瘤大小、骨折位置、炎症反應等，輔助醫生進行更精確的診斷。", "電商平台：自動分析商品圖片，同時提取商品屬性（顏色、材質、款式）和場景信息（室內、戶外），提升商品分類和搜尋的準確性，改善使用者體驗。"], "pitch": "想像一下，我們能用AI同時處理多項任務，而且只需要少量標註數據甚至完全不需要！StableMTL就是實現這個願景的關鍵。它像一個超級AI訓練師，能從各種模擬數據中學習，並將知識應用到真實世界。這不僅能大幅降低AI開發成本，還能開啟無限可能。例如，在自動駕駛領域，我們可以同時訓練AI識別交通號誌、行人、障礙物，大幅提升安全性。在醫療診斷方面，AI能同時分析多種病徵，協助醫生做出更精準的判斷。這項技術的潛在市場價值數十億美元，現在投資，就能搶佔AI多任務學習的先機，成為下一個AI獨角獸！", "audio": "docs/data/audios/2506.08013v1.wav"}
{"query": "AI", "id": "2506.07997v1", "url": "http://arxiv.org/abs/2506.07997v1", "title": "Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System", "summary": "The construction industry is characterized by both high physical and\npsychological risks, yet supports of mental health remain limited. While\nadvancements in artificial intelligence (AI), particularly large language\nmodels (LLMs), offer promising solutions, their potential in construction\nremains largely underexplored. To bridge this gap, we developed a\nconversational multi-agent system that addresses industry-specific challenges\nthrough an AI-driven approach integrated with domain knowledge. In parallel, it\nfulfills construction workers' basic psychological needs by enabling\ninteractions with multiple agents, each has a distinct persona. This approach\nensures that workers receive both practical problem-solving support and social\nengagement, ultimately contributing to their overall well-being. We evaluate\nits usability and effectiveness through a within-subjects user study with 12\nparticipants. The results show that our system significantly outperforms the\nsingle-agent baseline, achieving improvements of 18% in usability, 40% in\nself-determination, 60% in social presence, and 60% in trust. These findings\nhighlight the promise of LLM-driven AI systems in providing domain-specific\nsupport for construction workers.", "authors": ["Fan Yang", "Yuan Tian", "Jiansong Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:37:06.451208", "title_zh": "利用多代理人對話式AI系統支持建築工人的福祉", "summary_zh": "建築業面臨高 शारीरिक 與心理風險，但心理健康支持有限。本研究開發了一套多代理人對話式AI系統，結合領域知識，解決建築業的特定挑戰。系統透過與不同人格的代理人互動，滿足工人基本的心理需求，提供實際問題解決方案與社交互動，從而提升整體福祉。實驗結果顯示，相較於單一代理人系統，我們的系統在可用性、自主性、社交臨場感與信任度方面分別提升了18%、40%、60%與60%。這證明了大型語言模型驅動的AI系統在為建築工人提供領域特定支持方面的潛力。", "applications": ["工地裡，工人阿明心情不好，可以跟AI心理諮詢師聊聊，排解壓力，AI還能提醒他注意安全，避免工傷。", "老王是個水電工，遇到複雜的管線問題，可以問AI專家，AI會一步一步教他怎麼解決，省去查資料的時間。", "新來的工頭小李，對很多建材和工法不熟悉，可以隨時問AI老師，AI會提供相關知識和案例，幫助他快速上手。"], "pitch": "各位投資人，建築業長期面臨人力短缺、工安意外頻傳等問題，而我們的多代理人對話式AI系統，正是解決這些痛點的關鍵。它不僅能提升工人的心理健康與工作效率，更能降低工安事故的發生。想像一下，未來每個工地都配備這樣一套AI系統，它就像一位隨時待命的超級顧問，為工人提供全方位的支持。這將大幅提升建築業的生產力與安全性，創造巨大的商業價值。我們預計，這項技術將能應用於其他高風險行業，例如礦業、製造業等，市場潛力無限。現在投資我們，您將成為引領建築業AI革命的先驅！", "audio": "docs/data/audios/2506.07997v1.wav"}
{"query": "Foundation Model", "id": "2506.07886v1", "url": "http://arxiv.org/abs/2506.07886v1", "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:38:16.837702", "title_zh": "EgoM2P：以自我為中心的視角進行多模態多任務預訓練", "summary_zh": "本研究提出EgoM2P框架，旨在解決以自我為中心的視角下，如何有效理解多模態訊號的挑戰。EgoM2P利用時序感知的多模態tokens，透過遮蔽建模學習，訓練出一個通用的4D理解模型。此模型支援多種任務，包括眼球追蹤、以自我為中心的相機追蹤，以及從單眼視訊進行深度估計，甚至可以生成條件式的以自我為中心的視訊。EgoM2P在多項任務上達到或超越了專用模型的效果，且速度更快。我們將開源EgoM2P，以促進以自我為中心的視覺研究。", "applications": ["導航輔助：想像一下，戴上AR眼鏡，系統能根據你的視線和頭部動作，預測你的意圖，並在視野中即時顯示導航資訊，再也不用低頭看手機了。", "運動訓練：運動員佩戴設備後，系統能分析他們的動作、視線焦點和身體姿態，提供個人化的訓練建議，幫助他們提升表現，例如高爾夫揮桿或籃球投籃。", "遠端協作：工程師可以戴上頭戴式裝置，讓遠端的專家看到他們所看到的，並透過即時的視線追蹤和手勢識別，進行更有效的遠端指導和協作，減少錯誤和提高效率。"], "pitch": "各位投資人，我們正站在AIoT革命的風口浪尖！EgoM2P不僅僅是一個模型，它是一把解鎖未來人機互動的鑰匙。試想，透過我們的技術，AR/VR設備將變得更加智慧、更具沉浸感；機器人將能更精準地理解人類意圖，協作更加順暢；醫療領域，醫生可以透過AR眼鏡進行遠端手術指導，提升醫療水平。EgoM2P的潛力遠不止於此，它將成為元宇宙、智慧工廠、無人駕駛等領域的核心技術。我們正在打造一個全新的互動模式，一個以人為本的智慧世界。現在加入我們，共同開創這個千億級市場！", "audio": "docs/data/audios/2506.07886v1.wav"}
{"query": "Diffusion Model", "id": "2506.08009v1", "url": "http://arxiv.org/abs/2506.08009v1", "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:39:36.544597", "title_zh": "自我強制：彌合自迴歸影片擴散中的訓練-測試差距", "summary_zh": "本研究提出「自我強制」訓練方法，解決自迴歸影片擴散模型中長期存在的暴露偏差問題。傳統模型在訓練時依賴真實資料，但在實際應用時卻需根據自身產生的不完美結果生成影片。自我強制透過在訓練期間使用關鍵值（KV）快取進行自迴歸展開，讓模型根據先前自身生成的輸出產生每一幀，從而在影片層級進行整體性監督，直接評估整個生成序列的品質。此外，透過幾步擴散模型和隨機梯度截斷策略，兼顧計算成本和效能。實驗證明，此方法能在單一GPU上實現亞秒級延遲的即時串流影片生成，生成品質甚至超越速度較慢的非因果擴散模型。", "applications": ["想像一下，未來的線上遊戲！遊戲畫面不用事先算好，而是根據你的遊玩方式即時生成，每次玩都有獨一無二的體驗，就像真的身歷其境。", "假設你是個室內設計師，想讓客戶更快看到設計成果。現在只要輸入簡單的描述，就能即時生成不同風格的3D室內設計影片，快速溝通想法，大幅提升效率。", "如果醫院想用AI訓練醫生進行手術模擬，過去需要大量資源建立模型。現在利用這項技術，可以即時生成各種手術場景，讓醫生在逼真的環境下練習，提升手術成功率。"], "pitch": "各位投資人，我們正處於影片生成技術的革命性轉捩點！「自我強制」技術不僅解決了現有模型的瓶頸，更開創了即時、高品質影片生成的全新可能性。想像一下，未來影音內容的生產成本將大幅降低，個人化的互動式影片體驗將無處不在。從遊戲、娛樂、教育到醫療，各行各業都將因此受益。我們的技術擁有極高的商業價值，未來將能授權給各大影音平台、遊戲公司、教育機構，甚至能應用於元宇宙的內容生成。我們預計未來五年內，影片生成市場規模將達到數百億美元，而「自我強制」技術將在這個市場中佔據領先地位，為各位投資人帶來豐厚的回報！現在加入我們，一起打造影片生成的未來！", "audio": "docs/data/audios/2506.08009v1.wav"}
{"query": "AI", "id": "2506.07982v1", "url": "http://arxiv.org/abs/2506.07982v1", "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:29:23.275744", "title_zh": "τ²-Bench：在雙重控制環境中評估對話式代理", "summary_zh": "現有對話式AI代理的評估基準主要模擬單一控制環境，AI代理能使用工具與世界互動，使用者則是被動提供資訊。本研究提出τ²-Bench，模擬電信領域的雙重控制環境，代理和使用者都能使用工具在共享環境中操作。τ²-Bench包含：Dec-POMDP模型、可程式化任務生成器、可靠的使用者模擬器，以及細緻的代理效能分析。實驗顯示，從無使用者到雙重控制環境，代理效能顯著下降，突顯了引導使用者的挑戰。τ²-Bench為測試代理的推理能力和引導使用者行為的能力提供了一個可控的平台。", "applications": ["想像一下，以後打電話給電信客服，AI不只會幫你查帳單，還能一步步引導你設定數據漫遊，就像朋友一樣教你操作手機。", "家裡的智慧家電壞了，AI客服會引導你檢查電源、重啟設備，甚至教你簡單的故障排除，不用再盲目等待維修人員。", "使用複雜的軟體時，AI助手會像導航一樣，一步步引導你完成任務，再也不用擔心找不到功能或操作錯誤。"], "pitch": "各位投資人，我們正處於AI客服的轉型期！傳統AI只能被動回答問題，而我們的τ²-Bench技術，讓AI具備了引導使用者操作的能力，這將徹底改變人機互動模式。想像一下，未來電信公司、家電廠商、軟體開發商，都需要這種能主動引導使用者的AI客服。我們的技術不僅提升了客服效率，更降低了人力成本，潛在市場規模數十億美元！更重要的是，這項技術是AI邁向更複雜、更實用應用的關鍵一步。我們相信，τ²-Bench將成為AI客服領域的黃金標準，現在投資，您將站在AI革命的最前沿！", "audio": "docs/data/audios/2506.07982v1.wav"}
{"query": "Foundation Model", "id": "2506.07740v1", "url": "http://arxiv.org/abs/2506.07740v1", "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:30:39.914350", "title_zh": "Flow-Anything：從大規模單視角圖像學習真實世界光流估計", "summary_zh": "光流估計是電腦視覺的關鍵技術，是許多影片任務的基礎。Flow-Anything 提出一個大規模數據生成框架，從真實世界任何單視角圖像中學習光流估計。首先，利用單眼深度估計網路將單視角圖像轉換為3D表示，以便在虛擬相機下渲染光流和新視角圖像。其次，開發物件獨立體積渲染模組和深度感知修復模組，對3D表示中的動態物件進行建模。由此生成的 FA-Flow 數據集，首次展示了從大規模真實世界圖像生成光流訓練數據的優勢，超越了最先進的無監督和合成數據集上的監督方法。該模型可作為基礎模型，提升各種下游影片任務的效能。", "applications": ["**自動駕駛安全升級：** 想像一下，汽車能更精準地判斷行人或車輛的移動速度和方向，即使在光線不足或視線不佳的情況下，也能提前預警，大幅降低事故風險。", "**運動賽事分析：** 運動員的動作捕捉和分析變得更精準。例如，教練可以利用這項技術，精確分析籃球運動員的投籃姿勢，找出細微的改進空間，提升投籃命中率。", "**影視特效製作：** 製作更逼真的特效，例如模擬人群移動、火焰燃燒等。以往需要大量人工調整，現在可以透過 AI 自動生成，大幅降低製作成本，提升製作效率。"], "pitch": "想像一下，我們正在打造的是下一代影片理解引擎的核心！Flow-Anything 不僅僅是一個光流估計模型，它是一個從真實世界學習的強大基礎模型。這意味著，我們能夠以前所未有的精度理解、預測和生成影片內容。自動駕駛、智慧安防、運動分析、影視特效… 這些只是冰山一角！更令人興奮的是，我們正在探索將這項技術應用於元宇宙的內容生成，讓每個人都能輕鬆創造出逼真的虛擬世界。隨著影片數據爆炸式增長，對影片理解的需求將會持續攀升。Flow-Anything 將成為這場變革的關鍵推動力，潛在市場規模將達到數十億美元！現在加入我們，一起打造影片理解的未來！", "audio": "docs/data/audios/2506.07740v1.wav"}
{"query": "Diffusion Model", "id": "2506.08004v1", "url": "http://arxiv.org/abs/2506.08004v1", "title": "Dynamic View Synthesis as an Inverse Problem", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:32:02.092672", "title_zh": "動態視角合成作為一個逆問題", "summary_zh": "本研究將單眼影片的動態視角合成視為一個逆問題，並在無需訓練的環境下解決。透過重新設計預訓練視訊擴散模型的雜訊初始化階段，我們實現了高保真度的動態視角合成，無需任何權重更新或輔助模組。我們首先發現了零終端信噪比（SNR）排程對確定性反演造成根本障礙，並通過引入一種新的雜訊表示，稱為K階遞迴雜訊表示，來解決這個問題。我們推導出這種表示的閉合形式表達式，從而實現了VAE編碼和DDIM反演潛在變數之間的精確高效對齊。為了合成由相機運動產生嘅新可見區域，我們引入了隨機潛在調製，它在潛在空間上執行可見性感知採樣，以完成遮擋區域。綜合實驗表明，動態視角合成可以通過雜訊初始化階段中的結構化潛在操作有效地執行。", "applications": ["線上遊戲：玩家可以從任意角度觀看遊戲角色，提供更自由的遊戲體驗。", "虛擬實境旅遊：使用者可以透過現有影片，自由探索拍攝地點的各個角度，彷彿身歷其境。", "電影特效：在沒有額外拍攝的情況下，從不同的角度呈現爆炸、撞擊等特效畫面。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它能從普通影片中創造出令人驚豔的3D視角體驗。想像一下，你只需要一段手機影片，就能生成一個完整的3D模型，從任何角度觀看，這就是我們的技術能做到的。這項技術的應用潛力無窮，從遊戲、電影到電商、房地產，都能帶來顛覆性的改變。例如，電商平台可以讓消費者360度無死角地檢視商品，大大提升購買意願；房地產公司可以打造沉浸式的線上看房體驗，讓潛在買家足不出戶就能感受到房屋的空間感。更重要的是，我們的技術無需昂貴的硬體設備或複雜的訓練過程，成本效益極高。我們相信，這項技術將引領下一代視覺體驗的浪潮，成為元宇宙和虛擬實境領域的關鍵基礎設施。現在加入我們，一起打造視覺革命的未來！", "audio": "docs/data/audios/2506.08004v1.wav"}
{"query": "AI", "id": "2506.07957v1", "url": "http://arxiv.org/abs/2506.07957v1", "title": "Understanding the Error Sensitivity of Privacy-Aware Computing", "summary": "Homomorphic Encryption (HE) enables secure computation on encrypted data\nwithout decryption, allowing a great opportunity for privacy-preserving\ncomputation. In particular, domains such as healthcare, finance, and\ngovernment, where data privacy and security are of utmost importance, can\nbenefit from HE by enabling third-party computation and services on sensitive\ndata. In other words, HE constitutes the \"Holy Grail\" of cryptography: data\nremains encrypted all the time, being protected while in use.\n  HE's security guarantees rely on noise added to data to make relatively\nsimple problems computationally intractable. This error-centric intrinsic HE\nmechanism generates new challenges related to the fault tolerance and\nrobustness of HE itself: hardware- and software-induced errors during HE\noperation can easily evade traditional error detection and correction\nmechanisms, resulting in silent data corruption (SDC).\n  In this work, we motivate a thorough discussion regarding the sensitivity of\nHE applications to bit faults and provide a detailed error characterization\nstudy of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes\ndue to its fixed-point arithmetic support for AI and machine learning\napplications. We also delve into the impact of the residue number system (RNS)\nand the number theoretic transform (NTT), two widely adopted HE optimization\ntechniques, on CKKS' error sensitivity. To the best of our knowledge, this is\nthe first work that looks into the robustness and error sensitivity of\nhomomorphic encryption and, as such, it can pave the way for critical future\nwork in this area.", "authors": ["Matías Mazzanti", "Esteban Mocskos", "Augusto Vega", "Pradip Bose"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:53:58.454764", "title_zh": "理解隱私感知運算的錯誤敏感性", "summary_zh": "同態加密（HE）允許在加密數據上進行安全運算，無需解密，為保護隱私的運算提供了絕佳機會。醫療、金融和政府等對數據隱私和安全極為重要的領域，可以透過HE在敏感數據上實現第三方運算和服務。HE的安全性依賴於添加到數據中的雜訊，使相對簡單的問題在計算上變得難以處理。然而，這種以錯誤為中心的機制也帶來了新的挑戰：HE運算期間的硬體和軟體錯誤容易避開傳統的錯誤檢測和校正機制，導致靜默數據損壞（SDC）。本研究深入探討HE應用對位元錯誤的敏感性，並詳細分析了CKKS方案的錯誤特性，同時也研究了餘數系統（RNS）和數論變換（NTT）對CKKS錯誤敏感性的影響。這項研究首次關注同態加密的穩健性和錯誤敏感性，為該領域的未來重要研究奠定了基礎。", "applications": ["想像一下，醫院可以利用這項技術，在不洩露病人隱私的情況下，讓AI分析病人的病歷，找出潛在的疾病風險，及早預防。", "銀行可以使用這項技術，在加密的狀態下，分析客戶的交易數據，判斷是否有詐欺行為，同時保護客戶的財務隱私。", "政府可以利用這項技術，在不公開個人資訊的情況下，統計人口數據，制定更完善的公共政策。"], "pitch": "各位創投先進，我們正在打造的是數據安全的新紀元！同態加密技術，如同為數據穿上了一層隱形盔甲，讓數據在被使用的同時，依然保持加密狀態。試想一下，在醫療、金融、國防等領域，有多少數據因為安全疑慮而無法被充分利用？我們的研究，正是要提升同態加密的穩定性，讓這項技術真正落地應用。未來，我們將開發針對不同產業的同態加密解決方案，例如：AI模型可以在加密的醫療數據上進行訓練，藥廠可以在不洩露配方的情況下進行研發合作，甚至可以打造完全匿名的區塊鏈應用。這不僅僅是一項技術，更是一個龐大的市場，一個重新定義數據價值的機會！現在加入我們，一起開啟數據安全的新篇章！", "audio": "docs/data/audios/2506.07957v1.wav"}
{"query": "Foundation Model", "id": "2506.07647v1", "url": "http://arxiv.org/abs/2506.07647v1", "title": "Foundation Model Empowered Synesthesia of Machines (SoM): AI-native Intelligent Multi-Modal Sensing-Communication Integration", "summary": "To support future intelligent multifunctional sixth-generation (6G) wireless\ncommunication networks, Synesthesia of Machines (SoM) is proposed as a novel\nparadigm for artificial intelligence (AI)-native intelligent multi-modal\nsensing-communication integration. However, existing SoM system designs rely on\ntask-specific AI models and face challenges such as scarcity of massive\nhigh-quality datasets, constrained modeling capability, poor generalization,\nand limited universality. Recently, foundation models (FMs) have emerged as a\nnew deep learning paradigm and have been preliminarily applied to SoM-related\ntasks, but a systematic design framework is still lacking. In this paper, we\nfor the first time present a systematic categorization of FMs for SoM system\ndesign, dividing them into general-purpose FMs, specifically large language\nmodels (LLMs), and SoM domain-specific FMs, referred to as wireless foundation\nmodels. Furthermore, we derive key characteristics of FMs in addressing\nexisting challenges in SoM systems and propose two corresponding roadmaps,\ni.e., LLM-based and wireless foundation model-based design. For each roadmap,\nwe provide a framework containing key design steps as a guiding pipeline and\nseveral representative case studies of FM-empowered SoM system design.\nSpecifically, we propose LLM-based path loss generation (LLM4PG) and scatterer\ngeneration (LLM4SG) schemes, and wireless channel foundation model (WiCo) for\nSoM mechanism exploration, LLM-based wireless multi-task SoM transceiver\n(LLM4WM) and wireless foundation model (WiFo) for SoM-enhanced transceiver\ndesign, and wireless cooperative perception foundation model (WiPo) for\nSoM-enhanced cooperative perception, demonstrating the significant superiority\nof FMs over task-specific models. Finally, we summarize and highlight potential\ndirections for future research.", "authors": ["Xiang Cheng", "Boxun Liu", "Xuanyu Liu", "Ensong Liu", "Ziwei Huang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:55:21.591233", "title_zh": "基於基礎模型的機器聯覺(SoM)：AI原生智能多模態感知-通信集成", "summary_zh": "本研究提出基於基礎模型的機器聯覺(SoM)，旨在為未來的6G無線通信網絡提供AI原生的智能多模態感知與通信集成。現有SoM系統依賴於特定任務的AI模型，面臨數據稀缺、建模能力受限和泛化性差等挑戰。本研究系統性地對SoM系統設計的基礎模型進行分類，並提出基於大型語言模型(LLM)和無線基礎模型的兩種設計路線圖。通過案例研究，證明基礎模型在SoM機制探索、收發器設計和協同感知方面優於特定任務模型。這為未來無線通信的智能化開闢了新的方向。", "applications": ["想像一下，未來的無人機送貨更聰明！它們不僅能看到障礙物，還能『聽到』風的聲音，預測氣流變化，自動調整飛行路線，保證包裹安全準時送達。", "未來的智慧交通系統，車輛之間不僅能交換位置和速度信息，還能『感知』到前方道路的濕滑程度，提前提醒駕駛員減速，避免交通事故。", "在工廠裡，機器人不僅能看到零件的位置，還能『聽到』機器運轉的異常聲音，及早發現故障，避免生產線停工。"], "pitch": "各位投資人，我們正處於AI與無線通信融合的革命性時刻！傳統無線通信依賴人工設計，效率低下且難以適應複雜環境。我們的SoM技術，利用基礎模型，讓機器擁有『聯覺』能力，像人類一樣綜合運用多種感官信息，實現更智能、更可靠的無線通信。這不僅能大幅提升現有無線網絡的性能，更將催生全新的應用場景，例如：無人駕駛、智慧城市、工業互聯網等。試想一下，一個完全由AI驅動的無線世界，將帶來怎樣的巨大商業價值？我們的團隊擁有深厚的AI和無線通信背景，我們有信心將SoM技術打造成為下一代無線通信的基石，成為引領行業變革的領頭羊。現在加入我們，您將擁抱一個千億美元級別的市場，共同開啟AI賦能無線通信的黃金時代！", "audio": "docs/data/audios/2506.07647v1.wav"}
{"query": "Diffusion Model", "id": "2506.07999v1", "url": "http://arxiv.org/abs/2506.07999v1", "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:56:32.481289", "title_zh": "MADFormer：用於連續圖像生成的混合自迴歸與擴散轉換器", "summary_zh": "MADFormer結合了自迴歸（AR）和擴散模型，旨在優化圖像生成。它將圖像分成空間區塊，利用AR層進行全局條件設定，再用擴散層在各區塊內進行迭代局部細化。實驗證明，這種分區方式能有效提升高解析度圖像的生成品質，垂直混合AR和擴散層能在運算資源有限的情況下，顯著改善生成品質。MADFormer的研究為未來混合生成模型提供了實用的設計原則，展現了在效率和品質之間取得平衡的潛力。", "applications": ["客製化頭像生成：使用者可以輸入少量個人照片，系統就能生成風格多樣、高解析度的個人頭像，用於社交媒體或遊戲。", "老照片修復：將模糊或損壞的老照片透過AI修復，還原清晰細節，讓珍貴回憶重現。", "室內設計預覽：輸入房屋格局和個人喜好，AI就能生成不同風格的室內設計方案，讓使用者在裝修前預覽效果。"], "pitch": "各位創投夥伴，我們團隊帶來的是MADFormer，一項突破性的圖像生成技術，它巧妙結合了自迴歸和擴散模型，在高解析度圖像生成領域實現了質的飛躍。想像一下，未來遊戲、電影、廣告等產業，對高質量、客製化圖像的需求將會爆炸性增長。MADFormer能以更低的成本、更快的速度，生成以往難以想像的圖像內容。這不僅能大幅降低製作成本，更能激發無限的創意潛能。我們的技術已經在FFHQ-1024和ImageNet等數據集上驗證，效果顯著。我們預計，MADFormer將成為下一代圖像生成引擎的核心技術，佔領市場制高點。現在投資，您將成為這場圖像革命的早期參與者，共同分享百億美元級別的市場紅利！", "audio": "docs/data/audios/2506.07999v1.wav"}
{"query": "AI", "id": "2506.07955v1", "url": "http://arxiv.org/abs/2506.07955v1", "title": "Implementation Considerations for Automated AI Grading of Student Work", "summary": "This study explores the classroom implementation of an AI-powered grading\nplatform in K-12 settings through a co-design pilot with 19 teachers. We\ncombine platform usage logs, surveys, and qualitative interviews to examine how\nteachers use AI-generated rubrics and grading feedback. Findings reveal that\nwhile teachers valued the AI's rapid narrative feedback for formative purposes,\nthey distrusted automated scoring and emphasized the need for human oversight.\nStudents welcomed fast, revision-oriented feedback but remained skeptical of\nAI-only grading. We discuss implications for the design of trustworthy,\nteacher-centered AI assessment tools that enhance feedback while preserving\npedagogical agency.", "authors": ["Zewei", "Tian", "Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Zachary Zhang", "Kevin He", "Min Sun"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:28:11.077238", "title_zh": "學生作業自動AI評分之實施考量", "summary_zh": "本研究與19位中小學教師合作，共同設計並試用AI評分平台。透過平台使用紀錄、問卷和訪談，我們發現教師們重視AI快速生成敘述性回饋，有助於形成性評估，但對自動評分的信任度較低，強調人工監督的必要性。學生們歡迎快速且針對修改建議的回饋，但對完全由AI評分仍持懷疑態度。研究結果強調，設計值得信賴、以教師為中心的AI評估工具至關重要，這類工具應能增強回饋效果，同時保留教學自主性。", "applications": ["想像一下，以後老師改作業不用改到天昏地暗，AI可以先幫忙找出重點，老師再針對學生的個別狀況給予指導，這樣老師就有更多時間可以關心每個學生的學習進度。", "學生寫作文，AI可以馬上給予修改建議，像是用字遣詞、文法結構等等，學生可以即時修改，不用等到老師改完才能知道哪裡需要改進，學習效率更高。", "公司新人訓練時，AI可以自動評估新人的學習狀況，並提供客製化的學習建議，幫助新人更快上手，企業也能更有效率地培訓人才。"], "pitch": "各位投資人，想像一下，未來教育的樣貌將會因為AI而徹底改變！我們的AI評分平台，不僅能大幅減輕老師的負擔，更能提供學生更即時、更個人化的學習回饋。這不僅僅是一個評分工具，更是一個提升整體教育品質的革命性產品。目前市場上缺乏真正能與教師協作、值得信賴的AI評分解決方案，而我們正好填補了這個缺口。未來，我們可以將這項技術應用於各個領域，例如企業培訓、線上課程、甚至個人技能提升。透過AI的協助，學習將變得更有效率、更具互動性。現在投資我們，您將參與一場教育科技的變革，共同打造一個更智慧、更高效的學習未來！", "audio": "docs/data/audios/2506.07955v1.wav"}
{"query": "Foundation Model", "id": "2506.07603v1", "url": "http://arxiv.org/abs/2506.07603v1", "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:29:37.081905", "title_zh": "SurgBench：用於手術影片分析的統一大型基準", "summary_zh": "手術影片理解對於手術自動決策、技能評估和術後品質改善至關重要。SurgBench是一個統一的手術影片基準框架，包含一個預訓練資料集SurgBench-P和一個評估基準SurgBench-E。SurgBench-P涵蓋22種手術和11個專科的5300萬幀畫面，SurgBench-E則在六個類別（階段分類、鏡頭運動、工具識別、疾病診斷、動作分類和器官檢測）中提供72個細粒度任務的評估。實驗表明，現有的影片基礎模型難以在不同的手術影片分析任務中推廣，而基於SurgBench-P的預訓練可以顯著提高性能，並提供對未見過的手術和模態的卓越跨域泛化能力。", "applications": ["想像一下，醫生可以透過AI分析手術錄影，即時獲得建議，就像開車時有導航一樣，降低手術風險，提升成功率。", "我們可以利用AI分析手術影片，客觀評估醫生的手術技巧，就像運動員透過數據分析提升表現一樣，幫助醫生精進技術。", "未來，透過分析大量手術影片，AI可以協助開發更精準的醫療器材和手術流程，就像汽車工程師透過碰撞測試改善車輛安全一樣，提升整體醫療水平。"], "pitch": "各位投資人，我們正在打造手術影片分析的「ImageNet」時刻！SurgBench不僅是個資料集，更是推動手術AI發展的引擎。想像一下，AI能像資深外科醫生一樣，即時判斷手術階段、辨識工具、甚至診斷疾病，這將徹底改變手術室的運作模式。透過SurgBench，我們可以開發出：1. 手術導航系統：協助醫生進行更精準、安全的手術；2. 遠程手術協作平台：讓專家醫生能遠端指導手術，解決醫療資源不均的問題；3. 個性化手術訓練系統：根據醫生的能力，提供客製化的訓練方案。市場潛力巨大！全球手術器材和服務市場規模數千億美元，而手術AI的加入，將帶來指數級的增長。我們預計，SurgBench將成為手術AI領域的黃金標準，吸引無數開發者和研究人員投入，共同打造更智慧、更高效、更安全的醫療未來。現在加入，您將站在這場醫療革命的最前沿！", "audio": "docs/data/audios/2506.07603v1.wav"}
{"query": "Diffusion Model", "id": "2506.07998v1", "url": "http://arxiv.org/abs/2506.07998v1", "title": "Generative Modeling of Weights: Generalization or Memorization?", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:31:10.048997", "title_zh": "權重的生成模型：泛化還是記憶？", "summary_zh": "近年來，生成模型在圖像和影片生成方面取得了成功，因此有人開始探索使用它們來合成有效的神經網路權重。這些方法將訓練好的神經網路檢查點作為訓練數據，並旨在於推論過程中生成高性能的神經網路權重。然而，我們的研究發現，這些方法在很大程度上是通過記憶來合成權重的，它們產生的權重要么是訓練檢查點的複製品，要么只是簡單的插值。目前的這些方法在獲得不同且高性能的模型方面，並不能優於簡單的基準方法，例如在權重中添加噪音或採取簡單的權重集成。更令人驚訝的是，即使修改與圖像擴散模型中記憶相關的建模因素或應用數據增強，也無法有效緩解這種記憶現象。這項研究提醒我們，需要更謹慎地評估生成模型在新領域的應用能力，並對當前生成模型可以建模的數據類型進行更實際的評估。", "applications": ["AI藝術風格轉換：如果能真正生成新的模型權重，就能創造出前所未見的藝術風格，讓使用者輕鬆生成獨一無二的藝術作品。", "個性化醫療診斷：針對不同患者的基因數據，生成專屬的AI診斷模型，提供更精準的醫療建議，避免誤診。", "自動化程式碼優化：針對不同的程式碼結構，生成最佳化的編譯器權重，提升程式執行效率，讓App運行更順暢。"], "pitch": "各位創投，我們正在開發一種革命性的AI權重生成技術，它將徹底顛覆現有的AI模型訓練方式。雖然目前的研究顯示現有方法存在記憶問題，但這也代表著巨大的突破機會！想像一下，我們不再需要耗費大量資源從頭訓練模型，而是透過生成模型，快速產生針對特定任務的最佳化權重。這將大幅降低AI開發成本，加速AI應用落地。初期，我們可以聚焦在利基市場，例如個性化醫療、金融風險評估等，透過生成專屬模型，提供更精準的服務。未來，隨著技術成熟，我們更可以打造一個AI權重交易平台，讓AI開發者可以自由交易、組合、再生成新的權重，形成一個蓬勃發展的AI生態系。這不僅僅是一項技術，更是一個全新的商業模式，一個千億美元級的市場！現在加入我們，一起開創AI的下一個黃金時代！", "audio": "docs/data/audios/2506.07998v1.wav"}
{"query": "AI", "id": "2506.07949v1", "url": "http://arxiv.org/abs/2506.07949v1", "title": "Cost-Optimal Active AI Model Evaluation", "summary": "The development lifecycle of generative AI systems requires continual\nevaluation, data acquisition, and annotation, which is costly in both resources\nand time. In practice, rapid iteration often makes it necessary to rely on\nsynthetic annotation data because of the low cost, despite the potential for\nsubstantial bias. In this paper, we develop novel, cost-aware methods for\nactively balancing the use of a cheap, but often inaccurate, weak rater -- such\nas a model-based autorater that is designed to automatically assess the quality\nof generated content -- with a more expensive, but also more accurate, strong\nrater alternative such as a human. More specifically, the goal of our approach\nis to produce a low variance, unbiased estimate of the mean of the target\n\"strong\" rating, subject to some total annotation budget. Building on recent\nwork in active and prediction-powered statistical inference, we derive a family\nof cost-optimal policies for allocating a given annotation budget between weak\nand strong raters so as to maximize statistical efficiency. Using synthetic and\nreal-world data, we empirically characterize the conditions under which these\npolicies yield improvements over prior methods. We find that, especially in\ntasks where there is high variability in the difficulty of examples, our\npolicies can achieve the same estimation precision at a far lower total\nannotation budget than standard evaluation methods.", "authors": ["Anastasios N. Angelopoulos", "Jacob Eisenstein", "Jonathan Berant", "Alekh Agarwal", "Adam Fisch"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:36:20.593723", "title_zh": "成本最佳化的主動式AI模型評估", "summary_zh": "生成式AI系統的開發需要持續評估、資料收集和標註，這些都非常耗時且昂貴。為了加速迭代，開發者常使用成本較低的合成標註資料，但這可能導致偏差。本研究開發了一種成本敏感的方法，能主動平衡低成本但可能不準確的弱評估者（例如基於模型的自動評估器）和更昂貴但更準確的強評估者（例如人類）的使用。我們的目標是在有限的標註預算下，產生目標「強」評級的低變異數、無偏估計。我們利用主動和預測驅動的統計推斷，推導出一系列成本最佳化策略，以最大化統計效率的方式在弱評估者和強評估者之間分配標註預算。實驗結果表明，尤其是在範例難度差異很大的任務中，我們的策略能以遠低於標準評估方法的總標註預算，實現相同的估計精度。", "applications": ["線上購物評論分類：自動分析大量商品評論，判斷哪些評論需要人工審核以確保真實性和準確性，例如過濾機器人產生的假評論，節省人力成本。", "醫療影像診斷輔助：AI初步判讀X光片，將可疑病例優先交由醫生檢查，提高診斷效率並減輕醫生工作負擔。", "自動客服品質監控：AI自動評估客服人員的回答品質，僅將評分較低的對話轉交人工審核，確保服務品質並降低人工監控成本。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它將徹底改變AI模型的評估方式。想像一下，AI模型就像一個學生，需要不斷考試來提升能力。傳統的考試（人工評估）非常昂貴且耗時。我們的技術就像一個AI助教，能夠快速且低成本地進行初步評估，只有少數困難的題目才需要教授（人工專家）親自批改。這意味著，我們可以更快、更經濟地訓練出更強大的AI模型。市場潛力巨大！任何需要高品質AI模型的行業，如自動駕駛、金融風控、醫療診斷等，都將受益於我們的技術。我們預計，隨著AI模型越來越複雜，對高效評估的需求將呈指數級增長。我們的技術不僅能降低成本，還能加速AI的發展進程，搶佔市場先機。我們正在尋找有遠見的投資人，共同打造AI評估的新標準，開創一個AI驅動的未來！", "audio": "docs/data/audios/2506.07949v1.wav"}
{"query": "Foundation Model", "id": "2506.07584v1", "url": "http://arxiv.org/abs/2506.07584v1", "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data", "summary": "A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.", "authors": ["Hao Li", "Bowen Deng", "Chang Xu", "Zhiyuan Feng", "Viktor Schlegel", "Yu-Hao Huang", "Yizheng Sun", "Jingyuan Sun", "Kailai Yang", "Yiyao Yu", "Jiang Bian"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:37:42.168037", "title_zh": "MIRA：用於真實世界健康數據的醫療時間序列基礎模型", "summary_zh": "MIRA是一個專為醫療時間序列預測設計的基礎模型。它利用連續時間旋轉位置編碼精準建模時間間隔，透過頻率特定的專家混合層促進時間特化，並基於神經常微分方程的連續動態外推區塊，在任意目標時間戳上進行準確預測。MIRA在大型醫療數據集上預訓練，包含超過4540億個時間點。相較於其他模型，MIRA在分佈外和分佈內情境下，分別平均降低了10%和7%的預測誤差。我們也建立了一個全面的基準，為未來醫療時間序列建模研究奠定基礎。", "applications": ["**個人健康追蹤：** 想像一下，你的智慧手錶不只記錄心率，還能預測你未來幾小時的血壓變化，提前預警心血管風險，讓你及早採取行動。", "**醫院資源調度：** 醫院可以利用MIRA預測未來急診室的病患流量，提前調配醫護人員和床位，避免醫療資源擠兌，提升整體效率。", "**新藥開發：** 藥廠可以利用MIRA分析臨床試驗數據，更準確地預測藥物療效，加速新藥開發進程，讓更多病患受益。"], "pitch": "各位投資人，我們正處於醫療AI的黃金時代！MIRA不僅僅是一個模型，它是一個醫療時間序列的革命性突破。試想，一個能夠精準預測病患健康狀況、優化醫院運營、加速新藥開發的AI引擎，它的市場潛力有多大？現有的醫療數據分析方法往往受限於數據量和複雜性，而MIRA透過其獨特的架構，克服了這些挑戰，實現了跨機構、跨模態的數據整合和知識遷移。這意味著，即使在數據稀缺或隱私受限的環境下，MIRA也能發揮卓越的預測能力。我們相信，MIRA將成為醫療AI領域的Game Changer，引領醫療健康產業進入一個預測性、個性化的新時代。現在投資MIRA，您將搶佔先機，共同打造一個更健康、更智慧的未來！想像一下，未來的醫療決策不再依賴於事後分析，而是基於精準的預測和預防，這將為整個社會節省巨大的醫療成本，並提升人們的生活品質。MIRA的商業價值遠不止於此，它還可以應用於保險精算、健康管理、遠程醫療等眾多領域，創造無限的可能性。我們誠摯邀請您加入我們的行列，共同見證MIRA的輝煌前景！", "audio": "docs/data/audios/2506.07584v1.wav"}
{"query": "Diffusion Model", "id": "2506.07986v1", "url": "http://arxiv.org/abs/2506.07986v1", "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:39:04.324764", "title_zh": "重新思考多模態擴散轉換器中的跨模態互動", "summary_zh": "多模態擴散轉換器(MM-DiT)在文字驅動的視覺生成方面取得了顯著進展，但現有模型在文字提示與生成內容的精確對齊方面仍存在挑戰。本研究發現MM-DiT的注意力機制存在兩個關鍵問題：視覺和文字模態之間的token不平衡導致跨模態注意力受到抑制，以及缺乏時間步感知的注意力權重，進而阻礙對齊。為了解決這些問題，我們提出一種名為「溫度調整跨模態注意力(TACA)」的參數高效方法，通過溫度縮放和時間步相關調整來動態地重新平衡多模態互動。結合LoRA微調，TACA顯著提升了T2I-CompBench基準測試上的文本-圖像對齊效果，且計算開銷極小。經驗證，TACA能有效改善圖像-文本對齊。", "applications": ["想像一下，你可以用一段文字描述你想要的房間裝潢風格，例如『簡約北歐風，陽光灑落的客廳』，AI就能自動生成符合你描述的3D模型，讓你提前預覽裝潢效果。", "假設你是服裝設計師，只要輸入『一件具有未來感的銀色外套，搭配不對稱剪裁』，AI就能快速生成多種設計草圖，激發你的創作靈感，省去大量手繪時間。", "如果你是社群媒體小編，需要快速製作吸睛的廣告素材，只要輸入產品描述和想要的風格，AI就能自動生成高品質的產品圖片，讓你的廣告更具吸引力。"], "pitch": "各位投資人，我們正站在AI圖像生成革命的浪潮之巔！我們的技術TACA，能讓AI更精準地理解文字指令，生成更符合需求的圖像，解決了目前AI圖像生成領域最核心的痛點。這意味著什麼？想像一下，未來設計師、行銷人員、甚至一般消費者，都能輕鬆地將腦海中的想法轉化為視覺圖像，創造無限可能！從客製化產品設計、虛擬實境內容生成，到電影特效製作，TACA的應用前景無可限量。我們相信，TACA將成為下一代AI圖像生成引擎的核心技術，引領市場走向更精準、更個性化的圖像生成時代。現在加入我們，一起打造AI圖像生成的未來，贏取豐厚的回報！", "audio": "docs/data/audios/2506.07986v1.wav"}
{"query": "AI", "id": "2506.07907v1", "url": "http://arxiv.org/abs/2506.07907v1", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between between the\ndecay amplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De~Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:25:46.104394", "title_zh": "利用C-偶與C-奇量子關聯的D D̄介子對，對D⁰→K⁻π⁺和D̄⁰→K⁻π⁺衰變間強相位差的新穎測量", "summary_zh": "本研究提出一種測量D⁰和D̄⁰介子衰變振幅之間強相位差的新穎技術，利用在ψ(3770)產生閾值以上能量的e⁺e⁻碰撞中產生的量子關聯D D̄介子對。這些介子對產生於電荷共軛對稱性的偶數和奇數本徵態。利用此技術，首次使用此類數據樣本確定了D⁰-D̄⁰的相對強相位。使用BESIII實驗收集的積分光度為7.13 fb⁻¹，質心能量在4.13-4.23 GeV之間的數據集，測得D⁰→K⁻π⁺和D̄⁰→K⁻π⁺衰變之間的強相位差δDKπ為(192.8⁺¹¹.⁰ ⁺¹.⁹₋₁₂.₄ ₋₂.₄)°。", "applications": ["想像一下，我們可以更精準地分析醫療影像。這項技術就像是幫醫生配備了更銳利的眼睛，能更早發現病灶，例如極早期的癌症，提高治癒率。", "在材料科學領域，這項技術能幫助我們更深入了解新材料的微觀結構。這就像是擁有一台超強顯微鏡，讓我們能設計出更堅固、更輕便、更高效能的材料，應用於航空、汽車等產業。", "在通訊加密方面，如果能精準掌握基本粒子的相位，就能開發出更難破解的加密技術。這就像是打造了一道堅不可摧的防火牆，保護我們的數位資訊安全。"], "pitch": "各位投資人，我們正在開發一項革命性的量子測量技術，它能精準掌握亞原子粒子的微妙特性，其潛力遠超想像！目前我們已成功驗證了該技術在D介子衰變上的應用，未來將可拓展至醫療、材料、資安等領域。試想，更精準的醫療診斷、更強大的新材料、以及牢不可破的加密技術，都將因為這項技術而成為可能。這不僅是一項科學突破，更是一座蘊藏無限商機的金礦！我們需要您的資金支持，共同引領這場科技革命，開創量子科技的新紀元！現在投資，您將成為這項劃時代技術的早期投資者，共享豐碩的成果。", "audio": "docs/data/audios/2506.07907v1.wav"}
{"query": "Foundation Model", "id": "2506.07576v1", "url": "http://arxiv.org/abs/2506.07576v1", "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding", "summary": "Video understanding has been considered as one critical step towards world\nmodeling, which is an important long-term problem in AI research. Recently,\nmulti-modal foundation models have shown such potential via large-scale\npretraining. However, these models simply align encoders of different\nmodalities via contrastive learning, while lacking deeper multi-modal\ninteractions, which is critical for understanding complex target movements with\ndiversified video scenes. To fill this gap, we propose a unified Super Encoding\nNetwork (SEN) for video understanding, which builds up such distinct\ninteractions through recursive association of multi-modal encoders in the\nfoundation models. Specifically, we creatively treat those well-trained\nencoders as \"super neurons\" in our SEN. Via designing a Recursive Association\n(RA) block, we progressively fuse multi-modalities with the input video, based\non knowledge integrating, distributing, and prompting of super neurons in a\nrecursive manner. In this way, our SEN can effectively encode deeper\nmulti-modal interactions, for prompting various video understanding tasks in\ndownstream. Extensive experiments show that, our SEN can remarkably boost the\nfour most representative video tasks, including tracking, recognition,\nchatting, and editing, e.g., for pixel-level tracking, the average jaccard\nindex improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular\nCaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,\nand frame consistency increases 4.1% compared to the popular TuneA-Video\napproach.", "authors": ["Boyu Chen", "Siran Chen", "Kunchang Li", "Qinglin Xu", "Yu Qiao", "Yali Wang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:26:56.691958", "title_zh": "超級編碼網路：用於影片理解的多模態編碼器之遞迴關聯", "summary_zh": "本研究提出一個名為「超級編碼網路」(SEN) 的新型影片理解架構，旨在提升多模態基礎模型在理解複雜影片場景中的能力。SEN 將預訓練的編碼器視為「超級神經元」，並透過遞迴關聯(RA)模組，逐步融合多模態資訊。RA模組基於知識整合、分配和提示，以遞迴方式將多模態資訊與輸入影片融合。實驗證明，SEN 能顯著提升影片追蹤、辨識、聊天和編輯等任務的效能，例如，在像素級追蹤上，Jaccard 指數平均提高 2.7%，時間一致性降低 8.8%。", "applications": ["智慧監控：SEN 可以讓監視器更精準地追蹤畫面中的人物或物體，即使在複雜的環境中也能有效辨識異常行為，例如跌倒、打架等，及時發出警報。", "自動駕駛：透過分析車載鏡頭拍攝的影片，SEN 可以幫助自動駕駛系統更準確地理解路況，例如辨識行人、車輛、交通號誌等，做出更安全的決策。", "影音創作：SEN 可以應用於影片編輯軟體中，讓使用者更輕鬆地進行影片剪輯、特效添加等操作，例如自動為影片添加字幕、根據影片內容推薦合適的背景音樂等。"], "pitch": "各位投資人，我們正在開發一種革命性的影片理解技術，它將徹底改變AI對影片內容的理解方式。想像一下，未來的影片分析不再只是簡單的物件辨識，而是能像人類一樣理解影片的上下文、情感和意圖。我們的「超級編碼網路」正是實現這一目標的關鍵。它能讓AI具備更強大的影片分析能力，應用範圍極其廣泛。從智慧安防、自動駕駛，到影音娛樂、教育培訓，甚至是醫療診斷，只要涉及影片內容的理解，我們的技術就能創造巨大的價值。我們相信，隨著5G、AIoT等技術的快速發展，影片數據將呈現爆炸式增長。而我們所掌握的影片理解核心技術，將使我們在這個巨大的市場中佔據領先地位。現在加入我們，一起開創影片理解的新時代！", "audio": "docs/data/audios/2506.07576v1.wav"}
{"query": "Diffusion Model", "id": "2506.07923v1", "url": "http://arxiv.org/abs/2506.07923v1", "title": "Efficient Seismic Data Interpolation via Sparse Attention Transformer and Diffusion Model", "summary": "Seismic data interpolation is a critical pre-processing step for improving\nseismic imaging quality and remains a focus of academic innovation. To address\nthe computational inefficiencies caused by extensive iterative resampling in\ncurrent plug-and-play diffusion interpolation methods, we propose the\ndiffusion-enhanced sparse attention transformer (Diff-spaformer), a novel deep\nlearning framework. Our model integrates transformer architectures and\ndiffusion models via a Seismic Prior Extraction Network (SPEN), which serves as\na bridge module. Full-layer sparse multi-head attention and feed-forward\npropagation capture global information distributions, while the diffusion model\nprovides robust prior guidance. To mitigate the computational burden of\nhigh-dimensional representations, self-attention is computed along the channel\nrather than the spatial dimension. We show that using negative squared\nEuclidean distance to compute sparse affinity matrices better suits seismic\ndata modeling, enabling broader contribution from amplitude feature nodes. An\nadaptive ReLU function further discards low or irrelevant self-attention\nvalues. We conduct training within a single-stage optimization framework,\nrequiring only a few reverse diffusion sampling steps during inference.\nExtensive experiments demonstrate improved interpolation fidelity and\ncomputational efficiency for both random and continuous missing data, offering\na new paradigm for high-efficiency seismic data reconstruction under complex\ngeological conditions.", "authors": ["Xiaoli Wei", "Chunxia Zhang", "Baisong Jiang", "Anxiang Di", "Deng Xiong", "Jiangshe Zhang", "Mingming Gong"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:28:24.557981", "title_zh": "基於稀疏注意力Transformer與擴散模型的高效地震數據內插", "summary_zh": "本研究提出一種名為Diff-spaformer的新型深度學習框架，用於高效地震數據內插。該模型結合了Transformer架構和擴散模型，利用Seismic Prior Extraction Network (SPEN) 作為橋樑。透過全層稀疏多頭注意力機制捕捉全局資訊，並利用擴散模型提供穩健的先驗指導。為降低高維度運算負擔，自注意力計算沿通道而非空間維度進行。實驗證明，該方法在隨機和連續缺失數據情況下，均能提升內插的準確性與計算效率，為複雜地質條件下的高效地震數據重建提供新途徑。簡而言之，這項技術能更快速、更精確地重建地震數據。", "applications": ["**更精準的石油探勘：** 想像一下，石油公司可以更清楚地看到地底下的油藏，減少鑽探的失敗率，降低探勘成本，就像幫他們配備了超強夜視鏡。", "**更安全的建築選址：** 在地震頻繁的地區，我們可以更準確地評估地質結構，避開斷層帶，讓房屋蓋得更安全，就像幫建築師們配備了地質雷達。", "**更有效的災害預防：** 透過分析地震數據，我們可以更了解地殼的變動，預測潛在的地震風險，提前做好防災準備，就像幫我們配備了地震預警系統。"], "pitch": "各位投資人，我們正站在一場地震數據分析革命的風口浪尖！傳統的地震數據內插方法耗時且不精確，嚴重影響石油探勘、建築安全和災害預防等領域。我們的Diff-spaformer技術，結合Transformer和擴散模型，突破了算力瓶頸，大幅提升了數據重建的效率和精度。想像一下，未來的石油探勘不再是盲人摸象，而是精準定位，大幅降低鑽探成本，提高開採成功率！在建築安全方面，我們能提供更可靠的地質評估，避免因地質災害造成的巨大損失。更重要的是，這項技術有潛力應用於地震預測，提前預警，拯救無數生命！我們不僅僅是在銷售一種算法，更是在銷售一種更安全、更高效、更可持續的未來！現在投資，您將成為這場變革的早期參與者，共享巨大的市場紅利。我們預計，該技術在石油天然氣、建築工程、地震監測等領域的市場規模將達到數十億美元。加入我們，一起打造更安全的地球，挖掘更豐富的資源，實現更大的商業價值！", "audio": "docs/data/audios/2506.07923v1.wav"}
{"query": "AI", "id": "2506.07907v2", "url": "http://arxiv.org/abs/2506.07907v2", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between the decay\namplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-11T02:03:24.394715", "title_zh": "利用C-宇稱偶與奇的量子關聯D零反D零對，對D零→K負π正與反D零→K負π正衰變間強相位差的新穎測量", "summary_zh": "本研究提出一種新穎的強相位差測量技術，利用在ψ(3770)產生閾值以上能量的電子正負對撞機產生的量子關聯D零反D零對。此技術利用電荷共軛對稱性的偶與奇本徵態產生的D零反D零對。我們首次使用此數據樣本確定了D零-反D零的相對強相位。利用北京譜儀III實驗收集的7.13 fb⁻¹積分光度數據集，在4.13-4.23 GeV質心能量下，測得D零→K負π正與反D零→K負π正衰變之間的強相位差δDKπ為(192.8+11.0+1.9−12.4−2.4)°。", "applications": ["想像一下，未來海關可以更精準地分辨走私品。透過分析次原子粒子的衰變模式，我們可以開發出更靈敏的檢測儀器，揪出那些企圖蒙混過關的違禁品。", "在醫療領域，這項技術能幫助我們更深入了解藥物與人體細胞的交互作用。透過精確測量粒子間的細微差異，加速新藥開發，讓疾病治療更有效率。", "這就像是為原子世界打造了一把更精密的尺。有了更準確的測量，科學家能更深入探索宇宙的奧秘，例如暗物質的性質，甚至找到新的物理定律。"], "pitch": "各位投資人，我們正在開創一個全新的精準測量時代！這項技術不僅僅是一項科學突破，更是具有顛覆性商業潛力的明日之星。想像一下，透過我們對次原子粒子行為的精準掌握，可以應用於國防安全、新藥開發、材料科學等各個領域，創造出龐大的市場價值。這是一項具有無限可能的技術，現在加入，您將成為這場科技革命的領航者，共同塑造未來世界的樣貌！我們預期在五年內，光是安檢設備升級市場，就可達到數十億美元的規模，而這僅僅是冰山一角。不要錯過這個千載難逢的投資機會，讓我們一起引領科技創新，共創輝煌未來！", "audio": "docs/data/audios/2506.07907v2.wav"}
{"query": "Foundation Model", "id": "2506.07559v1", "url": "http://arxiv.org/abs/2506.07559v1", "title": "Cross-channel Perception Learning for H&E-to-IHC Virtual Staining", "summary": "With the rapid development of digital pathology, virtual staining has become\na key technology in multimedia medical information systems, offering new\npossibilities for the analysis and diagnosis of pathological images. However,\nexisting H&E-to-IHC studies often overlook the cross-channel correlations\nbetween cell nuclei and cell membranes. To address this issue, we propose a\nnovel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL\nfirst decomposes HER2 immunohistochemical staining into Hematoxylin and DAB\nstaining channels, corresponding to cell nuclei and cell membranes,\nrespectively. Using the pathology foundation model Gigapath's Tile Encoder,\nCCPL extracts dual-channel features from both the generated and real images and\nmeasures cross-channel correlations between nuclei and membranes. The features\nof the generated and real stained images, obtained through the Tile Encoder,\nare also used to calculate feature distillation loss, enhancing the model's\nfeature extraction capabilities without increasing the inference burden.\nAdditionally, CCPL performs statistical analysis on the focal optical density\nmaps of both single channels to ensure consistency in staining distribution and\nintensity. Experimental results, based on quantitative metrics such as PSNR,\nSSIM, PCC, and FID, along with professional evaluations from pathologists,\ndemonstrate that CCPL effectively preserves pathological features, generates\nhigh-quality virtual stained images, and provides robust support for automated\npathological diagnosis using multimedia medical data.", "authors": ["Hao Yang", "JianYu Wu", "Run Fang", "Xuelian Zhao", "Yuan Ji", "Zhiyu Chen", "Guibin He", "Junceng Guo", "Yang Liu", "Xinhua Zeng"], "published_date": "2025-06-09", "timestamp": "2025-06-11T02:04:57.583889", "title_zh": "用於H&E到IHC虛擬染色的跨通道感知學習", "summary_zh": "本研究提出一種新的跨通道感知學習(CCPL)策略，旨在解決現有H&E到IHC研究中忽略細胞核與細胞膜之間跨通道關聯性的問題。CCPL首先將HER2免疫組織化學染色分解為對應細胞核和細胞膜的蘇木精和DAB染色通道。利用病理基礎模型Gigapath的Tile Encoder，CCPL提取生成圖像和真實圖像的雙通道特徵，並測量細胞核和細胞膜之間的跨通道相關性。此外，CCPL還對單個通道的焦點光密度圖進行統計分析，以確保染色分佈和強度的連貫性。實驗結果表明，CCPL有效地保留了病理特徵，生成了高質量的虛擬染色圖像，並為使用多媒體醫療數據進行自動病理診斷提供了強大的支持。", "applications": ["1. 醫生遠距會診：偏鄉或資源不足的醫院，可以透過這項技術將傳統染色切片數位化，並轉換成其他染色方式，讓遠端的專家醫生能更全面地判讀病理影像，提升診斷準確性，及時給予治療建議。", "2. 加速新藥開發：藥廠可以利用這項技術，快速將現有的病理切片資料轉換成不同染色方式的影像，加速分析藥物對細胞的作用機制，縮短新藥開發時程，降低研發成本。", "3. 個人化健康管理：未來，透過AI分析個人病理切片資料，並轉換成不同染色結果，預測疾病風險，提供更精準的個人化健康建議，例如飲食、運動或生活習慣調整等。"], "pitch": "各位投資人，我們正在開發一項革命性的病理影像技術：跨通道感知學習(CCPL)，它能將傳統的H&E染色切片轉換成多種IHC染色結果，就像病理影像界的翻譯機！想像一下，全球病理實驗室每年產生數百萬張切片，但往往因為缺乏特定染色試劑或專業判讀人員而延誤診斷。CCPL能解決這個痛點，大幅提升診斷效率與準確性，特別是在癌症等重大疾病的診斷上，時間就是生命！\n\n更重要的是，CCPL的商業潛力巨大。我們可以將這項技術授權給醫院、診所、藥廠，甚至開發成雲端平台，提供全球病理影像分析服務。未來，結合AI和大數據，CCPL還能進一步發展成個人化醫療的關鍵技術，預測疾病風險，指導精準治療。我們相信，CCPL將徹底改變病理診斷的模式，創造巨大的社會價值和商業回報。現在加入我們，一起開創病理影像的未來！", "audio": "docs/data/audios/2506.07559v1.wav"}
{"query": "Diffusion Model", "id": "2506.07903v1", "url": "http://arxiv.org/abs/2506.07903v1", "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces", "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.", "authors": ["Kevin Rojas", "Yuchen Zhu", "Sichen Zhu", "Felix X. -F. Ye", "Molei Tao"], "published_date": "2025-06-09", "timestamp": "2025-06-11T02:07:18.160299", "title_zh": "擴散一切：基於任意狀態空間的多模態擴散模型", "summary_zh": "本研究提出一個創新的多模態擴散模型框架，可以直接在不同的數據模態上生成關聯數據，無需依賴複雜的外部預處理。透過為每個模態設計獨立的雜訊排程，模型能同時進行無條件生成和模態條件生成。研究團隊在文字-圖像生成和混合型表格數據合成上驗證了此方法，結果顯示其性能具有競爭力。這項技術突破了解碼器精準度的限制，為資料有限的應用開闢了新途徑，有望促進多模態資料生成領域的發展。", "applications": ["想像一下，你只要用文字描述你想要的畫面，這個AI就能自動生成對應的圖片，而且圖片的細節和風格完全符合你的要求。例如，輸入「陽光灑落在托斯卡尼田園上的美麗風景」，就能立即生成一幅令人驚嘆的畫作。", "醫生可以利用這個技術，結合病人的文字描述和X光片等影像資料，AI就能生成更精確的3D模型，幫助醫生更了解病情，制定更有效的治療方案。例如，輸入病患的症狀描述加上CT掃描，AI生成腫瘤的3D模型，輔助手術規劃。", "在電商平台上，消費者可以用文字描述想要的商品款式和功能，AI就能根據這些描述，自動生成商品的3D模型和宣傳圖片，讓消費者更直觀地了解商品，提升購買意願。例如，輸入「紅色真皮沙發，現代簡約風格」，就能生成不同角度的沙發照片。"], "pitch": "各位投資人，我們團隊帶來的是一項顛覆性的AI技術——「Diffuse Everything」。它解決了多模態資料生成的核心痛點，讓AI不再受限於單一數據類型。想像一下，一個AI能同時理解文字、圖像、聲音甚至表格數據，並將它們完美融合，創造出全新的內容。這將開啟一個全新的內容創作時代，無論是遊戲開發、廣告設計，甚至是醫療診斷，都將因為這項技術而產生革命性的變化。我們預計，在五年內，這項技術將成為AI領域的基礎設施，市場規模將達到數百億美元。現在加入我們，您將成為這場AI革命的領航者，共同開創一個無限可能的未來！", "audio": "docs/data/audios/2506.07903v1.wav"}
{"query": "AI", "id": "2506.09050v1", "url": "http://arxiv.org/abs/2506.09050v1", "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.", "authors": ["Yuki Imajuku", "Kohki Horie", "Yoichi Iwata", "Kensho Aoki", "Naohiro Takahashi", "Takuya Akiba"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:52:11.520831", "title_zh": "ALE-Bench：長時程目標驅動演算法工程的基準測試", "summary_zh": "ALE-Bench是一個新的基準測試，用於評估AI系統在解決複雜最佳化問題上的能力，這些問題來自現實世界的挑戰，例如包裹遞送路線規劃、人員排班、工廠生產計畫和電網平衡。它採用AtCoder Heuristic Contests中的真實任務，這些任務計算量大且沒有已知的精確解。與短期的編碼基準測試不同，ALE-Bench鼓勵在長時間範圍內迭代改進解決方案，並支持利用測試反饋和視覺化的互動式代理架構。初步評估顯示，現有大型語言模型在特定問題上表現出色，但在跨問題的一致性和長時程問題解決能力方面，與人類相比仍存在差距。這個基準測試旨在推動未來AI的發展。", "applications": ["想像一下，物流公司使用這項技術來優化送貨路線，不僅能節省油耗、減少碳排放，還能更快地將包裹送到客戶手中。", "醫院運用這項技術來安排醫生和護士的輪班，確保每個時段都有足夠的人力，同時也能讓醫護人員得到充分的休息，提升醫療品質。", "工廠利用這項技術來規劃生產流程，減少浪費、提高效率，讓產品更快上市，滿足市場需求。"], "pitch": "各位創投先進，我們正在開發ALE-Bench，一個革命性的AI演算法工程基準測試平台，它將徹底改變優化問題的解決方式。想像一下，一個AI系統可以自動設計出更高效的物流路線、更合理的生產排程、甚至更穩定的電網系統，這不僅能為企業節省數十億美元的成本，還能大幅提升資源利用率，為永續發展做出貢獻。ALE-Bench不僅僅是一個基準測試，它更是一個AI開發的加速器，能激勵研究人員開發出更強大的AI演算法，解決現實世界中最複雜的挑戰。我們相信，ALE-Bench將成為AI領域的Game Changer，而現在正是投資這個未來趨勢的絕佳時機。讓我們一起打造一個更智慧、更高效的世界！", "audio": "docs/data/audios/2506.09050v1.wav"}
{"query": "Foundation Model", "id": "2506.09042v1", "url": "http://arxiv.org/abs/2506.09042v1", "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "summary": "Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:53:40.014111", "title_zh": "Cosmos-Drive-Dreams：基於世界基礎模型的可擴展合成駕駛數據生成", "summary_zh": "為了解決自駕車訓練數據不足且難以捕捉罕見情境的問題，NVIDIA開發了Cosmos-Drive-Dreams合成數據生成管線。此管線利用Cosmos-Drive模型，該模型基於NVIDIA Cosmos世界基礎模型針對駕駛領域進行特化，能產生高品質、多視角、時空一致且可控的駕駛影片。透過Cosmos-Drive-Dreams，可以大規模生成包含高擬真度和挑戰性情境的駕駛數據集，有效改善長尾分布問題，並提升3D車道檢測、3D物件檢測和駕駛策略學習等下游任務的泛化能力。NVIDIA已開源此管線工具包、數據集和模型權重。", "applications": ["情境一：自駕計程車公司可以利用這項技術，模擬各種極端天氣（暴雨、濃霧）或突發狀況（行人突然衝出、車輛違規變換車道），來訓練自駕系統，確保在真實世界中能安全可靠地運行。", "情境二：遊戲開發商可以利用這項技術，快速生成多樣化的城市街道、鄉村道路等場景，讓賽車遊戲或開放世界遊戲的內容更豐富、更逼真，提升玩家的沉浸式體驗。", "情境三：汽車保險公司可以利用這項技術，重現過去發生的交通事故，分析事故原因，評估責任歸屬，甚至可以模擬未來可能發生的事故，提前預防，降低理賠風險。"], "pitch": "各位投資人，想像一下，我們正站在自駕車革命的浪潮之巔！但這場革命的關鍵，不是硬體，而是數據！真實世界數據昂貴且難以收集，特別是那些罕見但致命的邊緣案例。Cosmos-Drive-Dreams，正是解決這個痛點的殺手級應用！\n\n我們提供的，不僅僅是合成數據，而是基於NVIDIA Cosmos世界模型打造的、可控、高擬真的駕駛環境模擬器。這意味著，我們可以無限量地生成各種極端、危險的駕駛情境，讓自駕系統在虛擬世界中經歷千錘百鍊，確保在真實世界中萬無一失！\n\n想想看，這項技術的潛力有多大？它可以加速自駕車的開發和部署，降低事故率，甚至催生全新的保險模式！更重要的是，它還可以應用於遊戲、模擬訓練、城市規劃等領域，打造一個龐大的元宇宙生態系統！\n\n我們不僅僅是在賣數據，我們是在賣安全、賣效率、賣未來！投資Cosmos-Drive-Dreams，就是投資自駕車的未來，就是投資一個萬億美元級別的市場！現在加入我們，一起開創自駕車的新紀元！", "audio": "docs/data/audios/2506.09042v1.wav"}
{"query": "Diffusion Model", "id": "2506.09045v1", "url": "http://arxiv.org/abs/2506.09045v1", "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache", "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.", "authors": ["Zehong Ma", "Longhui Wei", "Feng Wang", "Shiliang Zhang", "Qi Tian"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:54:54.577489", "title_zh": "MagCache：基於幅度感知快取的快速影片生成", "summary_zh": "現有影片擴散模型的加速技術常依賴統一的啟發式方法或時間嵌入變體來跳過時間步並重複使用快取特徵，但這些方法通常需要大量校準，且容易因提示詞過擬合而產生不一致的輸出。本研究發現一個跨模型和提示詞的統一幅度定律：連續殘差輸出的幅度比率在大多數時間步中單調且穩定地下降，在最後幾個步驟中迅速下降。基於此，我們提出幅度感知快取(MagCache)，它使用誤差建模機制和自適應快取策略來跳過不重要的時間步。MagCache只需單一樣本進行校準，實驗結果表明，MagCache在Open-Sora和Wan 2.1上分別實現了2.1倍和2.68倍的加速，同時保持了卓越的視覺保真度。在可比較的計算預算下，MagCache在LPIPS、SSIM和PSNR指標上顯著優於現有方法。", "applications": ["想像一下，你想要快速製作一個社群媒體短片，展示你的寵物做一些可愛的動作。使用MagCache技術，你只需上傳一段短片，AI就能快速生成各種風格的影片，例如卡通風格、水彩風格等，讓你的影片更吸睛。", "假設你是一位老師，需要為學生製作教學影片。MagCache技術可以幫助你快速生成高品質的動畫或模擬影片，例如模擬化學反應、物理實驗等，讓教學內容更生動有趣。", "如果你是一位遊戲開發者，需要快速生成大量的遊戲素材，例如角色動畫、場景特效等。MagCache技術可以大幅縮短開發時間，讓你更快地推出新的遊戲內容。"], "pitch": "各位投資人，我們帶來的是MagCache，一項革命性的影片生成技術，它能大幅提升AI生成影片的速度與品質。想像一下，未來的影音內容創作將不再受限於時間與算力，每個人都能輕鬆創造出專業級的影片。這項技術的應用潛力無窮，從個人化行銷影片、教育內容製作，到遊戲開發、元宇宙內容生成，都將迎來爆炸性的成長。我們預期MagCache將成為AI影音領域的關鍵基礎設施，如同GPU之於深度學習，成為推動產業發展的核心引擎。現在投資MagCache，就是投資影音內容的未來！我們將透過授權、雲端服務、以及與各大影音平台合作等方式，快速將技術商業化，為投資人帶來豐厚的回報。這不僅是一個技術投資，更是一個參與未來內容革命的絕佳機會！", "audio": "docs/data/audios/2506.09045v1.wav"}
{"query": "AI", "id": "2506.09049v1", "url": "http://arxiv.org/abs/2506.09049v1", "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:37:53.933768", "title_zh": "VIKI-R：透過強化學習協調具體化多代理人合作", "summary_zh": "在動態環境中協調多個具體化代理人是人工智慧的核心挑戰。VIKI-R是一個基於視覺語言模型（VLM）的雙階段框架，專為具體化多代理人合作而設計。首先，它使用思維鏈（Chain-of-Thought）標註的示範對預訓練的VLM進行微調，然後在多層次的獎勵信號下進行強化學習。VIKI-R在VIKI-Bench基準測試中表現出色，VIKI-Bench是一個針對具體化多代理人合作的分層基準，包含多樣的機器人形態、多視角視覺觀察和結構化的監督信號。實驗證明，VIKI-R超越了現有方法，並能促使異構代理人之間產生複合式的合作模式。VIKI-R和VIKI-Bench共同為提升具體化AI系統中多代理人、視覺驅動的合作提供了一個統一的測試平台和方法。", "applications": ["智慧倉儲管理：多個機器人協同搬運貨物，根據視覺資訊判斷最佳路徑和搬運方式，提升倉儲效率。", "自動駕駛車隊：多輛自駕車透過視覺資訊共享路況，協同避開障礙物、優化行駛路線，減少交通擁堵。", "協作型機器人手術：多個手術機器人透過視覺引導，協同完成複雜手術，提高手術精準度和安全性。"], "pitch": "各位創投先進，我們正在打造一個AI界的變形金剛聯盟！VIKI-R不僅僅是一個多代理人協作系統，它更是一個賦能平台，讓不同形態、不同功能的機器人能夠像團隊一樣高效協作。想像一下，在智慧工廠裡，不再是單打獨鬥的機器手臂，而是機器人團隊自主協調，完成複雜的生產任務；在無人礦區，不再是各自為政的礦車，而是機器人車隊協同開採，大幅提升資源利用率。VIKI-R的核心價值在於其視覺驅動的協作能力，這意味著它能適應各種複雜的環境，無需預先編程，即可自主學習和優化協作策略。我們相信，隨著AI技術的發展，多代理人協作將成為主流，而VIKI-R將引領這個潮流。我們預計在未來五年內，VIKI-R將廣泛應用於製造業、物流業、醫療保健等領域，創造數十億美元的市場價值。現在加入我們，一起打造AI協作的新時代！", "audio": "docs/data/audios/2506.09049v1.wav"}
{"query": "Foundation Model", "id": "2506.09022v1", "url": "http://arxiv.org/abs/2506.09022v1", "title": "Do MIL Models Transfer?", "summary": "Multiple Instance Learning (MIL) is a cornerstone approach in computational\npathology (CPath) for generating clinically meaningful slide-level embeddings\nfrom gigapixel tissue images. However, MIL often struggles with small, weakly\nsupervised clinical datasets. In contrast to fields such as NLP and\nconventional computer vision, where transfer learning is widely used to address\ndata scarcity, the transferability of MIL models remains poorly understood. In\nthis study, we systematically evaluate the transfer learning capabilities of\npretrained MIL models by assessing 11 models across 21 pretraining tasks for\nmorphological and molecular subtype prediction. Our results show that\npretrained MIL models, even when trained on different organs than the target\ntask, consistently outperform models trained from scratch. Moreover,\npretraining on pancancer datasets enables strong generalization across organs\nand tasks, outperforming slide foundation models while using substantially less\npretraining data. These findings highlight the robust adaptability of MIL\nmodels and demonstrate the benefits of leveraging transfer learning to boost\nperformance in CPath. Lastly, we provide a resource which standardizes the\nimplementation of MIL models and collection of pretrained model weights on\npopular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab", "authors": ["Daniel Shao", "Richard J. Chen", "Andrew H. Song", "Joel Runevic", "Ming Y. Lu", "Tong Ding", "Faisal Mahmood"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:39:24.390655", "title_zh": "MIL模型是否具備遷移能力？", "summary_zh": "多實例學習(MIL)是計算病理學的基石，能從超高解析度的組織影像中產生具有臨床意義的切片層級嵌入。然而，MIL常在小型、弱監督的臨床數據集上表現不佳。本研究系統性地評估了預訓練MIL模型的遷移學習能力，結果顯示，即使在與目標任務不同的器官上訓練，預訓練模型也始終優於從頭訓練的模型。在泛癌數據集上進行預訓練，能實現跨器官和任務的強泛化能力，超越了切片基礎模型，同時使用的預訓練數據更少。本研究強調了MIL模型的強大適應性，並證明了利用遷移學習來提升計算病理學性能的優勢。", "applications": ["1. **癌症篩檢App：**想像一下，未來只要用手機App掃描病理切片，就能利用預訓練好的MIL模型，快速判斷是否有癌症風險，幫助民眾及早發現、及早治療。", "2. **遠距醫療病理診斷：**偏鄉地區缺乏病理醫生，透過MIL模型，可以將病理切片影像上傳雲端，讓遠端的專家協助診斷，大幅提升醫療資源的可及性。", "3. **新藥開發加速器：**藥廠可以利用MIL模型分析大量病理影像數據，找出潛在的藥物靶點，加速新藥開發的進程，降低研發成本。"], "pitch": "各位投資人，我們正在打造一個病理影像AI的革命性平台，核心技術是基於遷移學習的多實例學習(MIL)模型。傳統病理診斷耗時且高度依賴專家經驗，我們的技術能大幅提升診斷效率和準確性，解決醫療資源不均的問題。想像一下，未來每個人都能透過手機進行初步的癌症風險評估，這將是一個千億美元級別的市場！\n\n我們的競爭優勢在於：一是我們已經驗證了MIL模型在病理影像分析上的卓越性能，並遠超傳統方法；二是我們擁有豐富的預訓練模型資源，能快速適應不同的病理診斷任務；三是我們正在建立一個開放的平台，讓更多的研究者和醫生能參與到模型的訓練和應用中來，形成一個強大的生態系統。\n\n我們預計在未來三年內，將我們的技術應用於癌症篩檢、遠距醫療和新藥開發等領域，並與醫院、診所和藥廠建立深度合作夥伴關係。我們相信，我們的技術將會徹底改變病理診斷的模式，為人類健康做出更大的貢獻。現在加入我們，一起開啟病理AI的新時代！", "audio": "docs/data/audios/2506.09022v1.wav"}
{"query": "Diffusion Model", "id": "2506.09027v1", "url": "http://arxiv.org/abs/2506.09027v1", "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.", "authors": ["Runqian Wang", "Kaiming He"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:40:43.699370", "title_zh": "擴散與分散：具備表徵正規化的圖像生成", "summary_zh": "這項研究提出了一種名為「分散損失」（Dispersive Loss）的簡單、隨插即用的正規化方法，能有效提升基於擴散模型的圖像生成效果。不同於以往的擴散模型，此方法著重於表徵學習，鼓勵內部表徵在隱藏空間中分散，類似於對比自監督學習，但無需正樣本配對，因此不干擾迴歸的採樣過程。相較於表徵對齊（REPA）方法，此方法更簡潔，無需預訓練、額外參數或外部資料。實驗證明，在ImageNet數據集上，此方法能穩定地提升多種模型的效能。這項研究有望彌合生成模型和表徵學習之間的差距。", "applications": ["修復老照片或模糊圖片：想像一下，你可以用手機App輕鬆修復爺爺奶奶的老照片，讓模糊的影像變得清晰，重溫美好的回憶。", "生成獨一無二的藝術作品：藝術家可以利用這項技術，創造出前所未見的藝術風格，生成獨特且令人驚豔的數位畫作，豐富我們的視覺體驗。", "客製化商品設計：設計師可以根據客戶的需求，快速生成各種產品設計方案，例如客製化的手機殼、T恤圖案等，滿足個性化需求。"], "pitch": "各位創投先進，我們正在開發一項革命性的圖像生成技術，它能大幅提升生成圖像的品質和真實感，且成本極低。想像一下，未來電商平台可以利用這項技術，自動生成各種商品的逼真圖片，大幅降低攝影成本；遊戲公司可以快速創建精美的遊戲場景和角色，縮短開發週期；醫療領域可以生成高清晰度的醫學影像，輔助診斷。我們的「分散損失」方法，無需大量數據和複雜的訓練，就能達到甚至超越現有技術的效果。這不僅能為我們帶來巨大的商業價值，更能推動AI技術在各個領域的應用，改變人們的生活方式。現在投資我們，您將成為這場圖像革命的領航者，共同創造一個充滿無限可能的未來！", "audio": "docs/data/audios/2506.09027v1.wav"}
{"query": "AI", "id": "2506.09002v1", "url": "http://arxiv.org/abs/2506.09002v1", "title": "Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models", "summary": "Unit testing is essential for ensuring software reliability and correctness.\nClassic Search-Based Software Testing (SBST) methods and concolic\nexecution-based approaches for generating unit tests often fail to achieve high\ncoverage due to difficulties in handling complex program units, such as\nbranching conditions and external dependencies. Recent work has increasingly\nutilized large language models (LLMs) to generate test cases, improving the\nquality of test generation by providing better context and correcting errors in\nthe model's output. However, these methods rely on fixed prompts, resulting in\nrelatively low compilation success rates and coverage. This paper presents\nPALM, an approach that leverages large language models (LLMs) to enhance the\ngeneration of high-coverage unit tests. PALM performs program analysis to\nidentify branching conditions within functions, which are then combined into\npath constraints. These constraints and relevant contextual information are\nused to construct prompts that guide the LLMs in generating unit tests. We\nimplement the approach and evaluate it in 10 open-source Rust crates.\nExperimental results show that within just two or three hours, PALM can\nsignificantly improves test coverage compared to classic methods, with\nincreases in overall project coverage exceeding 50% in some instances and its\ngenerated tests achieving an average coverage of 75.77%, comparable to human\neffort (71.30%), highlighting the potential of LLMs in automated test\ngeneration. We submitted 91 PALM-generated unit tests targeting new code. Of\nthese submissions, 80 were accepted, 5 were rejected, and 6 remain pending\nreview. The results demonstrate the effectiveness of integrating program\nanalysis with AI and open new avenues for future research in automated software\ntesting.", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Hange Shi", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:29:22.809571", "title_zh": "透過混合程式分析與大型語言模型提升 Rust 單元測試覆蓋率", "summary_zh": "本研究提出 PALM 方法，結合程式分析與大型語言模型（LLM），顯著提升 Rust 程式的單元測試覆蓋率。傳統測試方法難以處理複雜的分支條件和外部依賴，而 PALM 透過程式分析找出分支條件，將其轉化為路徑約束，並結合上下文資訊，引導 LLM 生成更有效的測試案例。實驗結果顯示，PALM 在數小時內即可大幅提升測試覆蓋率，部分專案甚至超過 50%，平均覆蓋率達到 75.77%，與人工測試相當。已提交的測試案例中，絕大多數已被接受，證明了此方法在自動化測試領域的潛力。", "applications": ["**自動修復程式錯誤：** 想像一下，程式碼寫完後，PALM 就像一個超級偵錯員，自動產生測試案例，找出潛在的錯誤，並協助開發者快速修復，減少程式崩潰的機會。", "**提升軟體品質：** 銀行、醫療等對軟體穩定性要求極高的產業，可以利用 PALM 確保程式碼的品質，避免因程式錯誤造成重大損失。", "**加速開發流程：** 開發者不再需要花費大量時間編寫測試案例，PALM 可以自動生成，讓他們更專注於核心功能的開發，加速產品上市時間。"], "pitch": "各位投資人，我們正處於AI驅動軟體開發的革命性時刻！PALM不僅僅是個測試工具，它是軟體品質保證的未來。想想看，全球每年因軟體錯誤造成的損失高達數千億美元，而PALM能有效降低這些損失，為企業節省巨額成本。更重要的是，隨著AI技術的不斷發展，PALM的自我學習能力將持續提升，測試覆蓋率和效率將遠超人工。我們預見，PALM將成為所有軟體開發團隊的標配，市場潛力無限。現在投資PALM，就是投資軟體開發的未來，讓我們一起打造更可靠、更穩定的數位世界！", "audio": "docs/data/audios/2506.09002v1.wav"}
{"query": "Foundation Model", "id": "2506.08982v1", "url": "http://arxiv.org/abs/2506.08982v1", "title": "On Finetuning Tabular Foundation Models", "summary": "Foundation models are an emerging research direction in tabular deep\nlearning. Notably, TabPFNv2 recently claimed superior performance over\ntraditional GBDT-based methods on small-scale datasets using an in-context\nlearning paradigm, which does not adapt model parameters to target datasets.\nHowever, the optimal finetuning approach for adapting tabular foundational\nmodels, and how this adaptation reshapes their internal mechanisms, remains\nunderexplored. While prior works studied finetuning for earlier foundational\nmodels, inconsistent findings and TabPFNv2's unique architecture necessitate\nfresh investigation. To address these questions, we first systematically\nevaluate various finetuning strategies on diverse datasets. Our findings\nestablish full finetuning as the most practical solution for TabPFNv2 in terms\nof time-efficiency and effectiveness. We then investigate how finetuning alters\nTabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.\nWe reveal that the success of finetuning stems from the fact that after\ngradient-based adaptation, the dot products of the query-representations of\ntest objects and the key-representations of in-context training objects more\naccurately reflect their target similarity. This improved similarity allows\nfinetuned TabPFNv2 to better approximate target dependency by appropriately\nweighting relevant in-context samples, improving the retrieval-based prediction\nlogic. From the practical perspective, we managed to finetune TabPFNv2 on\ndatasets with up to 50K objects, observing performance improvements on almost\nall tasks. More precisely, on academic datasets with I.I.D. splits, finetuning\nallows TabPFNv2 to achieve state-of-the-art results, while on datasets with\ngradual temporal shifts and rich feature sets, TabPFNv2 is less stable and\nprior methods remain better.", "authors": ["Ivan Rubachev", "Akim Kotelnikov", "Nikolay Kartashev"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:30:59.879609", "title_zh": "表格型資料基礎模型的微調研究", "summary_zh": "本研究探討如何微調表格型資料的基礎模型，特別是TabPFNv2。TabPFNv2原本擅長利用少量資料進行學習，無需調整模型參數。然而，如何有效微調此類模型，以及微調如何改變其內部運作機制，仍是未解之謎。研究發現，完整微調是TabPFNv2最有效率且效果最好的方法。微調後的模型，能更精準地判斷測試資料與訓練資料的相似度，進而提升預測準確性。實驗證明，微調後的TabPFNv2在包含五萬筆資料的數據集上，也能有效提升效能，在學術資料集上甚至達到最先進的水平。這項研究為表格型資料基礎模型的應用開闢了新的可能性。", "applications": ["**個人貸款風險評估：** 銀行或金融機構可以利用微調後的TabPFNv2，根據客戶的少量歷史資料（例如：年齡、收入、信用評分）更精準地預測貸款違約風險，快速決定是否核准貸款，降低壞帳率。", "**疾病診斷輔助：** 醫生可以使用微調後的TabPFNv2，基於少量的病患數據（例如：症狀、檢驗報告）輔助診斷罕見疾病，提高診斷效率和準確性，避免延誤治療。", "**客戶流失預測：** 電信公司或零售業者可以利用微調後的TabPFNv2，分析少量的客戶行為數據（例如：購買紀錄、瀏覽習慣）預測客戶是否可能流失，及早採取挽留措施，提升客戶忠誠度。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，能徹底改變表格型資料的應用方式。想像一下，在數據量有限的情況下，也能擁有媲美甚至超越大型模型的精準度。這就是微調後的TabPFNv2的潛力！\n\n傳統機器學習需要大量數據才能訓練出可靠的模型，但現實世界中，許多領域都面臨數據稀缺的挑戰。我們的技術能讓企業在小數據集上快速建立精準的預測模型，大幅降低數據收集和處理的成本。例如，新藥開發、罕見疾病診斷、客製化行銷等領域，都將因此受益。\n\n更重要的是，我們的技術具有高度的可擴展性。未來，我們計劃將此技術應用於更廣泛的領域，例如：金融風險管理、智能製造、智慧城市等。我們相信，微調後的TabPFNv2將成為企業決策的強大助力，為投資者帶來豐厚的回報。現在投資，您將站在AI革命的最前沿，共同開創無限可能！", "audio": "docs/data/audios/2506.08982v1.wav"}
{"query": "Diffusion Model", "id": "2506.08809v1", "url": "http://arxiv.org/abs/2506.08809v1", "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference", "summary": "High-resolution sinogram inpainting is essential for computed tomography\nreconstruction, as missing high-frequency projections can lead to visible\nartifacts and diagnostic errors. Diffusion models are well-suited for this task\ndue to their robustness and detail-preserving capabilities, but their\napplication to high-resolution inputs is limited by excessive memory and\ncomputational demands. To address this limitation, we propose HiSin, a novel\ndiffusion based framework for efficient sinogram inpainting via\nresolution-guided progressive inference. It progressively extracts global\nstructure at low resolution and defers high-resolution inference to small\npatches, enabling memory-efficient inpainting. It further incorporates\nfrequency-aware patch skipping and structure-adaptive step allocation to reduce\nredundant computation. Experimental results show that HiSin reduces peak memory\nusage by up to 31.25% and inference time by up to 18.15%, and maintains\ninpainting accuracy across datasets, resolutions, and mask conditions.", "authors": ["Jiaze E", "Srutarshi Banerjee", "Tekin Bicer", "Guannan Wang", "Yanfu Zhang", "Bin Ren"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:32:39.355347", "title_zh": "HiSin：透過解析度導向的漸進式推論實現高效能高解析度正弦圖修復", "summary_zh": "在電腦斷層掃描中，高解析度正弦圖修復至關重要，因為遺失的高頻投影可能導致明顯的偽影和診斷錯誤。擴散模型擅長此任務，但其在高解析度輸入上的應用受限於過多的記憶體和計算需求。我們提出HiSin，一種基於擴散的新穎框架，透過解析度導向的漸進式推論實現高效的正弦圖修復。它在低解析度下逐步提取全局結構，並將高解析度推論延遲到小塊區域，從而實現記憶體效率高的修復。此外，它還整合了頻率感知補丁跳過和結構自適應步驟分配，以減少冗餘計算。實驗結果表明，HiSin將峰值記憶體使用量減少了高達31.25%，推論時間減少了高達18.15%，並在資料集、解析度和遮罩條件下保持了修復準確性。", "applications": ["想像一下，醫院的CT掃描儀經常會因為各種原因產生一些雜訊或缺失數據，導致影像不清晰，影響醫生診斷。HiSin就像一個影像魔術師，能自動修復這些缺失的部分，讓醫生看到更清晰、更準確的影像，減少誤診的風險。", "考古學家在挖掘古代文物時，常常會使用X光或CT掃描來觀察文物內部的結構。如果掃描結果不完整或有干擾，HiSin就能派上用場，幫助他們重建清晰的文物內部影像，從而更好地研究古代文明。", "機場安檢人員使用X光機檢查行李時，有時會遇到影像模糊或遮蔽的情況。HiSin可以輔助安檢系統，修復這些影像，提高安檢效率，同時也能更準確地識別潛在的危險物品。"], "pitch": "各位投資人，我們正處於醫療影像AI革命的風口浪尖！HiSin不僅僅是一個修復演算法，它代表著更高效、更精準的醫療診斷的未來。現有的高解析度影像處理方案耗時且資源密集，嚴重限制了臨床應用。HiSin透過獨特的解析度導向漸進式推論，大幅降低了計算成本，讓高解析度影像修復成為可能，潛力巨大。想像一下，未來，我們的技術可以應用於自動駕駛的雷達信號處理、衛星影像的清晰化，甚至是在藝術品修復領域大放異彩！這是一個數十億美元的市場，而HiSin有機會成為這個市場的領導者。我們需要您的投資，一起將這項技術推向世界，為人類健康和科技進步做出貢獻！", "audio": "docs/data/audios/2506.08809v1.wav"}
{"query": "AI", "id": "2506.08998v1", "url": "http://arxiv.org/abs/2506.08998v1", "title": "On Monotonicity in AI Alignment", "summary": "Comparison-based preference learning has become central to the alignment of\nAI models with human preferences. However, these methods may behave\ncounterintuitively. After empirically observing that, when accounting for a\npreference for response $y$ over $z$, the model may actually decrease the\nprobability (and reward) of generating $y$ (an observation also made by\nothers), this paper investigates the root causes of (non) monotonicity, for a\ngeneral comparison-based preference learning framework that subsumes Direct\nPreference Optimization (DPO), Generalized Preference Optimization (GPO) and\nGeneralized Bradley-Terry (GBT). Under mild assumptions, we prove that such\nmethods still satisfy what we call local pairwise monotonicity. We also provide\na bouquet of formalizations of monotonicity, and identify sufficient conditions\nfor their guarantee, thereby providing a toolbox to evaluate how prone learning\nmodels are to monotonicity violations. These results clarify the limitations of\ncurrent methods and provide guidance for developing more trustworthy preference\nlearning algorithms.", "authors": ["Gilles Bareilles", "Julien Fageot", "Lê-Nguyên Hoang", "Peva Blanchard", "Wassim Bouaziz", "Sébastien Rouault", "El-Mahdi El-Mhamdi"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:53:57.472960", "title_zh": "論AI對齊中的單調性", "summary_zh": "基於比較的偏好學習已成為將AI模型與人類偏好對齊的核心方法。然而，這些方法有時會產生反直覺的行為。研究發現，當模型考慮到對回應y的偏好高於z時，實際上可能降低生成y的機率（和獎勵）。本研究深入探討了非單調性的根本原因，針對包含直接偏好優化（DPO）、廣義偏好優化（GPO）和廣義布拉德利-特里（GBT）的通用比較偏好學習框架。在溫和的假設下，證明了這些方法仍然滿足我們稱之為局部成對單調性的性質。此外，我們提供了一系列單調性的形式化描述，並確定了保證它們的充分條件，從而提供了一個工具箱來評估學習模型違反單調性的可能性。這些結果闡明了當前方法的局限性，並為開發更值得信賴的偏好學習算法提供了指導。", "applications": ["**個人化推薦系統：** 想像一下，你告訴AI你比較喜歡A餐廳勝過B餐廳，但系統之後卻一直推薦類似B餐廳的選項。這個研究能幫助改善推薦系統，確保它們真的會記住你的偏好，並推薦你更喜歡的內容。", "**自動駕駛：** 如果你告訴自動駕駛系統你比較喜歡平穩的駕駛風格，系統卻突然開始急加速或急煞車，你會覺得很奇怪也很危險。這個研究有助於確保AI系統的行為始終如一，符合你的偏好，提高安全性。", "**AI助理：** 當你告訴AI助理你喜歡某種音樂類型，結果它卻開始播放完全相反的音樂，這會讓人感到困惑。這個研究可以幫助AI助理更好地理解和記住你的偏好，提供更個性化的服務。"], "pitch": "各位創投先進，我們正在解決AI領域一個至關重要的問題：AI對齊。目前，AI系統在學習人類偏好時，經常出現「說一套做一套」的情況，導致用戶體驗不佳，甚至產生安全隱患。我們的研究成果，揭示了這種非單調性行為的根本原因，並提供了一套解決方案，讓AI系統真正理解並遵循人類的偏好。想像一下，一個能夠完全理解使用者意圖的AI，在醫療診斷、金融投資、教育輔導等領域，將會釋放出多麼巨大的商業價值！我們不僅僅是提供一個技術方案，更是打造一個更值得信賴、更人性化的AI未來。現在投資我們，您將站在AI對齊的最前沿，共同引領下一代AI技術的發展，掌握未來AI市場的制高點！未來，我們甚至可以將此技術應用於開發『讀心術』AI，透過分析人類微表情與生理數據，精準預測人類需求，打造劃時代的AI產品與服務，其商業潛力無可限量！", "audio": "docs/data/audios/2506.08998v1.wav"}
{"query": "Foundation Model", "id": "2506.08955v1", "url": "http://arxiv.org/abs/2506.08955v1", "title": "Segment Concealed Objects with Incomplete Supervision", "summary": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves\nsegmenting objects that seamlessly blend into their surrounding environments,\nutilizing incompletely annotated data, such as weak and semi-annotations, for\nmodel training. This task remains highly challenging due to (1) the limited\nsupervision provided by the incompletely annotated training data, and (2) the\ndifficulty of distinguishing concealed objects from the background, which\narises from the intrinsic similarities in concealed scenarios. In this paper,\nwe introduce the first unified method for ISCOS to address these challenges. To\ntackle the issue of incomplete supervision, we propose a unified mean-teacher\nframework, SEE, that leverages the vision foundation model, ``\\emph{Segment\nAnything Model (SAM)}'', to generate pseudo-labels using coarse masks produced\nby the teacher model as prompts. To mitigate the effect of low-quality\nsegmentation masks, we introduce a series of strategies for pseudo-label\ngeneration, storage, and supervision. These strategies aim to produce\ninformative pseudo-labels, store the best pseudo-labels generated, and select\nthe most reliable components to guide the student model, thereby ensuring\nrobust network training. Additionally, to tackle the issue of intrinsic\nsimilarity, we design a hybrid-granularity feature grouping module that groups\nfeatures at different granularities and aggregates these results. By clustering\nsimilar features, this module promotes segmentation coherence, facilitating\nmore complete segmentation for both single-object and multiple-object images.\nWe validate the effectiveness of our approach across multiple ISCOS tasks, and\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancing\nthe performance of existing models.", "authors": ["Chunming He", "Kai Li", "Yachao Zhang", "Ziyun Yang", "Youwei Pang", "Longxiang Tang", "Chengyu Fang", "Yulun Zhang", "Linghe Kong", "Xiu Li", "Sina Farsiu"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:55:36.593228", "title_zh": "使用不完整監督分割隱藏物體", "summary_zh": "本研究提出一種新的方法，用於在只有不完整標註資料的情況下，分割那些與周圍環境無縫融合的隱藏物體。由於標註資料不完整，以及隱藏物體與背景高度相似，這項任務極具挑戰性。我們提出名為SEE的統一平均教師框架，利用視覺基礎模型SAM生成偽標籤，並設計一系列策略來生成、儲存和監督這些偽標籤，以確保穩健的網路訓練。此外，我們設計了一種混合粒度特徵分組模組，通過聚類相似特徵，促進分割一致性，進而更完整地分割單個或多個物體。實驗結果表明，我們的模型在多個隱藏物體分割任務上都達到了最先進的性能，並且可以作為一個隨插即用的解決方案，提升現有模型的效能。", "applications": ["**智慧安防：** 在監視器畫面中，即使嫌犯試圖隱藏身形融入背景，這項技術也能準確識別並追蹤，提高破案率。", "**醫療影像分析：** 醫生可以利用這項技術，在X光片或MRI影像中更清楚地辨識出隱藏在組織中的微小腫瘤，及早發現並治療。", "**自動駕駛：** 在惡劣天氣或光線不足的情況下，車載鏡頭可能難以辨識道路上的行人或障礙物，這項技術能幫助車輛更準確地感知周圍環境，提升行車安全。"], "pitch": "各位投資人，想像一下，一個能看穿偽裝的世界！我們的技術正在開啟這個可能性。現今的AI視覺辨識在隱藏物體的辨識上仍然存在巨大盲點，這不僅是技術挑戰，更是潛在的巨大商機。我們的『不完整監督隱藏物體分割』技術，能讓機器在近乎無監督的情況下，學習辨識隱藏的目標，這代表什麼？\n\n首先，安防產業將迎來革命。想想機場安檢、邊境管制，甚至是商場防盜，我們的技術能有效提升安全等級，減少犯罪發生。\n\n其次，醫療診斷將更加精準。早期癌症的檢測往往仰賴醫生肉眼判斷，我們的技術能輔助醫生發現微小病灶，大幅提升治癒率。\n\n更重要的是，自動駕駛將更加安全可靠。在惡劣環境下，我們的技術能幫助汽車『看』得更清楚，減少事故發生。\n\n這項技術的應用範圍遠不止於此。想像一下，軍事偵察、工業瑕疵檢測、甚至是考古研究，都能因為我們的技術而取得突破性進展。\n\n我們不僅僅是在開發一個演算法，我們正在打造一個全新的視覺感知平台。我們預計在未來五年內，將這項技術推廣到全球市場，與各產業龍頭合作，共同打造一個更安全、更智慧的世界。現在加入我們，您將成為這場視覺革命的先驅！", "audio": "docs/data/audios/2506.08955v1.wav"}
{"query": "Diffusion Model", "id": "2506.08796v1", "url": "http://arxiv.org/abs/2506.08796v1", "title": "Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling", "summary": "Recently, the rectified flow (RF) has emerged as the new state-of-the-art\namong flow-based diffusion models due to its high efficiency advantage in\nstraight path sampling, especially with the amazing images generated by a\nseries of RF models such as Flux 1.0 and SD 3.0. Although a straight-line\nconnection between the noisy and natural data distributions is intuitive, fast,\nand easy to optimize, it still inevitably leads to: 1) Diversity concerns,\nwhich arise since straight-line paths only cover a fairly restricted sampling\nspace. 2) Multi-scale noise modeling concerns, since the straight line flow\nonly needs to optimize the constant velocity field $\\bm v$ between the two\ndistributions $\\bm\\pi_0$ and $\\bm\\pi_1$. In this work, we present\nDiscretized-RF, a new family of rectified flow (also called momentum flow\nmodels since they refer to the previous velocity component and the random\nvelocity component in each diffusion step), which discretizes the straight path\ninto a series of variable velocity field sub-paths (namely ``momentum fields'')\nto expand the search space, especially when close to the distribution\n$p_\\text{noise}$. Different from the previous case where noise is directly\nsuperimposed on $\\bm x$, we introduce noise on the velocity $\\bm v$ of the\nsub-path to change its direction in order to improve the diversity and\nmulti-scale noise modeling abilities. Experimental results on several\nrepresentative datasets demonstrate that learning momentum flow matching by\nsampling random velocity fields will produce trajectories that are both diverse\nand efficient, and can consistently generate high-quality and diverse results.\nCode is available at https://github.com/liuruixun/momentum-fm.", "authors": ["Zhiyuan Ma", "Ruixun Liu", "Sixian Liu", "Jianjun Li", "Bowen Zhou"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:56:45.617158", "title_zh": "流動多樣且高效：透過隨機速度場採樣學習動量流匹配", "summary_zh": "這項研究提出一種新的修正流模型，稱為Discretized-RF，它將傳統的直線路徑分割成一系列變速場子路徑，擴展搜尋空間，尤其是在接近雜訊分佈時。不同於直接在資料上疊加雜訊，此方法在子路徑的速度上引入雜訊，改變其方向，從而提升多樣性和多尺度雜訊建模能力。實驗結果表明，透過採樣隨機速度場學習動量流匹配，可以產生多樣且高效的軌跡，並能持續生成高品質且多樣化的結果。簡單來說，這項技術讓AI生成圖像更豐富、更真實，也更有效率。", "applications": ["遊戲開發：快速生成多樣化的遊戲場景和角色，節省美術設計時間，並提供更豐富的遊戲體驗。", "影視特效：高效創建逼真的視覺特效，例如自然災害、奇幻生物等，降低製作成本，提升視覺衝擊力。", "服裝設計：設計師可以快速生成各種服裝款式和紋理，激發靈感，並根據客戶需求進行個性化定制。"], "pitch": "各位創投先進，想像一下，未來AI不再只是複製現有圖像，而是能創造出前所未見的藝術品、設計出獨一無二的產品！我們的Discretized-RF技術，突破了傳統AI圖像生成的限制，讓AI更像一位真正的藝術家，擁有無限的創意和想像力。這項技術不僅能大幅提升圖像生成效率，降低成本，更能開創全新的商業模式。例如，我們可以打造一個AI設計平台，讓使用者輕鬆生成個性化的產品設計、藝術作品，甚至虛擬世界的場景和角色。未來，每個企業、每個個人都能擁有自己的AI設計師，創造出無限的商業價值。現在投資，您將站在AI圖像革命的最前沿，共同見證AI創造力的無限可能！", "audio": "docs/data/audios/2506.08796v1.wav"}
{"query": "AI", "id": "2506.08962v1", "url": "http://arxiv.org/abs/2506.08962v1", "title": "WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis", "summary": "This research-to-practice work-in-progress (WIP) paper presents an AI-enabled\nsmart tutor designed to provide homework assessment and feedback for students\nin an undergraduate circuit analysis course. We detail the tutor's design\nphilosophy and core components, including open-ended question answering and\nhomework feedback generation. The prompts are carefully crafted to optimize\nresponses across different problems. The smart tutor was deployed on the\nMicrosoft Azure platform and is currently in use in an undergraduate circuit\nanalysis course at the School of Electrical and Computer Engineering in a\nlarge, public, research-intensive institution in the Southeastern United\nStates. Beyond offering personalized instruction and feedback, the tutor\ncollects student interaction data, which is summarized and shared with the\ncourse instructor. To evaluate its effectiveness, we collected student\nfeedback, with 90.9% of responses indicating satisfaction with the tutor.\nAdditionally, we analyze a subset of collected data on preliminary circuit\nanalysis topics to assess tutor usage frequency for each problem and identify\nfrequently asked questions. These insights help instructors gain real-time\nawareness of student difficulties, enabling more targeted classroom\ninstruction. In future work, we will release a full analysis once the complete\ndataset is available after the Spring 2025 semester. We also explore the\npotential applications of this smart tutor across a broader range of\nengineering disciplines by developing improved prompts, diagram-recognition\nmethods, and database management strategies, which remain ongoing areas of\nresearch.", "authors": ["Liangliang Chen", "Huiru Xie", "Jacqueline Rohde", "Ying Zhang"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:31:03.189904", "title_zh": "WIP：大型語言模型增強型大學電路分析智慧導師", "summary_zh": "本研究展示一個AI驅動的智慧導師，專為大學電路分析課程的學生提供作業評估和回饋。它基於大型語言模型，能解答開放式問題並生成客製化的作業回饋。此智慧導師已部署在Microsoft Azure平台上，並在美國東南部一所大型公立研究型大學的電路分析課程中使用。除了提供個人化指導，它還收集學生互動數據，並將摘要分享給授課教師，幫助教師即時掌握學生學習困難，以便調整教學策略。初步的學生回饋顯示90.9%的學生對此導師感到滿意。未來，我們將進一步分析完整數據集，並探索其在更廣泛工程領域的應用潛力，例如改進提示、開發圖表識別方法和資料庫管理策略。", "applications": ["想像一下，高中生在學習物理電學時遇到困難，這個AI智慧導師可以一步一步引導他理解電路原理，就像一位隨時待命的私人教師。", "對於需要考取電機工程師證照的人來說，這個智慧導師可以提供客製化的模擬試題和詳盡的解答，幫助他們高效備考。", "電子工程師在設計複雜電路時，可以利用這個智慧導師快速驗證設計方案，找出潛在問題，節省大量的時間和成本。"], "pitch": "各位投資人，我們正在打造的是一個革命性的教育科技產品——基於大型語言模型的智慧導師，它不僅能個性化輔導學生，更能即時反饋學習數據，幫助老師精準教學。試想一下，未來每個學生都擁有一個24小時隨時待命的AI家教，學習效率將大幅提升！更重要的是，這個技術可以拓展到其他理工學科，甚至語言學習、藝術指導等領域，市場潛力巨大。我們不僅僅是提供一個輔導工具，我們是在打造一個全新的教育生態系統。預計未來五年內，我們的智慧導師將覆蓋全球數百萬理工科學生，成為教育科技領域的獨角獸！現在加入我們，共同開創AI教育的新時代！", "audio": "docs/data/audios/2506.08962v1.wav"}
{"query": "Foundation Model", "id": "2506.08949v1", "url": "http://arxiv.org/abs/2506.08949v1", "title": "SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation", "summary": "In the era of information explosion, efficiently leveraging large-scale\nunlabeled data while minimizing the reliance on high-quality pixel-level\nannotations remains a critical challenge in the field of medical imaging.\nSemi-supervised learning (SSL) enhances the utilization of unlabeled data by\nfacilitating knowledge transfer, significantly improving the performance of\nfully supervised models and emerging as a highly promising research direction\nin medical image analysis. Inspired by the ability of Vision Foundation Models\n(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised\nSAM-2), a novel approach that leverages SAM-2's robust feature extraction\ncapabilities to uncover latent knowledge in unlabeled medical images, thus\neffectively enhancing feature support for fully supervised medical image\nsegmentation. Specifically, building upon the single-stream \"weak-to-strong\"\nconsistency regularization framework, this paper introduces a Discriminative\nFeature Enhancement (DFE) mechanism to further explore the feature\ndiscrepancies introduced by various data augmentation strategies across\nmultiple views. By leveraging feature similarity and dissimilarity across\nmulti-scale augmentation techniques, the method reconstructs and models the\nfeatures, thereby effectively optimizing the salient regions. Furthermore, a\nprompt generator is developed that integrates Physical Constraints with a\nSliding Window (PCSW) mechanism to generate input prompts for unlabeled data,\nfulfilling SAM-2's requirement for additional prompts. Extensive experiments\ndemonstrate the superiority of the proposed method for semi-supervised medical\nimage segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,\nSSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous\nstate-of-the-art method by +3.65 Dice. Code will be available at\nhttps://github.com/AIGeeksGroup/SSS.", "authors": ["Hongjie Zhu", "Xiwei Liu", "Rundong Xue", "Zeyu Zhang", "Yong Xu", "Daji Ergu", "Ying Cai", "Yang Zhao"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:32:20.384344", "title_zh": "SSS：基於高效提示的半監督SAM-2醫學影像分割", "summary_zh": "在醫學影像領域，如何有效利用大量未標記數據，同時減少對高質量像素級標註的依賴，是個重要挑戰。本研究提出SSS方法，利用SAM-2強大的特徵提取能力，挖掘未標記醫學影像中的潛在知識，增強全監督醫學影像分割的特徵支持。透過“弱到強”一致性正則化框架，引入判別特徵增強機制，探索多視角數據增強策略帶來的特徵差異，重建和建模特徵，有效優化顯著區域。此外，開發結合物理約束和滑動窗口的提示生成器，滿足SAM-2對額外提示的需求。實驗證明，SSS在半監督醫學影像分割上優於現有方法。", "applications": ["遠距醫療影像判讀：偏鄉地區醫療資源有限，透過SSS技術，即使只有少量標記數據，也能訓練出準確的AI模型輔助醫生判讀X光片、CT掃描等影像，及早發現疾病。", "AI輔助手術導航：在手術過程中，醫生可以利用SSS技術快速分析患者的MRI影像，即時生成精確的器官和病灶分割圖，幫助醫生更精準地定位和切除病灶，減少手術風險。", "個人化健康管理：未來，SSS技術可以應用於穿戴式裝置或手機App，分析用戶上傳的醫療影像（如皮膚照片、眼底照片），進行初步的健康風險評估，提供個人化的健康建議。"], "pitch": "各位投資人，我們正站在醫學影像AI的黃金交叉點！SSS技術，不僅解決了醫學影像標註數據稀缺的痛點，更充分釋放了Vision Foundation Model在醫療領域的潛力。想像一下，全球醫療機構每年產生海量的未標記影像數據，SSS就像一把金鑰匙，能將這些沉睡的數據轉化為強大的AI診斷能力。我們的技術不僅能提升診斷效率、降低醫療成本，更能加速新藥研發、實現精準醫療。我們預計，未來五年內，SSS將成為醫學影像AI市場的關鍵技術，搶佔先機者，必將贏得豐厚回報！我們團隊擁有深厚的AI技術積累和豐富的醫療領域經驗，期待與各位攜手，共創醫學影像AI的新紀元！", "audio": "docs/data/audios/2506.08949v1.wav"}
{"query": "Diffusion Model", "id": "2506.08677v1", "url": "http://arxiv.org/abs/2506.08677v1", "title": "MAMBO: High-Resolution Generative Approach for Mammography Images", "summary": "Mammography is the gold standard for the detection and diagnosis of breast\ncancer. This procedure can be significantly enhanced with Artificial\nIntelligence (AI)-based software, which assists radiologists in identifying\nabnormalities. However, training AI systems requires large and diverse\ndatasets, which are often difficult to obtain due to privacy and ethical\nconstraints. To address this issue, the paper introduces MAMmography ensemBle\nmOdel (MAMBO), a novel patch-based diffusion approach designed to generate\nfull-resolution mammograms. Diffusion models have shown breakthrough results in\nrealistic image generation, yet few studies have focused on mammograms, and\nnone have successfully generated high-resolution outputs required to capture\nfine-grained features of small lesions. To achieve this, MAMBO integrates\nseparate diffusion models to capture both local and global (image-level)\ncontexts. The contextual information is then fed into the final patch-based\nmodel, significantly aiding the noise removal process. This thoughtful design\nenables MAMBO to generate highly realistic mammograms of up to 3840x3840\npixels. Importantly, this approach can be used to enhance the training of\nclassification models and extended to anomaly detection. Experiments, both\nnumerical and radiologist validation, assess MAMBO's capabilities in image\ngeneration, super-resolution, and anomaly detection, highlighting its potential\nto enhance mammography analysis for more accurate diagnoses and earlier lesion\ndetection.", "authors": ["Milica Škipina", "Nikola Jovišić", "Nicola Dall'Asen", "Vanja Švenda", "Anil Osman Tur", "Slobodan Ilić", "Elisa Ricci", "Dubravko Ćulibrk"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:33:24.093007", "title_zh": "MAMBO：用於乳房X光影像的高解析度生成方法", "summary_zh": "乳房X光攝影是乳癌檢測的黃金標準。為解決AI訓練數據不足的問題，本研究提出MAMBO，一種基於擴散模型的新穎方法，用於生成完整解析度的乳房X光影像。MAMBO整合了多個擴散模型，捕捉局部和全域的上下文資訊，生成高達3840x3840像素的逼真乳房X光片。這種方法能有效提升分類模型的訓練，並擴展至異常檢測。實驗證明，MAMBO在影像生成、超解析度及異常檢測方面表現出色，有潛力提升乳房X光分析的準確性，並提早發現病灶。", "applications": ["想像一下，未來在家就能使用手機App，上傳乳房X光片進行初步AI分析，及早發現潛在風險，省去奔波醫院的時間。", "醫療資源匱乏的偏遠地區，可以利用MAMBO生成的合成數據，訓練AI模型，提升當地醫療人員的診斷能力。", "醫學研究人員可以利用MAMBO生成大量不同病灶的乳房X光片，加速新藥開發和治療方案的驗證。"], "pitch": "各位投資人，我們正處於AI醫療革命的風口浪尖！MAMBO技術不僅能生成高擬真乳房X光影像，解決數據匱乏的難題，更重要的是，它能賦能AI，提升乳癌檢測的準確性和效率。想像一下，一個能及早發現微小病灶的AI系統，能拯救多少生命！MAMBO的應用前景廣闊，從遠程醫療、AI輔助診斷，到新藥開發，都蘊藏著巨大的商業價值。我們不僅僅在銷售軟體，我們是在投資未來，一個更健康、更智能的未來。現在加入我們，共同打造AI醫療的新紀元，回報將遠超您的想像！", "audio": "docs/data/audios/2506.08677v1.wav"}
{"query": "AI", "id": "2506.08945v1", "url": "http://arxiv.org/abs/2506.08945v1", "title": "Who is using AI to code? Global diffusion and impact of generative AI", "summary": "Generative coding tools promise big productivity gains, but uneven uptake\ncould widen skill and income gaps. We train a neural classifier to spot\nAI-generated Python functions in 80 million GitHub commits (2018-2024) by\n200,000 developers and track how fast--and where--these tools take hold. By\nDecember 2024, AI wrote an estimated 30.1% of Python functions from U.S.\ncontributors, versus 24.3% in Germany, 23.2% in France, 21.6% in India, 15.4%\nin Russia and 11.7% in China. Newer GitHub users use AI more than veterans,\nwhile male and female developers adopt at similar rates. Within-developer\nfixed-effects models show that moving to 30% AI use raises quarterly commits by\n2.4%. Coupling this effect with occupational task and wage data puts the annual\nvalue of AI-assisted coding in the United States at $9.6-$14.4 billion, rising\nto $64-$96 billion if we assume higher estimates of productivity effects\nreported by randomized control trials. Moreover, generative AI prompts learning\nand innovation, leading to increases in the number of new libraries and library\ncombinations that programmers use. In short, AI usage is already widespread but\nhighly uneven, and the intensity of use, not only access, drives measurable\ngains in output and exploration.", "authors": ["Simone Daniotti", "Johannes Wachs", "Xiangnan Feng", "Frank Neffke"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:37:04.495639", "title_zh": "誰在使用AI編碼？生成式AI的全球擴散與影響", "summary_zh": "這項研究分析了GitHub上8000萬個提交紀錄，發現生成式AI正在快速滲透程式碼開發。截至2024年底，美國開發者撰寫的Python函數中，約有30.1%由AI生成，領先其他國家。研究顯示，新進開發者比資深開發者更常使用AI，且男女開發者的採用率相似。更重要的是，AI的使用提升了開發效率，估計每年為美國帶來數十億美元的經濟價值。此外，AI還能促進學習與創新，增加程式設計師使用的程式庫種類。總之，AI編碼已廣泛應用，但使用程度不均，且使用強度而非僅僅是存取，才能真正推動產出和探索的顯著提升。", "applications": ["1. **新手程式設計師的救星：** 想像一下，程式新手再也不用害怕複雜的語法。AI編碼工具就像一位耐心的導師，能自動生成程式碼片段，讓學習曲線更平緩，更快上手。", "2. **效率提升神器：** 對於資深工程師來說，AI編碼工具可以自動完成重複性的工作，例如撰寫基礎程式碼或測試案例，讓他們能專注於更具挑戰性和創造性的任務，大幅提升工作效率。", "3. **個人化學習助手：** 學生可以使用AI編碼工具來生成不同的程式碼範例，從而更深入地理解程式設計概念。AI可以根據學生的學習進度和偏好，提供客製化的學習體驗。"], "pitch": "各位創投夥伴，我們正在見證一場程式碼革命！這項研究證明，生成式AI不僅是個噱頭，而是程式開發的未來。想像一下，一個程式設計師的生產力提升數倍，整個軟體開發的週期縮短，新創企業能更快推出產品，大型企業也能更靈活地應對市場變化。我們的技術不僅能追蹤AI編碼的使用情況，更能深入分析其對生產力、創新和經濟的影響。這代表著巨大的商業機會！我們可以開發更智能的AI編碼工具，提供客製化的程式碼生成服務，甚至建立一個AI驅動的程式碼市場。未來，AI編碼將成為各行各業的基礎設施，從醫療保健到金融，都將受益於AI編碼帶來的效率提升和創新加速。現在投資，就能搶佔先機，成為這場革命的領航者，共同打造一個AI賦能的程式碼未來！", "audio": "docs/data/audios/2506.08945v1.wav"}
{"query": "Foundation Model", "id": "2506.08936v1", "url": "http://arxiv.org/abs/2506.08936v1", "title": "BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models", "summary": "We present BioLangFusion, a simple approach for integrating pre-trained DNA,\nmRNA, and protein language models into unified molecular representations.\nMotivated by the central dogma of molecular biology (information flow from gene\nto transcript to protein), we align per-modality embeddings at the biologically\nmeaningful codon level (three nucleotides encoding one amino acid) to ensure\ndirect cross-modal correspondence. BioLangFusion studies three standard fusion\ntechniques: (i) codon-level embedding concatenation, (ii) entropy-regularized\nattention pooling inspired by multiple-instance learning, and (iii) cross-modal\nmulti-head attention -- each technique providing a different inductive bias for\ncombining modality-specific signals. These methods require no additional\npre-training or modification of the base models, allowing straightforward\nintegration with existing sequence-based foundation models. Across five\nmolecular property prediction tasks, BioLangFusion outperforms strong unimodal\nbaselines, showing that even simple fusion of pre-trained models can capture\ncomplementary multi-omic information with minimal overhead.", "authors": ["Amina Mollaysa", "Artem Moskale", "Pushpak Pati", "Tommaso Mansi", "Mangal Prakash", "Rui Liao"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:38:26.082676", "title_zh": "BioLangFusion：DNA、mRNA和蛋白質語言模型的多模態融合", "summary_zh": "BioLangFusion是一種整合DNA、mRNA和蛋白質預訓練語言模型的簡潔方法，旨在創建統一的分子表示。基於分子生物學中心法則（基因到轉錄本到蛋白質的信息流），我們在生物學上具有意義的密碼子層面（三個核苷酸編碼一個氨基酸）對齊了各模態的嵌入，確保直接的跨模態對應關係。BioLangFusion研究了三種標準融合技術：密碼子層面的嵌入串聯、受多實例學習啟發的熵正則化注意力池化，以及跨模態多頭注意力。這些方法無需額外的預訓練或修改基礎模型，即可與現有的基於序列的基礎模型直接整合。在五項分子特性預測任務中，BioLangFusion優於強大的單模態基線，表明即使是預訓練模型的簡單融合也能以最小的開銷捕獲互補的多組學信息。", "applications": ["個人化醫療：根據個人的基因、mRNA和蛋白質數據，更精準地預測藥物反應，從而實現更有效的治療。", "疾病診斷：通過分析患者的多組學數據，早期發現疾病的生物標記，提高診斷的準確性和效率。", "新藥開發：加速新藥的篩選和設計過程，通過模擬不同分子之間的相互作用，預測藥物的療效和安全性。"], "pitch": "各位投資人，想像一下，如果我們能像解讀語言一樣解讀生命密碼，精準預測疾病、設計藥物，甚至創造全新的生物技術，那將會是怎樣的變革？BioLangFusion正是這樣一個顛覆性的技術。它整合了DNA、mRNA和蛋白質等多組學數據，打造了一個強大的AI模型，能夠以前所未有的精度預測分子特性。這不僅僅是技術突破，更是開啟了個人化醫療、精準診斷和加速新藥開發的鑰匙。我們預計，BioLangFusion將在未來五年內徹底改變生物醫藥行業，創造數十億美元的市場價值。現在加入我們，共同投資BioLangFusion，搶占生物醫藥AI的制高點，共同見證生命科學的下一次重大突破！", "audio": "docs/data/audios/2506.08936v1.wav"}
{"query": "Diffusion Model", "id": "2506.08632v1", "url": "http://arxiv.org/abs/2506.08632v1", "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping", "summary": "Recent advancements in generative models have revolutionized video synthesis\nand editing. However, the scarcity of diverse, high-quality datasets continues\nto hinder video-conditioned robotic learning, limiting cross-platform\ngeneralization. In this work, we address the challenge of swapping a robotic\narm in one video with another: a key step for crossembodiment learning. Unlike\nprevious methods that depend on paired video demonstrations in the same\nenvironmental settings, our proposed framework, RoboSwap, operates on unpaired\ndata from diverse environments, alleviating the data collection needs. RoboSwap\nintroduces a novel video editing pipeline integrating both GANs and diffusion\nmodels, combining their isolated advantages. Specifically, we segment robotic\narms from their backgrounds and train an unpaired GAN model to translate one\nrobotic arm to another. The translated arm is blended with the original video\nbackground and refined with a diffusion model to enhance coherence, motion\nrealism and object interaction. The GAN and diffusion stages are trained\nindependently. Our experiments demonstrate that RoboSwap outperforms\nstate-of-the-art video and image editing models on three benchmarks in terms of\nboth structural coherence and motion consistency, thereby offering a robust\nsolution for generating reliable, cross-embodiment data in robotic learning.", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Dong Chen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:39:54.486327", "title_zh": "RoboSwap：一個基於GAN驅動的影片擴散框架，用於無監督機器手臂替換", "summary_zh": "RoboSwap 是一個創新的影片編輯框架，旨在解決機器人學習中數據稀缺的問題。它利用生成對抗網路（GAN）和擴散模型，實現了在影片中無監督地替換機器手臂。首先，RoboSwap 將機器手臂從背景中分割出來，並使用 GAN 將一個機器手臂轉換為另一個。然後，將轉換後的機器手臂與原始影片背景融合，並使用擴散模型來增強影片的連貫性、運動真實感和物體交互。實驗結果表明，RoboSwap 在結構連貫性和運動一致性方面優於現有的影片和圖像編輯模型，為機器人學習中生成可靠的跨平台數據提供了一個強大的解決方案。這降低了對大量配對影片資料的需求，加速機器人學習的進程。", "applications": ["想像一下，你可以用手機App輕鬆更換影片中的機器手臂，讓它看起來像是在執行不同的任務。比如，你想展示你的機器人手臂組裝家具的能力，但你只有它在組裝電子元件的影片，RoboSwap 就能幫你把影片中的電子元件替換成家具。", "在機器人教育領域，老師可以利用 RoboSwap 快速生成各種不同機器手臂執行任務的影片，讓學生更直觀地學習不同手臂的特性和應用，而無需實際購買和操作所有型號的機器手臂。", "製造業的工程師可以利用 RoboSwap 模擬不同機器手臂在生產線上的工作情況，優化生產流程，預先評估新手臂的性能，而無需進行昂貴的物理原型測試。"], "pitch": "RoboSwap 不僅僅是一個影片編輯工具，它是機器人學習領域的革命性技術！我們正在解決機器人數據稀缺這個長期存在的痛點，釋放機器人跨平台學習的巨大潛力。想像一下，一個機器人可以在虛擬環境中學習無數種技能，然後將這些技能無縫轉移到真實世界。RoboSwap 正是實現這一願景的關鍵。它能大幅降低機器人學習的成本和時間，加速機器人技術在各行各業的應用。我們的技術具有巨大的商業價值，從機器人教育、製造業到娛樂業，都有廣闊的市場前景。我們相信，RoboSwap 將成為機器人時代的 Adobe Photoshop，引領下一代機器人技術的發展。現在投資 RoboSwap，就是投資機器人技術的未來！", "audio": "docs/data/audios/2506.08632v1.wav"}
{"query": "AI", "id": "2506.08935v1", "url": "http://arxiv.org/abs/2506.08935v1", "title": "Can A Gamer Train A Mathematical Reasoning Model?", "summary": "While large language models (LLMs) have achieved remarkable performance in\nvarious tasks including mathematical reasoning, their development typically\ndemands prohibitive computational resources. Recent advancements have reduced\ncosts for training capable models, yet even these approaches rely on high-end\nhardware clusters. In this paper, we demonstrate that a single average gaming\nGPU can train a solid mathematical reasoning model, by integrating\nreinforcement learning and memory optimization techniques. Specifically, we\ntrain a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB\nmemory that achieves comparable or better performance on mathematical reasoning\nbenchmarks than models several times larger, in resource-constrained\nenvironments. Our results challenge the paradigm that state-of-the-art\nmathematical reasoning necessitates massive infrastructure, democratizing\naccess to high-performance AI research.\nhttps://github.com/shinandrew/YouronMath.", "authors": ["Andrew Shin"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:25:12.984985", "title_zh": "遊戲玩家也能訓練數學推理模型嗎？", "summary_zh": "本研究展示，僅需一張普通的遊戲級GPU，透過強化學習和記憶體優化技術，就能訓練出一個強大的數學推理模型。我們在配備16GB記憶體的RTX 3080 Ti上訓練了一個15億參數的模型，在資源有限的環境下，其數學推理能力可與規模更大的模型相媲美甚至超越。這項成果挑戰了頂尖數學推理必須依賴龐大基礎設施的傳統觀念，有助於普及高效能AI研究。", "applications": ["1. 孩子在家用電腦就能訓練AI家教：不必花大錢購買昂貴的伺服器，只要用家裡玩遊戲的電腦，就能訓練出能解答數學題目的AI，輔導孩子學習。", "2. 偏鄉學校也能有AI輔助教學：資源不足的學校，也能利用現有的電腦設備，訓練出針對性的AI模型，提升學生的學習效果。", "3. 個人開發者也能參與AI研究：降低了AI研究的門檻，讓更多有興趣的個人開發者，也能參與到數學推理AI的研究與開發中。"], "pitch": "各位投資人，想像一下，AI不再是巨頭的遊戲！我們突破性技術讓AI數學推理模型的訓練成本大幅降低，只需一張遊戲顯卡，就能達到甚至超越過去需要昂貴伺服器才能實現的性能。這意味著什麼？\n\n首先，市場潛力巨大！教育科技市場正蓬勃發展，我們的技術能讓每個家庭、每所學校都能擁有客製化的AI家教，大幅降低教育成本，提升學習效率。其次，我們將顛覆AI研究的格局，讓更多個人開發者和小型團隊參與進來，加速AI技術的創新與應用。更重要的是，這項技術可以應用於金融建模、科學研究等領域，創造巨大的商業價值。我們正在打造一個更普惠、更民主化的AI未來，現在加入，您將成為這場變革的領航者！", "audio": "docs/data/audios/2506.08935v1.wav"}
{"query": "Foundation Model", "id": "2506.08902v1", "url": "http://arxiv.org/abs/2506.08902v1", "title": "Intention-Conditioned Flow Occupancy Models", "summary": "Large-scale pre-training has fundamentally changed how machine learning\nresearch is done today: large foundation models are trained once, and then can\nbe used by anyone in the community (including those without data or compute\nresources to train a model from scratch) to adapt and fine-tune to specific\ntasks. Applying this same framework to reinforcement learning (RL) is appealing\nbecause it offers compelling avenues for addressing core challenges in RL,\nincluding sample efficiency and robustness. However, there remains a\nfundamental challenge to pre-train large models in the context of RL: actions\nhave long-term dependencies, so training a foundation model that reasons across\ntime is important. Recent advances in generative AI have provided new tools for\nmodeling highly complex distributions. In this paper, we build a probabilistic\nmodel to predict which states an agent will visit in the temporally distant\nfuture (i.e., an occupancy measure) using flow matching. As large datasets are\noften constructed by many distinct users performing distinct tasks, we include\nin our model a latent variable capturing the user intention. This intention\nincreases the expressivity of our model, and enables adaptation with\ngeneralized policy improvement. We call our proposed method\nintention-conditioned flow occupancy models (InFOM). Comparing with alternative\nmethods for pre-training, our experiments on $36$ state-based and $4$\nimage-based benchmark tasks demonstrate that the proposed method achieves $1.8\n\\times$ median improvement in returns and increases success rates by $36\\%$.\nWebsite: https://chongyi-zheng.github.io/infom Code:\nhttps://github.com/chongyi-zheng/infom", "authors": ["Chongyi Zheng", "Seohong Park", "Sergey Levine", "Benjamin Eysenbach"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:26:28.700147", "title_zh": "意圖條件式流動佔據模型", "summary_zh": "本研究提出一種名為「意圖條件式流動佔據模型」（InFOM）的強化學習預訓練方法。InFOM利用流動匹配技術，預測智能體在未來將訪問哪些狀態，並通過潛在變量捕捉不同使用者的意圖。這種方法能有效處理強化學習中長期依賴性的問題，提高樣本效率和穩健性。實驗結果顯示，InFOM在多項基準測試中，回報中位數提高了1.8倍，成功率提升了36%。InFOM為強化學習的大規模預訓練提供了一條新途徑，有助於開發更強大、更通用的智能體。", "applications": ["自動駕駛：InFOM可以幫助自動駕駛系統預測其他車輛或行人的意圖，從而做出更安全、更有效的決策，例如預測行人是否會突然衝出馬路，提前減速避讓。", "智慧醫療：InFOM可以用於輔助醫生制定治療方案，預測患者對不同治療方案的反應，並根據患者的個體差異進行個性化治療。", "機器人流程自動化（RPA）：InFOM可以讓機器人更好地理解使用者的意圖，從而更靈活、更智能地執行各種任務，例如自動處理電子郵件、填寫表單等。"], "pitch": "各位創投先進，我們正處於AI的黃金時代！想像一下，如果機器能像人一樣，不僅學習技能，更能理解『意圖』，會發生什麼？我們的「意圖條件式流動佔據模型」（InFOM）正是實現這一點的關鍵技術。它讓機器在強化學習中，能預測未來、理解使用者意圖，大幅提升學習效率和決策能力。這意味著，從自駕車到醫療診斷，再到工業自動化，InFOM擁有無限潛力！\n\n試想：未來的自駕車不再只是遵守交通規則，更能預測行人意圖，避免事故；醫療AI不再只是分析數據，更能理解患者需求，提供個性化治療；工廠機器人不再只是重複動作，更能理解生產目標，自主優化流程。InFOM將賦予機器前所未有的智能，徹底顛覆各行各業！\n\n我們已經證明InFOM在多個基準測試中表現出色，領先現有技術。現在，我們需要您的資金，將InFOM推向市場，打造一個全新的AI時代。這不僅僅是一項投資，更是一次參與AI革命的機會！讓我們一起攜手，讓InFOM成為AI領域的下一個獨角獸！", "audio": "docs/data/audios/2506.08902v1.wav"}
{"query": "Diffusion Model", "id": "2506.08617v1", "url": "http://arxiv.org/abs/2506.08617v1", "title": "Diffusion model for analyzing quantum fingerprints in conductance fluctuation", "summary": "A conditional diffusion model has been developed to analyze intricate\nconductance fluctuations called universal conductance fluctuations or quantum\nfingerprints appearing in quantum transport phenomena. The model reconstructs\nimpurity arrangements and quantum interference patterns in nanometals by using\nmagnetoconductance data, providing a novel approach to analyze complex data\nbased on machine learning. In addition, we visualize the attention weights in\nthe model, which efficiently extract information on the non-local correlation\nof the electron wave functions, and the score functions, which represent the\nforce fields in the wave-function space.", "authors": ["Naoto Yokoi", "Yuki Tanaka", "Yukito Nonaka", "Shunsuke Daimon", "Junji Haruyama", "Eiji Saitoh"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:27:18.184257", "title_zh": "用於分析電導波動中量子指紋的擴散模型", "summary_zh": "本研究開發了一種條件擴散模型，用於分析量子傳輸現象中複雜的電導波動，即所謂的通用電導波動或量子指紋。該模型利用磁電導數據重建奈米金屬中的雜質排列和量子干涉圖案，為基於機器學習的複雜數據分析提供了一種新穎方法。此外，我們可視化模型中的注意力權重，有效地提取電子波函數的非局部相關性信息，以及表示波函數空間中力場的評分函數。", "applications": ["想像一下，醫生可以利用這項技術，透過簡單的電導測量，就能了解奈米級藥物在人體內的分布和作用方式，從而更精準地用藥。", "工廠可以利用這項技術，監測奈米材料的生產過程，即時發現並修正瑕疵，確保產品品質穩定可靠。", "科學家可以利用這項技術，更深入地了解量子世界的奧秘，例如超導材料的特性，進而開發出更高效能的電子元件。"], "pitch": "各位投資人，我們帶來一項顛覆性的技術，它能解鎖量子世界的秘密！想像一下，我們能像掃描指紋一樣，精準掌握奈米材料的內部結構。這項基於擴散模型的量子指紋分析技術，不僅能應用於材料科學、醫學診斷，更能催生新一代的量子電腦和高效能電子元件。我們預期，隨著量子技術的發展，這項技術將成為各行各業不可或缺的工具，市場潛力無可限量。現在加入我們，一起開創量子科技的新紀元！", "audio": "docs/data/audios/2506.08617v1.wav"}
{"query": "AI", "id": "2506.09988v1", "url": "http://arxiv.org/abs/2506.09988v1", "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "published_date": "2025-06-11", "timestamp": "2025-06-12T03:51:41.981962", "title_zh": "EditInspector：文本引導圖像編輯評估基準", "summary_zh": "隨著生成式AI的快速發展，文本引導圖像編輯技術日益普及。然而，我們缺乏一個完善的框架來驗證和評估這些編輯的品質。為此，我們推出了EditInspector，一個基於人工標註的新基準，用於評估文本引導圖像編輯。透過EditInspector，我們評估了當前最先進的視覺和語言模型在各個維度上的編輯評估能力，包括準確性、偽影檢測、視覺品質、與場景的融合度、常識一致性以及描述編輯引起變化的能力。研究發現，現有模型在全面評估編輯方面存在困難，並且在描述變化時經常產生幻覺。為了解決這些問題，我們提出了兩種新方法，它們在偽影檢測和差異描述生成方面均優於現有模型。", "applications": ["**線上購物：**想像一下，你想買一件紅色外套，但網站上只有藍色的。你可以用文字描述「將外套顏色改成紅色」，AI就能自動幫你換顏色，讓你更清楚看到穿起來的效果，減少買錯的機會。", "**社交媒體濾鏡：**現在的濾鏡只能簡單美顏或加特效。未來，你可以用文字描述，例如「讓我的眼睛更大更有神」，AI就能根據你的描述，精準調整你的五官，打造獨一無二的個人化濾鏡。", "**遊戲角色客製化：**在遊戲中，你可以用文字描述你想要的角色外觀，例如「讓他的頭髮變成銀白色，眼睛變成藍色」，AI就能立即生成符合你描述的角色外觀，讓你的遊戲體驗更加個性化。"], "pitch": "各位投資人，我們正處於AI圖像編輯革命的風口浪尖！EditInspector不僅是一個基準，更是一個加速器，它將推動文本引導圖像編輯技術的發展，並催生出巨大的商業價值。想像一下，未來人人都能輕鬆創造出精美的圖像內容，無需專業技能，這將顛覆設計、行銷、電商等各個產業。我們的技術能有效解決現有AI編輯的缺陷，提高編輯品質和準確性，具有極高的市場競爭力。我們預計，在未來五年內，文本引導圖像編輯市場將達到數十億美元的規模，而EditInspector將成為這個市場的關鍵基礎設施。現在投資我們，您將有機會參與這場AI圖像編輯的革命，共同創造一個充滿創意和可能性的未來！我們不僅僅是在開發技術，更是在打造一個全新的視覺表達生態系統，邀請您與我們一同開創這個新時代！", "audio": "docs/data/audios/2506.09988v1.wav"}
{"query": "Foundation Model", "id": "2506.09982v1", "url": "http://arxiv.org/abs/2506.09982v1", "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation", "summary": "Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.", "authors": ["Zijie Wu", "Chaohui Yu", "Fan Wang", "Xiang Bai"], "published_date": "2025-06-11", "timestamp": "2025-06-12T03:52:55.119943", "title_zh": "AnimateAnyMesh：一個用於文本驅動通用網格動畫的前饋式4D基礎模型", "summary_zh": "AnimateAnyMesh是一個創新的前饋式框架，能快速根據文字描述生成任意3D網格動畫。它利用獨特的DyMeshVAE架構，有效壓縮和重建動態網格序列，同時分離空間和時間特徵，並保留局部拓撲結構。透過在壓縮潛在空間中使用基於修正流的訓練策略，實現高質量文本條件生成。此外，我們還創建了包含超過400萬個帶有文本註釋的多樣化動態網格序列的DyMesh數據集。實驗結果表明，該方法能在幾秒鐘內生成語義準確且時間連貫的網格動畫，在質量和效率上均顯著優於現有方法。我們的研究是使4D內容創建更易於訪問和實用的重要一步。", "applications": ["想像一下，你可以用手機拍一張靜態雕像的照片，然後輸入文字「跳舞」，雕像就能立刻在你手機螢幕上跳起舞來！", "遊戲開發者再也不用花費大量時間手動製作角色動畫，只要輸入指令「英雄奔跑」，就能自動生成逼真的跑步動作。", "建築師可以輸入「大樓在地震中搖晃」，立即模擬建築物在地震中的反應，以便更好地設計抗震結構。"], "pitch": "各位投資人，我們正在打造的是4D內容生成的未來！AnimateAnyMesh不僅是一個技術突破，更是一個潛力無限的市場金礦。想像一下，一個讓任何人都能輕鬆創造逼真3D動畫的世界，這將顛覆遊戲、電影、設計、教育等無數產業。我們的技術能大幅降低動畫製作門檻和成本，讓創意無限釋放。未來，我們可以將AnimateAnyMesh整合到元宇宙平台，讓用戶創造個性化虛擬化身和互動內容；授權給電商平台，讓消費者更直觀地預覽商品；甚至應用於醫療領域，模擬手術過程，提升培訓效果。我們擁有的不僅是領先的技術，更是創造一個全新內容生態的願景。現在加入我們，一起引領4D內容革命，共同分享這個千億級市場的巨大紅利！", "audio": "docs/data/audios/2506.09982v1.wav"}
{"query": "Diffusion Model", "id": "2506.09993v1", "url": "http://arxiv.org/abs/2506.09993v1", "title": "Text-Aware Image Restoration with Diffusion Models", "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "published_date": "2025-06-11", "timestamp": "2025-06-12T03:54:25.769230", "title_zh": "基於擴散模型的文本感知圖像修復", "summary_zh": "本研究提出「文本感知圖像修復」(TAIR) 任務，旨在同時恢復圖像的視覺內容和文本的準確性。現有基於擴散模型的圖像修復方法，在處理自然圖像時表現出色，但在還原圖像中的文字區域時，經常產生看似合理但錯誤的文字圖樣，稱為「文本圖像幻覺」。為解決此問題，我們建立了一個包含10萬張高質量場景圖像的大規模基準測試集SA-Text，並提出一個名為TeReDiff的多任務擴散框架，將擴散模型的內部特徵整合到文本檢測模塊中，實現聯合訓練，從而提取豐富的文本表示，並將其用作後續去噪步驟的提示。實驗結果表明，我們的TAIR方法在文本識別準確性方面顯著優於現有的圖像修復方法。", "applications": ["**智能交通監控：** 想像一下，透過AI修復模糊的車牌，即使在惡劣天氣或光線不足的情況下，也能清晰辨識違規車輛或追蹤失竊車輛，大幅提升執法效率。", "**歷史文獻數位化：** 將老舊、破損的古籍或文件圖像修復，使模糊的文字變得清晰可讀，讓珍貴的歷史資料得以保存和研究，重現歷史的樣貌。", "**安全監控系統：** 還原監視器畫面中模糊的文字，例如商店招牌或公告欄，協助警方快速掌握案發現場的關鍵資訊，提升破案率。"], "pitch": "各位投資人，我們團隊帶來的是革命性的「文本感知圖像修復」技術，簡稱TAIR。這項技術不僅能修復模糊圖像，更厲害的是，它能精準還原圖像中的文字資訊，解決了傳統圖像修復技術的痛點。試想一下，未來無人車需要清晰辨識路標，執法單位需要還原監視器畫面中的關鍵資訊，歷史學家需要解讀古籍中的模糊文字，這些都離不開TAIR。我們已建立大規模的基準測試集，並開發出領先業界的TeReDiff框架，實驗數據證明我們的技術遠勝於現有方案。我們預計，TAIR將在智能交通、安全監控、文獻數位化等領域掀起巨大變革，成為AI圖像處理領域的黃金標準。現在加入我們，您將參與一個極具潛力的市場，共同開創AI圖像修復的新紀元！", "audio": "docs/data/audios/2506.09993v1.wav"}
{"query": "AI", "id": "2506.09985v1", "url": "http://arxiv.org/abs/2506.09985v1", "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "published_date": "2025-06-11", "timestamp": "2025-06-12T06:37:55.238033", "title_zh": "V-JEPA 2：自我監督式影片模型實現理解、預測與規劃", "summary_zh": "V-JEPA 2是一種透過觀看大量網路影片，學習理解世界並採取行動的AI模型。它結合網路影片資料和少量機器人互動資料，預訓練出一個能理解、預測和規劃的架構。在動作理解和人類行為預測上，V-JEPA 2都表現出色。與大型語言模型結合後，它在影片問答任務上也達到領先水準。更厲害的是，V-JEPA 2還能應用於機器人規劃任務，讓機器人在新環境中，無需任何訓練或獎勵，就能完成拾取和放置物體的任務。這項技術展現了自我監督學習在現實世界中的巨大潛力。", "applications": ["智慧家庭：V-JEPA 2可以讓你的掃地機器人更聰明，不只會掃地，還能辨識障礙物、預測你的行走路線，甚至在你快跌倒時及時扶你一把！", "自動駕駛：有了V-JEPA 2，自駕車不只能看到紅綠燈，更能預測行人意圖、判斷路況變化，真正做到安全又聰明的自動駕駛。", "遠端醫療：醫生可以透過V-JEPA 2控制遠端的機器人手臂，進行精準的手術或檢查，即使身在偏遠地區也能獲得最好的醫療照護。"], "pitch": "各位投資人，想像一下，一個AI能像人類一樣，透過觀察學習、理解世界，並做出合理的預測和規劃。V-JEPA 2正是這樣的突破性技術！它不僅在學術界取得領先成果，更具備巨大的商業潛力。試想，將V-JEPA 2應用於工業自動化，可以大幅提升生產效率和品質；應用於無人倉儲，可以實現更智慧化的物流管理；甚至應用於虛擬實境，可以打造更逼真、更具互動性的沉浸式體驗。我們相信，V-JEPA 2將引領下一波AI革命，成為各行各業不可或缺的核心技術。現在投資V-JEPA 2，就是投資未來！我們預期在五年內，V-JEPA 2相關應用市場規模將達到數十億美元，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.09985v1.wav"}
{"query": "Foundation Model", "id": "2506.09883v1", "url": "http://arxiv.org/abs/2506.09883v1", "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "published_date": "2025-06-11", "timestamp": "2025-06-12T06:39:27.358869", "title_zh": "利用幾何蒸餾微調具備3D感知能力的視覺語言模型", "summary_zh": "現有的視覺語言模型在處理視覺和語言任務上表現出色，但在理解3D空間結構方面存在根本限制。我們提出一種名為「幾何蒸餾」的輕量級、無標註微調框架，將人類啟發的幾何線索注入到預訓練的視覺語言模型中，且不修改其架構。透過從現成的3D基礎模型（例如MASt3R、VGGT）中提取稀疏對應關係、相對深度關係和密集成本體積，我們的模型能夠塑造出具備幾何感知能力的表示，同時保持與自然圖像文本輸入的兼容性。在3D視覺語言推理和3D感知基準測試中，我們的模型始終優於先前的方法，以更低的計算成本實現了改進的3D空間推理。", "applications": ["**智慧導航：** 想像一下，你的手機不只知道你面前有什麼，還能理解這些東西的相對距離和空間關係。這樣，它就能更準確地導航，告訴你『走過紅色的郵筒後，左轉進入第二個巷口』，而不是模糊地說『往前走一段路後左轉』。", "**虛擬試穿/試用：** 在網路上買衣服或家具時，你可以透過手機鏡頭看到這些東西在你身上的效果，或者擺放在你家裡的样子，而且非常真實。這個技術可以讓你更放心地購物，減少退貨的麻煩。", "**AR遊戲互動：** AR遊戲可以更真實地與你的周圍環境互動。例如，遊戲中的怪物可以躲在你的桌子後面，而不是漂浮在半空中。這能讓遊戲體驗更有沉浸感和趣味性。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它能賦予AI如同人類般的3D空間感知能力。現有的AI在理解圖像和文字方面表現出色，但它們對真實世界的空間結構理解仍然有限。我們的「幾何蒸餾」技術，就像為AI裝上了一雙『3D眼鏡』，讓它們能夠真正『看懂』周圍的世界。這項技術的潛在商業價值巨大。想像一下，自動駕駛汽車可以更精準地避開障礙物，機器人可以更靈巧地執行任務，AR/VR體驗可以更加逼真。更重要的是，我們的技術成本低廉，可以輕鬆地整合到現有的AI系統中。我們相信，這項技術將會徹底改變人機互動的方式，並在自動駕駛、機器人、AR/VR等領域帶來巨大的商業機會。現在投資我們，您將成為這場變革的先驅者，共同開創AI的新時代！", "audio": "docs/data/audios/2506.09883v1.wav"}
{"query": "Diffusion Model", "id": "2506.09955v1", "url": "http://arxiv.org/abs/2506.09955v1", "title": "Canonical Latent Representations in Conditional Diffusion Models", "summary": "Conditional diffusion models (CDMs) have shown impressive performance across\na range of generative tasks. Their ability to model the full data distribution\nhas opened new avenues for analysis-by-synthesis in downstream discriminative\nlearning. However, this same modeling capacity causes CDMs to entangle the\nclass-defining features with irrelevant context, posing challenges to\nextracting robust and interpretable representations. To this end, we identify\nCanonical LAtent Representations (CLAReps), latent codes whose internal CDM\nfeatures preserve essential categorical information while discarding\nnon-discriminative signals. When decoded, CLAReps produce representative\nsamples for each class, offering an interpretable and compact summary of the\ncore class semantics with minimal irrelevant details. Exploiting CLAReps, we\ndevelop a novel diffusion-based feature-distillation paradigm, CaDistill. While\nthe student has full access to the training set, the CDM as teacher transfers\ncore class knowledge only via CLAReps, which amounts to merely 10 % of the\ntraining data in size. After training, the student achieves strong adversarial\nrobustness and generalization ability, focusing more on the class signals\ninstead of spurious background cues. Our findings suggest that CDMs can serve\nnot just as image generators but also as compact, interpretable teachers that\ncan drive robust representation learning.", "authors": ["Yitao Xu", "Tong Zhang", "Ehsan Pajouheshgar", "Sabine Süsstrunk"], "published_date": "2025-06-11", "timestamp": "2025-06-12T06:41:18.294383", "title_zh": "條件擴散模型中的典型潛在表示", "summary_zh": "條件擴散模型在生成任務中表現出色，但容易將類別特徵與無關背景糾纏，影響表示的穩健性和可解釋性。本研究提出典型潛在表示（CLAReps），它能保留關鍵類別信息，同時捨棄無關信號。解碼後，CLAReps能產生具代表性的類別樣本，簡潔地呈現核心語義，減少不必要的細節。我們基於CLAReps開發了名為CaDistill的特徵蒸餾方法，讓學生模型僅透過CLAReps（僅佔訓練數據的10%）學習，大幅提升對抗性魯棒性和泛化能力。這表明條件擴散模型不僅能生成圖像，還能作為精簡且可解釋的教師，驅動穩健的表示學習。", "applications": ["**AI藝術創作助手：** 輸入簡單的文字描述，就能生成特定風格或主題的圖像，且能有效避免生成不相關的背景元素，讓使用者更精準地控制創作內容，例如：輸入「貓咪坐在窗邊」，就能生成一張只有貓咪和窗戶的圖像，而不會出現雜亂的房間擺設。", "**醫療影像分析：** 在X光或MRI影像中，自動識別並突出顯示病灶區域，同時抑制正常組織的干擾，幫助醫生更快速、準確地診斷疾病，例如：在肺部X光片中，精準識別出腫瘤，並忽略肋骨的影響。", "**產品設計：** 根據使用者提供的關鍵需求，生成多種產品設計方案，並能有效避免生成不符合需求的設計元素，加速產品原型設計流程，例如：設計一款「輕便且防水的背包」，系統能生成多種符合條件的背包設計，而不會出現不防水的材質或過重的設計。"], "pitch": "各位投資人，我們正處於AI圖像生成技術的黃金時代，但現有模型往往產生過於複雜、難以控制的結果。想像一下，如果我們能像控制樂高積木一樣，精準地操控AI生成圖像的每一個細節，那將會開啟怎樣的商業潛力？\n\n我們的CLAReps技術，正是實現這一目標的關鍵。它就像一個AI圖像的「基因編輯器」，能讓我們精準地提取和重組圖像的核心特徵，生成高度可控、高度定制化的內容。\n\n這意味著，我們可以將AI圖像生成技術應用於更廣泛的領域：個性化廣告、定制化教育內容、甚至是虛擬世界的建設。想像一下，在元宇宙中，每個人都能輕鬆創建自己的專屬形象和場景，而無需掌握複雜的3D建模技術。這將是一個數十億美元的市場！\n\n更重要的是，我們的CaDistill特徵蒸餾方法，能大幅提升模型的魯棒性和泛化能力，使其在面對真實世界的複雜數據時，也能保持卓越的性能。這意味著，我們的技術不僅能生成漂亮的圖像，更能應用於自動駕駛、醫療診斷等對安全性要求極高的領域。\n\n我們相信，CLAReps技術將引領下一代AI圖像生成革命。現在加入我們，共同開創這個充滿無限可能的未來！", "audio": "docs/data/audios/2506.09955v1.wav"}
{"query": "AI", "id": "2506.09977v1", "url": "http://arxiv.org/abs/2506.09977v1", "title": "How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies", "summary": "Understanding how humans revise their beliefs in light of new information is\ncrucial for developing AI systems which can effectively model, and thus align\nwith, human reasoning. While theoretical belief revision frameworks rely on a\nset of principles that establish how these operations are performed, empirical\nevidence from cognitive psychology suggests that people may follow different\npatterns when presented with conflicting information. In this paper, we present\nthree comprehensive user studies showing that people consistently prefer\nexplanation-based revisions, i.e., those which are guided by explanations, that\nresult in changes to their belief systems that are not necessarily captured by\nclassical belief change theory. Our experiments systematically investigate how\npeople revise their beliefs with explanations for inconsistencies, whether they\nare provided with them or left to formulate them themselves, demonstrating a\nrobust preference for what may seem non-minimal revisions across different\ntypes of scenarios. These findings have implications for AI systems designed to\nmodel human reasoning or interact with humans, suggesting that such systems\nshould accommodate explanation-based, potentially non-minimal belief revision\noperators to better align with human cognitive processes.", "authors": ["Stylianos Loukas Vasileiou", "Antonio Rago", "Maria Vanina Martinez", "William Yeoh"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:53:19.277096", "title_zh": "人們如何修正不一致的信念？透過使用者研究檢驗人類的信念修正", "summary_zh": "本研究探討人類在面對新資訊時如何修正信念，這對於開發能有效模仿人類推理的人工智慧系統至關重要。研究發現，人們傾向於基於解釋來修正信念，即使這會導致與傳統信念變更理論不符的改變。透過三個使用者研究，我們發現人們更喜歡以解釋為基礎的修正，無論這些解釋是由系統提供還是由他們自己產生。這意味著，為了更好地與人類認知過程保持一致，AI系統應考慮納入基於解釋的、可能非最小化的信念修正運算。", "applications": ["**情境一：** 想像一個AI醫療診斷系統，當診斷結果與病患先前認知不符時，系統會提供詳細的解釋，例如說明罕見疾病的可能性或解釋檢測結果的誤差範圍，幫助病患理解並接受新的診斷。", "**情境二：** 在自動駕駛汽車中，如果汽車突然改變行駛路線，它會向乘客解釋原因，例如前方道路封閉或偵測到緊急狀況，讓乘客理解並信任汽車的決策。", "**情境三：** 線上教育平台可以根據學生的學習情況，提供個性化的學習建議。如果學生對某個概念理解有誤，系統不僅會指出錯誤，還會提供詳細的解釋和案例，幫助學生修正錯誤的觀念。"], "pitch": "各位投資人，我們正在開發一種革命性的人工智慧技術，它能像人類一樣思考和學習，關鍵在於理解並模擬人類的信念修正過程。傳統AI往往基於冷冰冰的邏輯，忽略了人類情感和解釋的重要性。我們的技術則不同，它能根據情境提供合理的解釋，讓人們更容易接受AI的決策，從而建立信任。試想一下，未來的AI助手不僅能完成任務，還能像一位智慧的顧問，提供建議並解釋背後的原因。這將顛覆醫療、教育、交通等各個領域。我們預計，這項技術將在五年內成為AI領域的標竿，帶來數十億美元的市場機會。現在加入我們，一起塑造AI的未來！", "audio": "docs/data/audios/2506.09977v1.wav"}
{"query": "Foundation Model", "id": "2506.09881v1", "url": "http://arxiv.org/abs/2506.09881v1", "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "authors": ["Siyu Chen", "Ting Han", "Chengzheng Fu", "Changshe Zhang", "Chaolei Wang", "Jinhe Su", "Guorong Cai", "Meiliu Wu"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:54:42.017818", "title_zh": "利用深度與語言實現開放詞彙領域泛化語義分割", "summary_zh": "本研究提出Vireo，一個創新的單階段框架，旨在解決開放詞彙領域泛化語義分割(OV-DGSS)問題。OV-DGSS的目標是在未見過的類別中生成像素級別的遮罩，同時保持在未見過領域中的穩健性，這對於惡劣條件下的自動駕駛等實際場景至關重要。Vireo基於凍結的視覺基礎模型(VFMs)，並通過深度VFMs整合場景幾何訊息，以提取領域不變的結構特徵。為了解決領域轉移下視覺和文本模態之間的差距，Vireo引入了GeoText Prompts、Coarse Mask Prior Embedding (CMPE) 和Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH) 三個關鍵組件。實驗結果表明，Vireo在領域泛化和開放詞彙識別方面均取得了最先進的性能，為多樣化和動態環境中的穩健視覺理解提供了一個統一且可擴展的解決方案。", "applications": ["1. 未來無人車能更精準辨識路上的障礙物，例如突然出現的小動物或是不熟悉的交通標誌，即使在雨天或濃霧中也能安全行駛。", "2. 智慧農業可以利用這項技術來辨識農田中不同的作物和雜草，幫助農民更有效率地噴灑農藥和施肥，提高農作物產量。", "3. 在醫療影像分析上，醫生能更準確地辨識X光片或斷層掃描中的病灶，像是腫瘤或骨折，提升診斷的準確性。"], "pitch": "各位投資人，想像一下，一個AI系統不僅能看懂這個世界，還能理解它！Vireo技術突破性地結合了深度感知和語言理解，讓機器在複雜、未知的環境中也能精準辨識物體。這意味著什麼？自動駕駛不再受限於特定路況，智慧城市管理更加高效，甚至醫療診斷也能大幅提升準確性。這項技術的應用潛力無窮，我們正在打造的是一個能真正理解世界的AI大腦！未來，我們可以將Vireo應用於虛擬實境、機器人導航、國防安全等領域，市場規模將是數十億美元級別。現在投資Vireo，您將站在AI革命的最前沿，共同開創一個更加智慧、安全、便捷的未來！", "audio": "docs/data/audios/2506.09881v1.wav"}
{"query": "Diffusion Model", "id": "2506.09932v1", "url": "http://arxiv.org/abs/2506.09932v1", "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:55:46.732901", "title_zh": "HadaNorm：透過均值中心轉換的擴散轉換器量化", "summary_zh": "擴散模型在圖像生成領域表現出色，但其高記憶體和計算需求限制了在資源有限設備上的應用。後訓練量化(PTQ)透過降低矩陣運算的位元寬度提供了解決方案。然而，標準PTQ方法難以處理離群值。本研究提出HadaNorm，一種新型線性轉換，透過在應用哈達瑪轉換之前正規化激活特徵通道來有效減輕離群值，從而實現更積極的激活量化。實驗證明，HadaNorm能有效降低轉換器各個組件的量化誤差，在效率和性能之間取得優於現有方法的平衡。", "applications": ["手機App修圖：即使手機硬體規格不高，也能快速生成高品質的圖片，讓修圖App更強大。", "智慧家居攝影機：在本地端即時處理影像，進行人臉辨識或物體偵測，無需將大量資料傳輸到雲端，保護隱私。", "無人機航拍：在電力有限的情況下，也能高效處理航拍影像，進行即時分析和地圖繪製。"], "pitch": "各位投資人，我們正處於AI圖像生成爆炸性成長的時代，但高昂的運算成本是阻礙其普及的關鍵。HadaNorm技術，能大幅降低擴散模型所需的硬體資源，讓AI圖像生成不再是雲端巨頭的專利，而是能真正走入每個人的生活。想像一下，未來的手機、無人機、甚至穿戴裝置，都能具備強大的圖像生成能力，這將開啟全新的應用場景和商業模式。我們的技術不僅優於現有方案，更具備極高的擴展性，未來可應用於其他AI模型，市場潛力無限。現在投資HadaNorm，就是投資AI的未來，讓我們一起打造更智慧、更普及的AI世界！", "audio": "docs/data/audios/2506.09932v1.wav"}
{"query": "AI", "id": "2506.09975v1", "url": "http://arxiv.org/abs/2506.09975v1", "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:29:20.805318", "title_zh": "當偵測失效時：微調模型產生類人社群媒體文本的力量", "summary_zh": "偵測AI生成的社群媒體文本極具挑戰，因文本短小且用語非正式。此問題至關重要，社群媒體是網路影響行動的重要攻擊載體，大量AI生成貼文可能被用來支持或反對特定政策。我們從威脅行為者的角度出發，創建包含超過50萬筆AI生成社群媒體貼文的數據集，涵蓋11個爭議性主題。研究表明，在攻擊者不公開其微調模型的情況下，偵測難度顯著增加。人類研究和消融實驗也證實了微調大型語言模型對各種偵測演算法的威脅。這項研究結果對所有偵測領域都有影響，因為微調是一種普遍適用且實際的大型語言模型使用方式。", "applications": ["**防止假新聞散播：**想像一下，這項技術能幫助社群平台自動標記或過濾掉由AI大量生成的、帶有特定政治立場的假新聞，讓大家看到的資訊更真實可靠。", "**保護品牌聲譽：**公司可以利用這項技術，監測網路上是否有大量AI生成的負面評論或攻擊，及時採取措施，保護品牌形象。", "**打擊網路詐騙：**偵測AI生成的詐騙訊息，例如假冒客服人員或親友的訊息，保護民眾免受詐騙。"], "pitch": "各位投資人，我們帶來的是一項劃時代的技術，它將顛覆AI內容偵測的遊戲規則。隨著AI生成內容的能力越來越強大，傳統的偵測方法已經捉襟見肘。我們的研究表明，通過微調模型，AI可以輕易生成難以與真人區分的社群媒體文本，這意味著網路上的虛假訊息、惡意攻擊和詐騙行為將更加猖獗。我們的技術能夠有效應對這種新型威脅，為社群平台、企業和政府提供強大的防禦能力。想像一下，未來每個社群平台、每個企業都需要我們的技術來保護自己免受AI生成內容的侵害，這將是一個數十億美元的市場！我們不僅能保護現有的網路生態，還將引領下一代AI內容安全技術的發展。現在加入我們，共同打造一個更安全、更真實的網路世界！", "audio": "docs/data/audios/2506.09975v1.wav"}
{"query": "Foundation Model", "id": "2506.09855v1", "url": "http://arxiv.org/abs/2506.09855v1", "title": "Foundation Model-Aided Deep Reinforcement Learning for RIS-Assisted Wireless Communication", "summary": "Reconfigurable intelligent surfaces (RIS) have emerged as a promising\ntechnology for enhancing wireless communication by dynamically controlling\nsignal propagation in the environment. However, their efficient deployment\nrelies on accurate channel state information (CSI), which leads to high channel\nestimation overhead due to their passive nature and the large number of\nreflective elements. In this work, we solve this challenge by proposing a novel\nframework that leverages a pre-trained open-source foundation model (FM) named\nlarge wireless model (LWM) to process wireless channels and generate versatile\nand contextualized channel embeddings. These embeddings are then used for the\njoint optimization of the BS beamforming and RIS configurations. To be more\nspecific, for joint optimization, we design a deep reinforcement learning (DRL)\nmodel to automatically select the BS beamforming vector and RIS phase-shift\nmatrix, aiming to maximize the spectral efficiency (SE). This work shows that a\npre-trained FM for radio signal understanding can be fine-tuned and integrated\nwith DRL for effective decision-making in wireless networks. It highlights the\npotential of modality-specific FMs in real-world network optimization.\nAccording to the simulation results, the proposed method outperforms the\nDRL-based approach and beam sweeping-based approach, achieving 9.89% and 43.66%\nhigher SE, respectively.", "authors": ["Mohammad Ghassemi", "Sara Farrag Mobarak", "Han Zhang", "Ali Afana", "Akram Bin Sediq", "Melike Erol-Kantarci"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:30:55.806908", "title_zh": "基於基礎模型的深度強化學習於RIS輔助無線通訊之應用", "summary_zh": "本研究提出一種創新的框架，利用預訓練的開源基礎模型（FM），即大型無線模型（LWM），來處理無線通道並生成多功能且情境化的通道嵌入。這些嵌入被用於聯合優化基地台波束成形和RIS配置。具體而言，我們設計了一個深度強化學習（DRL）模型，自動選擇基地台波束成形向量和RIS相位偏移矩陣，旨在最大化頻譜效率（SE）。模擬結果表明，該方法優於傳統的DRL和波束掃描方法，頻譜效率分別提高了9.89%和43.66%。此研究展示了針對無線電信號理解的預訓練FM可被微調並與DRL集成，從而在無線網路中實現有效的決策。", "applications": ["想像一下，在人潮擁擠的演唱會現場，手機訊號總是斷斷續續。有了這項技術，我們可以透過智慧控制牆面或燈具上的RIS，動態調整訊號傳輸路徑，確保每個人都能順暢地直播或分享精彩瞬間。", "在智慧工廠中，大量的感測器和機器人需要穩定的無線通訊。這項技術可以幫助工廠優化無線網路，減少訊號干擾，提高生產效率和安全性，實現真正的智慧製造。", "未來的無人機送貨，需要精準的定位和可靠的通訊。透過RIS輔助，我們可以克服建築物阻擋等問題，確保無人機能夠安全、準確地將包裹送到目的地。"], "pitch": "各位創投先進，我們正在打造無線通訊的未來！想像一下，5G/6G時代，網路流量爆炸性增長，現有的基地台建設速度遠遠跟不上需求。我們的技術，利用可重構智慧表面（RIS）和AI的力量，能大幅提升現有網路的容量和覆蓋範圍，無需大量新建基地台，成本效益極高！\n\n我們的創新之處在於，我們將大型無線模型（LWM）與深度強化學習（DRL）結合，讓網路能夠智慧地適應環境變化，動態優化訊號傳輸。這就像為無線網路裝上了一顆聰明的大腦！\n\n這項技術的潛在市場巨大。從智慧城市、工業物聯網到自動駕駛，各行各業都對更快速、更可靠的無線通訊有著迫切的需求。我們預計，未來五年內，RIS市場將呈現指數級增長，而我們將成為這個市場的領導者！\n\n我們不僅僅是在優化現有網路，更是在為未來的無線世界奠定基礎。現在加入我們，一起開創無線通訊的新紀元！", "audio": "docs/data/audios/2506.09855v1.wav"}
{"query": "Diffusion Model", "id": "2506.09740v1", "url": "http://arxiv.org/abs/2506.09740v1", "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:32:16.709693", "title_zh": "ELBO-T2IAlign：一種通用的基於ELBO的方法，用於校準擴散模型中像素級文字-圖像對齊", "summary_zh": "擴散模型在圖像生成方面表現出色，近年研究更顯示它們能透過注意力機制或損失函數編碼文字-圖像對齊資訊，這對圖像分割、文字引導的圖像編輯和組合圖像生成等下游任務至關重要。然而，現有方法過度依賴擴散模型中完美的文字-圖像對齊假設，但實際並非如此。本研究利用零樣本指代圖像分割作為代理任務，評估主流擴散模型的像素級圖像和類別級文字對齊。我們發現，小型、遮擋或罕見物體類別的圖像中容易出現錯位。因此，我們提出ELBO-T2IAlign，一種簡單有效的基於證據下界(ELBO)的方法來校準對齊，無需訓練且通用，適用於各種擴散模型架構。實驗證明了該方法的有效性。", "applications": ["想像一下，你可以對著手機說：『把照片裡那隻小狗的帽子換成聖誕帽』，AI就能精準地把小狗頭上的帽子替換掉，這就是精準文字圖像對齊的應用。", "如果你想設計一個獨一無二的T恤，只要輸入『一隻戴墨鏡的貓咪，背景是熱帶海灘』，AI就能自動生成高質量的圖像，讓你輕鬆印在T恤上，而且貓咪和海灘的組合會非常自然。", "未來的線上購物，不再需要攝影棚拍攝商品圖。只要輸入商品的文字描述，AI就能生成各種角度、各種場景下的商品圖片，大幅降低商家的運營成本。"], "pitch": "各位投資人，我們帶來的是ELBO-T2IAlign技術，它解決了擴散模型中文字與圖像對齊的核心問題。這項技術就像是為AI圖像生成引擎裝上了一顆更精準的定位系統，讓AI更能理解人類的意圖。試想一下，未來廣告設計、遊戲開發、電商行銷，都將因為這項技術而產生革命性的變化！更精準的圖像生成，意味著更高的用戶參與度、更低的製作成本、以及更無限的創意可能性。我們預期，這項技術將成為元宇宙內容生成的基石，並在未來五年內創造數十億美元的市場價值。現在加入我們，一起打造AI圖像生成的未來！", "audio": "docs/data/audios/2506.09740v1.wav"}
{"query": "AI", "id": "2506.09968v1", "url": "http://arxiv.org/abs/2506.09968v1", "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance", "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.", "authors": ["Wentao Ge", "Yuqing Sun", "Ziyan Wang", "Haoyue Zheng", "Weiyang He", "Piaohong Wang", "Qianyu Zhu", "Benyou Wang"], "published_date": "2025-06-11", "timestamp": "2025-06-12T18:36:01.129659", "title_zh": "SRLAgent：透過遊戲化與大型語言模型輔助強化自我調節學習技能", "summary_zh": "SRLAgent是一個利用遊戲化和大型語言模型(LLM)輔助的系統，旨在提升大學生的自我調節學習(SRL)能力。研究發現，許多大學生在目標設定、時間管理和反思學習方面遇到困難。SRLAgent基於Zimmerman的三階段SRL框架，提供一個互動的遊戲環境，讓學生進行目標設定、策略執行和自我反思。系統利用LLM提供即時回饋和鷹架，支援學生的獨立學習。實驗結果顯示，使用SRLAgent的學生在SRL技能上有顯著提升，且參與度更高。這項研究強調了在遊戲化環境中嵌入SRL鷹架和即時AI支援的價值，為教育科技的設計提供了新的方向，旨在促進更深入的學習和後設認知技能的發展。", "applications": ["想像一下，SRLAgent就像一位24小時的AI家教，隨時在你學習遇到困難時給予指導和鼓勵。無論是準備考試、撰寫報告，還是學習新技能，它都能幫助你更有效地設定目標、安排時間，並反思自己的學習過程，讓學習變得更輕鬆、更有趣。", "孩子們常常對學習感到厭倦，覺得很枯燥。SRLAgent可以將學習內容轉化為有趣的遊戲，讓孩子在玩樂中學習，激發他們的學習興趣和動力。家長也可以透過SRLAgent了解孩子的學習進度和困難，及時給予幫助。", "對於需要不斷學習新知識和技能的職場人士來說，SRLAgent是一個高效的學習夥伴。它可以幫助你快速掌握新知識、提升工作效率，並在工作中不斷成長。例如，學習新的程式語言、市場行銷策略，或是專案管理技巧。"], "pitch": "各位投資人，我們正處於AI賦能教育的黃金時代！SRLAgent不僅僅是一個學習工具，它是一個革命性的教育平台，利用遊戲化和LLM技術，個性化地提升學習者的自我調節能力。想像一下，全球數百萬大學生、職場人士，甚至中小學生，都在使用我們的平台，更高效、更自主地學習。這是一個龐大的市場，而且還在快速增長。SRLAgent的數據分析能力還能為教育機構提供寶貴的洞察，幫助他們優化課程設計和教學方法。我們預計，SRLAgent將成為未來教育領域的領頭羊，為投資者帶來豐厚的回報。現在加入我們，一起塑造教育的未來！", "audio": "docs/data/audios/2506.09968v1.wav"}
{"query": "AI", "id": "2506.09958v1", "url": "http://arxiv.org/abs/2506.09958v1", "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:25:14.263997", "title_zh": "Kvasir-VQA-x1：用於醫療推理和在腸胃內視鏡檢查中實現穩健MedVQA的多模態數據集", "summary_zh": "Kvasir-VQA-x1是一個大型腸胃內視鏡影像問答數據集，旨在提升醫療決策支援系統的效能。它擴展了原有的Kvasir-VQA數據集，新增了超過15萬個問題答案對，專門測試模型的臨床推理能力。這些問題由大型語言模型生成，並根據複雜度分層，同時加入模擬常見影像偽影的視覺增強，以確保模型能適應真實臨床場景。此數據集提供標準VQA效能和模型對視覺干擾的穩健性兩種評估方式，有助於開發更可靠有效的多模態AI系統，並已完全公開，遵循FAIR數據原則。", "applications": ["想像一下，醫生在做胃鏡檢查時，AI能即時回答醫生提出的問題，例如「這個病灶看起來像什麼？」、「這個區域是否需要切片檢查？」，幫助醫生做出更準確的判斷。", "如果我們開發一個App，讓民眾可以上傳自己的腸胃鏡照片，AI就能初步分析照片，提供可能的健康風險評估，讓民眾更了解自己的腸胃健康狀況。", "未來，AI可以整合病人的病歷資料和腸胃鏡影像，預測病人未來罹患腸胃疾病的風險，提醒病人及早進行預防和治療。"], "pitch": "各位投資人，我們正在打造腸胃內視鏡AI輔助診斷的未來！Kvasir-VQA-x1數據集是我們成功的基石，它能訓練出更聰明、更可靠的AI模型，協助醫生精準診斷腸胃疾病。想像一下，AI不僅能減少誤診率，還能大幅提升診斷效率，降低醫療成本！隨著人口老化和生活習慣改變，腸胃疾病的發病率不斷攀升，市場潛力巨大。我們的技術不僅能應用於醫院，更能推廣到基層診所，甚至進入家庭，成為每個人的腸胃健康守護者。我們有信心，這項技術將顛覆傳統醫療模式，為投資者帶來豐厚的回報！現在加入我們，一起開創AI醫療的新紀元！", "audio": "docs/data/audios/2506.09958v1.wav"}
{"query": "Foundation Model", "id": "2506.09755v1", "url": "http://arxiv.org/abs/2506.09755v1", "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era", "summary": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges.", "authors": ["Shuo Jiang", "Min Xie", "Frank Youhua Chen", "Jian Ma", "Jianxi Luo"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:26:48.175719", "title_zh": "智慧設計4.0：邁向具主動性人工智慧時代的典範演進", "summary_zh": "智慧設計（ID）在提升工程創新、效率、品質和生產力方面貢獻卓著。隨著大型語言模型（LLM）等基礎模型的興起，具備了通用知識推理能力，為工程設計帶來了新的變革。本文介紹了智慧設計4.0（ID 4.0），它是由具主動性的人工智慧系統所驅動的新典範。我們回顧了ID的四個發展階段，並提出了ID 4.0的概念框架，討論了其通過協調、自主的多代理系統實現工程設計流程端到端自動化的潛力。未來，ID 4.0將在更複雜的設計場景、更實際的設計實現、新型代理協調機制以及自主設計目標設定等方面，展現更大的適應性、自主性和有效性。", "applications": ["想像一下，未來設計師不再需要從頭開始繪製藍圖。透過ID 4.0，只要輸入需求，AI就能自動生成多個設計方案，甚至考慮到材料成本和環境影響，大幅縮短設計時間，讓建築師有更多時間與客戶溝通，打造更符合需求的夢想家園。", "如果家裡的電器壞了，不再需要等待維修人員上門。ID 4.0可以根據電器的型號和故障描述，自動生成維修步驟和所需零件，甚至可以透過AR技術，指導使用者自行維修，省時又省錢。", "在醫療領域，ID 4.0可以幫助醫生設計個性化的治療方案。例如，針對癌症患者，AI可以分析患者的基因數據和病理報告，設計出最有效的化療方案，並預測治療效果，提高治療成功率。"], "pitch": "各位投資人，我們正站在一個劃時代的轉捩點上！智慧設計4.0（ID 4.0）不僅僅是一個技術概念，它是一場工程設計領域的革命！想像一下，一個由AI驅動的設計團隊，24小時不間斷地工作，以驚人的速度和效率完成設計任務。這不僅能大幅降低成本，更能加速產品上市時間，搶佔市場先機。更重要的是，ID 4.0有能力解決人類無法解決的複雜設計問題，例如：設計出更節能環保的建築、更安全可靠的交通工具、甚至開發出全新的醫療設備。我們相信，ID 4.0將顛覆傳統產業，催生出無數的創新應用，並為投資者帶來豐厚的回報！現在投資ID 4.0，就是投資未來！讓我們一起攜手，打造一個更智慧、更美好的世界！", "audio": "docs/data/audios/2506.09755v1.wav"}
{"query": "Diffusion Model", "id": "2506.09665v1", "url": "http://arxiv.org/abs/2506.09665v1", "title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "summary": "We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.", "authors": ["Jacob Munkberg", "Zian Wang", "Ruofan Liang", "Tianchang Shen", "Jon Hasselgren"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:27:56.854870", "title_zh": "VideoMat：從影片擴散模型中提取PBR材質", "summary_zh": "VideoMat運用微調後的影片擴散模型，結合影片的內在分解和基於物理的可微渲染，從文字提示或單張圖像生成高品質的3D模型材質。首先，我們調整影片擴散模型，使其符合輸入的幾何形狀和光照條件，產生具有一致材質屬性的3D模型多視圖。接著，利用最新模型從生成的影片中提取內在屬性（基礎顏色、粗糙度、金屬度）。最後，我們將這些內在屬性與生成的影片一起輸入可微路徑追蹤器，以穩健地提取與常見內容創作工具直接相容的PBR材質。", "applications": ["遊戲開發者可以快速生成逼真的遊戲場景材質，省去繁瑣的手工調整。", "室內設計師可以將客戶提供的照片或影片轉換為3D模型材質，方便進行虛擬展示和修改。", "電商平台可以利用這項技術自動生成商品3D模型的材質，提升商品展示效果和消費者購買意願。"], "pitch": "各位投資人，想像一下，未來創建3D內容將不再需要耗時費力的手動建模和材質調整。VideoMat技術的出現，將徹底顛覆這個產業。我們利用AI的力量，能從簡單的影片或圖片中自動提取高品質的PBR材質，這意味著內容創作者可以大幅降低成本、加快開發速度，並創造出更逼真、更吸引人的3D內容。無論是遊戲、電影、電商還是元宇宙，對於高品質3D素材的需求都將是巨大的。VideoMat不僅僅是一個技術，更是一個通往無限可能的入口。我們預計，隨著元宇宙的發展和3D內容的普及，VideoMat將成為3D內容創作領域的關鍵基礎設施，市場潛力不可估量。現在投資VideoMat，就是投資3D內容的未來！", "audio": "docs/data/audios/2506.09665v1.wav"}
{"query": "AI", "id": "2506.09947v1", "url": "http://arxiv.org/abs/2506.09947v1", "title": "KI4Demokratie: An AI-Based Platform for Monitoring and Fostering Democratic Discourse", "summary": "Social media increasingly fuel extremism, especially right-wing extremism,\nand enable the rapid spread of antidemocratic narratives. Although AI and data\nscience are often leveraged to manipulate political opinion, there is a\ncritical need for tools that support effective monitoring without infringing on\nfreedom of expression. We present KI4Demokratie, an AI-based platform that\nassists journalists, researchers, and policymakers in monitoring right-wing\ndiscourse that may undermine democratic values. KI4Demokratie applies machine\nlearning models to a large-scale German online data gathered on a daily basis,\nproviding a comprehensive view of trends in the German digital sphere. Early\nanalysis reveals both the complexity of tracking organized extremist behavior\nand the promise of our integrated approach, especially during key events.", "authors": ["Rudy Alexandro Garrido Veliz", "Till Nikolaus Schaland", "Simon Bergmoser", "Florian Horwege", "Somya Bansal", "Ritesh Nahar", "Martin Semmann", "Jörg Forthmann", "Seid Muhie Yimam"], "published_date": "2025-06-11", "timestamp": "2025-06-13T02:03:16.537260", "title_zh": "KI4Demokratie：一個基於人工智慧的平台，用於監測和促進民主 discourse", "summary_zh": "KI4Demokratie是一個基於人工智慧的平台，旨在協助記者、研究人員和政策制定者監測可能破壞民主價值的右翼言論。該平台利用機器學習模型分析大規模的德國線上數據，每日更新，提供德國數位領域趨勢的全面視圖。初步分析顯示，追蹤有組織的極端主義行為具有複雜性，但也證明了我們整合方法的潛力，尤其是在關鍵事件期間。", "applications": ["新聞媒體可以利用這個平台，即時追蹤網路上的仇恨言論和假新聞，更快更準確地報導相關事件，提升新聞品質和公信力。", "政府部門可以運用這個平台，監控網路輿情，及早發現並處理可能危害社會穩定的言論，維護公共安全。", "教育機構可以利用這個平台，分析網路上的不實資訊和極端思想，設計更有針對性的課程，提升學生的媒體素養和批判性思考能力。"], "pitch": "各位投資人，今天向各位介紹的是KI4Demokratie，一個劃時代的AI平台，它不只是個工具，更是民主的守護者。在假新聞和極端言論氾濫的時代，KI4Demokratie能精準監測並分析網路輿情，協助媒體、政府和教育機構即時應對。想像一下，我們能提前預警社會動盪，阻止仇恨言論擴散，甚至能辨識並打擊境外勢力的認知作戰。這不僅關乎社會責任，更蘊藏巨大的商業價值。未來，我們將進一步開發AI模型，預測輿論走向，提供更精準的決策支持。KI4Demokratie的潛力無限，它將成為維護民主價值、引領輿論走向的關鍵力量。現在投資KI4Demokratie，您投資的不僅僅是一個平台，更是投資一個更安全、更健康的未來！", "audio": "docs/data/audios/2506.09947v1.wav"}
{"query": "Foundation Model", "id": "2506.09748v1", "url": "http://arxiv.org/abs/2506.09748v1", "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "published_date": "2025-06-11", "timestamp": "2025-06-13T02:04:34.519078", "title_zh": "基於語義和結構約束的無人機絕對視覺定位分層圖像匹配", "summary_zh": "本研究提出一種用於無人機絕對視覺定位的分層跨源圖像匹配方法，旨在解決在全球導航衛星系統（GNSS）信號不可用時的定位挑戰。該方法整合了語義感知和結構約束的粗略匹配模塊，以及輕量級的精細匹配模塊。粗略匹配模塊利用視覺基礎模型提取的語義特徵，在語義和結構約束下建立區域級的對應關係。隨後，精細匹配模塊提取精細特徵並建立像素級的對應關係。該方法無需依賴相對定位技術，通過圖像檢索模塊和分層圖像匹配模塊構建無人機絕對視覺定位流程。實驗結果表明，該方法在各種挑戰性條件下具有卓越的準確性和魯棒性。", "applications": ["無人機送貨：在沒有GPS訊號的環境下，例如城市高樓之間或室內，無人機能精準地將包裹送到目的地。", "災難救援：在地震或洪水等災難發生後，GPS訊號可能中斷，無人機可利用此技術快速定位受困人員，協助救援隊伍。", "自動駕駛：在隧道、地下停車場等GPS訊號弱的區域，自動駕駛汽車可以利用此技術進行精確導航。"], "pitch": "各位投資人，想像一下，一個完全自主、不受GPS限制的無人機世界！我們的分層圖像匹配技術，讓無人機在任何環境下都能精準定位，開啟無限可能。從智慧農業的精準噴灑，到智慧城市的無人巡檢，再到軍事領域的自主偵察，應用場景廣闊。更重要的是，隨著低軌衛星網路的普及，我們的技術將與之結合，打造全球無縫覆蓋的無人機定位系統，徹底顛覆物流、安防、以及各行各業的運作模式。我們不只在解決當前的痛點，更在佈局未來的藍圖！現在投資，您將成為這場無人機革命的領航者！", "audio": "docs/data/audios/2506.09748v1.wav"}
{"query": "Diffusion Model", "id": "2506.09644v1", "url": "http://arxiv.org/abs/2506.09644v1", "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "published_date": "2025-06-11", "timestamp": "2025-06-13T02:05:46.595423", "title_zh": "DGAE：擴散引導的自編碼器，用於高效的潛在表徵學習", "summary_zh": "本研究提出一種名為DGAE的新型自編碼器，旨在提升圖像和影片生成模型的效率。DGAE利用擴散模型引導解碼器，從壓縮後的潛在空間中恢復更多資訊，有效解決了高壓縮率下自編碼器性能下降的問題，同時將潛在空間的維度縮小了一倍。實驗結果顯示，DGAE在ImageNet-1K圖像生成任務上表現出色，並能加速擴散模型的收斂。簡而言之，DGAE在保證圖像品質的前提下，實現了更高效、更精簡的圖像和影片表示。", "applications": ["假設你手機容量不夠，DGAE技術可以幫你把照片、影片壓縮得更小，畫質卻幾乎不變，讓你省下更多空間。", "現在很多AI繪圖工具，如果用上DGAE，就能用更少的運算資源，畫出更高品質的圖片，速度也會更快。", "未來的虛擬實境（VR）或擴增實境（AR）應用，需要傳輸大量的影像資料，DGAE可以大幅降低傳輸的數據量，讓畫面更流暢，不再卡頓。"], "pitch": "各位投資人，我們正站在AI影像革命的浪潮之上！DGAE技術，是這場革命的關鍵引擎。它不僅僅是壓縮技術，更是提升AI生成模型效率的秘密武器。想像一下，未來AI生成的內容將無所不在，從遊戲、電影到廣告、教育，都需要高效、高品質的影像生成。DGAE能讓這些應用更快、更好、更便宜地實現。更重要的是，DGAE大幅降低了AI模型對算力的需求，讓AI技術不再是少數巨頭的專利，而是人人都能觸及的工具。我們預計，DGAE將成為未來AI影像處理的基礎設施，市場潛力無限。現在投資DGAE，就是投資AI的未來！", "audio": "docs/data/audios/2506.09644v1.wav"}
{"query": "AI", "id": "2506.10975v1", "url": "http://arxiv.org/abs/2506.10975v1", "title": "GenWorld: Towards Detecting AI-generated Real-world Simulation Videos", "summary": "The flourishing of video generation technologies has endangered the\ncredibility of real-world information and intensified the demand for\nAI-generated video detectors. Despite some progress, the lack of high-quality\nreal-world datasets hinders the development of trustworthy detectors. In this\npaper, we propose GenWorld, a large-scale, high-quality, and real-world\nsimulation dataset for AI-generated video detection. GenWorld features the\nfollowing characteristics: (1) Real-world Simulation: GenWorld focuses on\nvideos that replicate real-world scenarios, which have a significant impact due\nto their realism and potential influence; (2) High Quality: GenWorld employs\nmultiple state-of-the-art video generation models to provide realistic and\nhigh-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes\nvideos generated from diverse generators and various prompt modalities (e.g.,\ntext, image, video), offering the potential to learn more generalizable\nforensic features. We analyze existing methods and find they fail to detect\nhigh-quality videos generated by world models (i.e., Cosmos), revealing\npotential drawbacks of ignoring real-world clues. To address this, we propose a\nsimple yet effective model, SpannDetector, to leverage multi-view consistency\nas a strong criterion for real-world AI-generated video detection. Experiments\nshow that our method achieves superior results, highlighting a promising\ndirection for explainable AI-generated video detection based on physical\nplausibility. We believe that GenWorld will advance the field of AI-generated\nvideo detection. Project Page: https://chen-wl20.github.io/GenWorld", "authors": ["Weiliang Chen", "Wenzhao Zheng", "Yu Zheng", "Lei Chen", "Jie Zhou", "Jiwen Lu", "Yueqi Duan"], "published_date": "2025-06-12", "timestamp": "2025-06-13T03:52:05.588354", "title_zh": "GenWorld：邁向偵測AI生成的真實世界模擬影片", "summary_zh": "隨著影片生成技術蓬勃發展，AI生成影片的偵測需求日益增加。現有偵測器缺乏高品質的真實世界數據集，導致發展受限。本研究提出GenWorld，一個大規模、高品質的真實世界模擬數據集，專為AI生成影片偵測而設計。GenWorld模擬真實場景，採用多種先進影片生成模型，提供逼真的偽造影片，並包含來自不同生成器和提示模態的影片，以學習更具泛化性的鑑識特徵。我們提出SpannDetector模型，利用多視角一致性作為真實世界AI生成影片偵測的強大標準。實驗證明，本方法能有效偵測，為基於物理合理性的可解釋AI生成影片偵測，提供了一個有前景的方向。", "applications": ["新聞查核：當我們在社群媒體上看到一段關於災難或政治事件的影片時，可以使用GenWorld技術來驗證影片的真偽，避免被假新聞誤導。", "保險理賠：保險公司可以利用GenWorld技術來判斷提交的事故影片是否經過AI偽造，防止詐欺行為，例如虛報車禍或意外傷害。", "教育訓練：學校或企業可以使用GenWorld技術設計AI生成影片偵測課程，讓學生或員工學習如何辨識和應對AI偽造的影片，提升媒體素養。"], "pitch": "各位投資人，想像一下，在AI生成內容氾濫的未來，我們如何確保眼見為真？GenWorld不僅是一個數據集，更是一把對抗假訊息的利劍。我們開發的SpannDetector模型，能有效辨識AI生成的模擬影片，其應用範圍廣泛，從新聞媒體的真相驗證，到金融機構的詐欺防範，甚至是國安層面的資訊戰防禦，都有著巨大的潛力。隨著deepfake技術日益精進，市場對AI生成影片偵測的需求將會爆炸性成長。GenWorld團隊已掌握領先優勢，我們有信心將這項技術推向全球，成為AI時代的守門人，並為投資者帶來豐厚的回報。現在投資GenWorld，就是投資一個更真實、更安全的未來！", "audio": "docs/data/audios/2506.10975v1.wav"}
{"query": "Foundation Model", "id": "2506.10966v1", "url": "http://arxiv.org/abs/2506.10966v1", "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation", "summary": "Robotic manipulation in real-world settings remains challenging, especially\nregarding robust generalization. Existing simulation platforms lack sufficient\nsupport for exploring how policies adapt to varied instructions and scenarios.\nThus, they lag behind the growing interest in instruction-following foundation\nmodels like LLMs, whose adaptability is crucial yet remains underexplored in\nfair comparisons. To bridge this gap, we introduce GenManip, a realistic\ntabletop simulation platform tailored for policy generalization studies. It\nfeatures an automatic pipeline via LLM-driven task-oriented scene graph to\nsynthesize large-scale, diverse tasks using 10K annotated 3D object assets. To\nsystematically assess generalization, we present GenManip-Bench, a benchmark of\n200 scenarios refined via human-in-the-loop corrections. We evaluate two policy\ntypes: (1) modular manipulation systems integrating foundation models for\nperception, reasoning, and planning, and (2) end-to-end policies trained\nthrough scalable data collection. Results show that while data scaling benefits\nend-to-end methods, modular systems enhanced with foundation models generalize\nmore effectively across diverse scenarios. We anticipate this platform to\nfacilitate critical insights for advancing policy generalization in realistic\nconditions. Project Page: https://genmanip.axi404.top/.", "authors": ["Ning Gao", "Yilun Chen", "Shuai Yang", "Xinyi Chen", "Yang Tian", "Hao Li", "Haifeng Huang", "Hanqing Wang", "Tai Wang", "Jiangmiao Pang"], "published_date": "2025-06-12", "timestamp": "2025-06-13T03:53:19.853024", "title_zh": "GENMANIP：基於LLM驅動的模擬，實現通用指令遵循操作", "summary_zh": "現實環境中的機器人操作仍具挑戰性，尤其在泛化能力方面。現有模擬平台對探索策略如何適應不同指令和場景的支持不足，落後於指令遵循基礎模型（如LLM）的發展。為此，我們推出GenManip，一個真實的桌面模擬平台，專為策略泛化研究而設計。它通過LLM驅動的任務導向場景圖自動生成管道，利用10K個帶註釋的3D對象資源合成大規模、多樣化的任務。我們還提出了GenManip-Bench，包含200個場景的基準測試，並通過人機協作修正進行完善。評估結果表明，雖然數據擴展有助於端到端方法，但利用基礎模型增強的模塊化系統在多樣化場景中泛化效果更好。我們期望這個平台能促進對在真實條件下推進策略泛化的重要見解。", "applications": ["想像一下，你可以用口語指令，像是「把紅色的蘋果放到藍色的碗裡」，就能直接指揮家裡的機器人幫你整理東西，再也不用自己彎腰撿東西了！", "醫院裡，醫生可以透過語音指令，讓機器人自動準備手術需要的工具，大幅減少手術時間和人為錯誤的風險，提升醫療效率。", "在工廠裡，即使生產線臨時需要調整，工程師也能用簡單的指令，快速重新配置機器人的工作流程，應對客製化的訂單需求，提高生產彈性。"], "pitch": "各位投資人，我們正處於AI賦能機器人的黃金時代！GenManip平台不僅僅是一個模擬器，它是機器人智能的加速器。想像一下，未來的工廠、醫院、甚至家庭，都將充滿著能聽懂人話、靈活工作的機器人。GenManip通過LLM賦能，讓機器人能像人類一樣理解指令、適應變化，這將徹底顛覆傳統的自動化模式。我們的GenManip-Bench基準測試，將成為業界評估機器人智能的黃金標準。更重要的是，我們正在建立一個龐大的機器人技能庫，未來可以通過訂閱服務、license授權等方式，將這些技能賦予各行各業的機器人，創造巨大的商業價值。現在投資GenManip，就是投資機器人智能的未來，讓我們一起打造一個更智能、更便捷的世界！預計五年內，GenManip將成為機器人開發領域的領導者，市場估值有望突破十億美元！", "audio": "docs/data/audios/2506.10966v1.wav"}
{"query": "Diffusion Model", "id": "2506.10981v1", "url": "http://arxiv.org/abs/2506.10981v1", "title": "SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis", "summary": "Generative models have gained significant attention in novel view synthesis\n(NVS) by alleviating the reliance on dense multi-view captures. However,\nexisting methods typically fall into a conventional paradigm, where generative\nmodels first complete missing areas in 2D, followed by 3D recovery techniques\nto reconstruct the scene, which often results in overly smooth surfaces and\ndistorted geometry, as generative models struggle to infer 3D structure solely\nfrom RGB data. In this paper, we propose SceneCompleter, a novel framework that\nachieves 3D-consistent generative novel view synthesis through dense 3D scene\ncompletion. SceneCompleter achieves both visual coherence and 3D-consistent\ngenerative scene completion through two key components: (1) a\ngeometry-appearance dual-stream diffusion model that jointly synthesizes novel\nviews in RGBD space; (2) a scene embedder that encodes a more holistic scene\nunderstanding from the reference image. By effectively fusing structural and\ntextural information, our method demonstrates superior coherence and\nplausibility in generative novel view synthesis across diverse datasets.\nProject Page: https://chen-wl20.github.io/SceneCompleter", "authors": ["Weiliang Chen", "Jiayi Bi", "Yuanhui Huang", "Wenzhao Zheng", "Yueqi Duan"], "published_date": "2025-06-12", "timestamp": "2025-06-13T03:54:25.001679", "title_zh": "SceneCompleter：用於生成式新視圖合成的密集3D場景補全", "summary_zh": "SceneCompleter是一個創新的框架，透過密集3D場景補全實現3D一致的生成式新視圖合成。它採用幾何-外觀雙流擴散模型，在RGBD空間中聯合合成新視圖，並利用場景嵌入器從參考圖像中編碼更全面的場景理解。透過有效地融合結構和紋理信息，SceneCompleter在生成式新視圖合成中展現了卓越的連貫性和合理性。這項技術克服了傳統方法在僅從RGB數據推斷3D結構時遇到的困難，產生更真實、更精確的3D場景重建。", "applications": ["線上購物：想像一下，你可以在購買家具前，用手機掃描客廳，就能立即看到家具擺放在你家中的真實樣貌，甚至可以隨意調整顏色和大小，預覽效果。", "虛擬旅遊：不用出國，也能身歷其境！透過手機或VR裝置，你可以探索世界各地的名勝古蹟，甚至可以自由切換視角，彷彿親臨現場。", "室內設計：設計師可以快速生成不同風格的室內設計方案，讓客戶更直觀地看到設計效果，減少溝通成本，提高設計效率。"], "pitch": "各位投資人，我們相信SceneCompleter將徹底改變3D內容生成的方式。現有的技術往往需要大量的數據和複雜的建模過程，而SceneCompleter只需少量參考圖像，就能生成高品質、3D一致的新視圖。這項技術的應用前景廣闊，從電商、遊戲、影視到建築、設計，都將因此受益。想像一下，未來的遊戲開發者可以更快速地創建逼真的遊戲場景；電影製作人可以更輕鬆地進行特效製作；房地產公司可以為潛在買家提供更沉浸式的看房體驗。我們預計，SceneCompleter將成為元宇宙時代的關鍵技術之一，並在未來幾年內帶來數十億美元的市場機會。現在投資我們，您將成為這場變革的領先者！", "audio": "docs/data/audios/2506.10981v1.wav"}
{"query": "AI", "id": "2506.10953v1", "url": "http://arxiv.org/abs/2506.10953v1", "title": "Build the web for agents, not agents for the web", "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.", "authors": ["Xing Han Lù", "Gaurav Kamath", "Marius Mosbach", "Siva Reddy"], "published_date": "2025-06-12", "timestamp": "2025-06-13T06:37:59.401318", "title_zh": "為代理人建構網路，而非為網路建構代理人", "summary_zh": "近年來，大型語言模型（LLM）和多模態模型的進步激發了開發網路代理人的濃厚興趣，這些AI系統能夠自主導航並在網路環境中完成任務。儘管它們在自動化複雜網路互動方面具有巨大潛力，但由於人類設計的介面與LLM能力之間存在根本不匹配，當前的方法面臨著重大挑戰。本論文倡導網路代理人研究中的範式轉變：我們應該開發一種專門為代理人能力優化的新型互動範式，而不是強迫網路代理人適應為人類設計的介面。為此，我們引入了代理人網路介面（AWI）的概念，這是一種專門為代理人導航網站而設計的介面。我們建立了六項AWI設計指導原則，強調安全性、效率和標準化，以考慮所有主要利益相關者的利益。這種重新定義旨在克服現有介面的根本限制，為更有效、可靠和透明的網路代理人設計鋪平道路，這將是更廣泛的機器學習社群的共同努力。", "applications": ["想像一下，你可以對著手機說：「幫我訂一張下星期五晚上七點在信義威秀影城的電影票，要《奧本海默》。」AI代理人就會自動瀏覽電影網站，找到合適的場次並完成訂票，完全不需要你手動操作。", "假設你需要比較不同銀行的貸款利率。你只需告訴AI代理人你的需求，它就會自動訪問各家銀行的網站，收集相關資訊，並整理成一個易於閱讀的表格，讓你輕鬆做出選擇。", "如果家裡長輩不擅長使用網路購物，可以透過AI代理人協助他們。他們只要口頭描述想要購買的商品，AI代理人就會自動搜尋商品、比較價格，並完成下單，省去複雜的操作步驟。"], "pitch": "各位投資人，我們正在打造一個革命性的網路互動方式，讓AI代理人成為您的數位助手。想像一下，未來的網路不再需要繁瑣的操作，而是由AI代理人根據您的指令自動完成。我們的Agentic Web Interface（AWI）將徹底改變電子商務、金融服務、客戶服務等領域。透過AWI，企業可以大幅降低人力成本，提高效率，並提供更個性化的服務。我們預計，AWI將成為下一代網路的基礎設施，創造一個數千億美元的市場。現在加入我們，一起開創AI驅動的網路新時代！未來，我們可以將AWI技術授權給各大企業，收取授權費用，或者開發基於AWI的SaaS產品，例如智能客服機器人、自動化行銷工具等。甚至可以將AWI整合到元宇宙中，讓用戶在虛擬世界中也能享受到便捷的AI服務。", "audio": "docs/data/audios/2506.10953v1.wav"}
{"query": "Foundation Model", "id": "2506.10956v1", "url": "http://arxiv.org/abs/2506.10956v1", "title": "Distillation of atomistic foundation models across architectures and chemical domains", "summary": "Machine-learned interatomic potentials have transformed computational\nresearch in the physical sciences. Recent atomistic `foundation' models have\nchanged the field yet again: trained on many different chemical elements and\ndomains, these potentials are widely applicable, but comparably slow and\nresource-intensive to run. Here we show how distillation via synthetic data can\nbe used to cheaply transfer knowledge from atomistic foundation models to a\nrange of different architectures, unlocking much smaller, more efficient\npotentials. We demonstrate speed-ups of $> 10\\times$ by distilling from one\ngraph-network architecture into another, and $> 100\\times$ by leveraging the\natomic cluster expansion framework. We showcase applicability across chemical\nand materials domains: from liquid water to hydrogen under extreme conditions;\nfrom porous silica and a hybrid halide perovskite solar-cell material to\nmodelling organic reactions. Our work shows how distillation can support the\nroutine and computationally efficient use of current and future atomistic\nfoundation models in real-world scientific research.", "authors": ["John L. A. Gardner", "Daniel F. Thomas du Toit", "Chiheb Ben Mahmoud", "Zoé Faure Beaulieu", "Veronika Juraskova", "Laura-Bianca Paşca", "Louise A. M. Rosset", "Fernanda Duarte", "Fausto Martelli", "Chris J. Pickard", "Volker L. Deringer"], "published_date": "2025-06-12", "timestamp": "2025-06-13T06:39:52.830955", "title_zh": "跨架構與化學領域的原子基礎模型知識蒸餾", "summary_zh": "本研究展示如何透過合成數據的知識蒸餾，將大型原子基礎模型的知識轉移到更小、更高效的模型上。這些基礎模型雖然應用廣泛，但運算速度較慢且耗費資源。透過知識蒸餾，我們成功將模型速度提升10倍以上，甚至超過100倍，同時保持其在不同化學和材料領域的準確性。這項技術可應用於液態水、極端條件下的氫、多孔二氧化矽、鈣鈦礦太陽能電池材料以及有機反應建模等領域。知識蒸餾有助於在實際科研中更有效率地使用現有和未來的原子基礎模型。", "applications": ["想像一下，藥廠可以利用這項技術，加速新藥的開發流程。透過更快速的分子動力學模擬，他們能更精準地預測藥物與蛋白質的交互作用，縮短研發時程，更快推出救命新藥。", "在材料科學領域，工程師可以利用這項技術，設計出更堅固、更輕盈的材料，應用於航空、汽車等產業。例如，開發更耐高溫的引擎材料，提升飛機的燃油效率，或打造更輕巧的電動車車身，增加續航里程。", "農民也可以受惠於此技術。透過模擬土壤中的化學反應，可以更精準地了解肥料的使用效率，減少不必要的浪費，並降低對環境的影響，實現更永續的農業發展。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它將徹底改變材料科學、化學工程和藥物開發等領域。原子基礎模型雖然強大，但運算成本高昂，阻礙了其廣泛應用。我們的知識蒸餾技術，就像是為這些模型裝上了渦輪增壓引擎，大幅提升其運算效率，同時保持其精準度。這意味著，我們可以將原本需要超級電腦才能完成的模擬，在一般的伺服器上運行，甚至在行動裝置上實現。想像一下，未來新藥開發的成本將大幅降低，新材料的設計速度將加快數倍，甚至可以根據個人基因，客製化出最適合的藥物和材料。這不僅僅是一項技術突破，更是一個龐大的市場機會。我們正在打造一個全新的模擬生態系統，一個讓科學家、工程師和研究人員都能輕鬆使用原子級別模擬的世界。現在加入我們，一起引領這場科學模擬的革命，共同創造一個更美好的未來！", "audio": "docs/data/audios/2506.10956v1.wav"}
{"query": "Diffusion Model", "id": "2506.10978v1", "url": "http://arxiv.org/abs/2506.10978v1", "title": "Fine-Grained Perturbation Guidance via Attention Head Selection", "summary": "Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer (DiT) architectures where\nquality-relevant computations are distributed across layers. In this paper, we\ninvestigate the granularity of attention perturbations, ranging from the layer\nlevel down to individual attention heads, and discover that specific heads\ngovern distinct visual concepts such as structure, style, and texture quality.\nBuilding on this insight, we propose \"HeadHunter\", a systematic framework for\niteratively selecting attention heads that align with user-centric objectives,\nenabling fine-grained control over generation quality and visual attributes. In\naddition, we introduce SoftPAG, which linearly interpolates each selected\nhead's attention map toward an identity matrix, providing a continuous knob to\ntune perturbation strength and suppress artifacts. Our approach not only\nmitigates the oversmoothing issues of existing layer-level perturbation but\nalso enables targeted manipulation of specific visual styles through\ncompositional head selection. We validate our method on modern large-scale\nDiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,\ndemonstrating superior performance in both general quality enhancement and\nstyle-specific guidance. Our work provides the first head-level analysis of\nattention perturbation in diffusion models, uncovering interpretable\nspecialization within attention layers and enabling practical design of\neffective perturbation strategies.", "authors": ["Donghoon Ahn", "Jiwon Kang", "Sanghyun Lee", "Minjae Kim", "Jaewon Min", "Wooseok Jang", "Saungwu Lee", "Sayak Paul", "Susung Hong", "Seungryong Kim"], "published_date": "2025-06-12", "timestamp": "2025-06-13T06:41:07.555945", "title_zh": "基於注意力頭選擇的細粒度擾動引導", "summary_zh": "本研究探索在擴散模型中，如何更精確地控制圖像生成。現有方法透過擾動模型來引導生成方向，尤其是在無法使用無分類器引導的場景中，注意力擾動表現出色。但過去方法缺乏精準的擾動位置選擇。本研究發現，不同注意力頭控制著不同的視覺概念，例如結構、風格和紋理品質。因此，我們提出HeadHunter框架，透過迭代選擇與使用者目標一致的注意力頭，實現對生成品質和視覺屬性的細粒度控制。此外，我們引入SoftPAG，透過線性插值調整擾動強度，減少生成瑕疵。此方法不僅解決了過度平滑問題，還能透過組合頭選擇來精準操控視覺風格。經驗證，在Stable Diffusion 3和FLUX.1等模型上，此方法在提升整體品質和風格引導方面表現優異。", "applications": ["**個性化頭像生成：** 想讓你的AI頭像更具特色？例如，指定髮型、服裝風格，甚至調整臉部表情的細微之處，都能輕鬆實現。", "**產品設計微調：** 設計師可以精確調整產品的視覺元素，例如材質紋理、光澤度，快速迭代出符合市場需求的設計方案。", "**藝術風格轉換：** 想讓照片呈現特定畫家的風格？透過選擇特定的注意力頭，就能將照片轉換成印象派、抽象派等不同風格，創作獨一無二的藝術作品。"], "pitch": "想像一下，一個AI圖像生成引擎，不僅能創造出逼真的圖像，還能像一位技藝精湛的畫家，精準地按照你的指令調整每一個細節。我們的技術“HeadHunter”正是實現這一願景的關鍵。它賦予了AI前所未有的控制力，讓使用者能夠以前所未有的方式塑造視覺內容。這項技術的潛在商業價值巨大：從遊戲、娛樂產業的角色和場景設計，到廣告行銷的客製化素材，再到電商平台的商品展示，甚至是醫療影像的精準分析，HeadHunter都能帶來革命性的改變。我們相信，HeadHunter將成為下一代圖像生成技術的核心引擎，引領AI視覺創作的新時代。現在投資，你將站在這波浪潮的最前端，共同開創一個由AI驅動的無限可能的視覺未來！", "audio": "docs/data/audios/2506.10978v1.wav"}
{"query": "AI", "id": "2506.10934v1", "url": "http://arxiv.org/abs/2506.10934v1", "title": "Dynamic Epistemic Friction in Dialogue", "summary": "Recent developments in aligning Large Language Models (LLMs) with human\npreferences have significantly enhanced their utility in human-AI collaborative\nscenarios. However, such approaches often neglect the critical role of\n\"epistemic friction,\" or the inherent resistance encountered when updating\nbeliefs in response to new, conflicting, or ambiguous information. In this\npaper, we define dynamic epistemic friction as the resistance to epistemic\nintegration, characterized by the misalignment between an agent's current\nbelief state and new propositions supported by external evidence. We position\nthis within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit,\n2011), where friction emerges as nontrivial belief-revision during the\ninteraction. We then present analyses from a situated collaborative task that\ndemonstrate how this model of epistemic friction can effectively predict belief\nupdates in dialogues, and we subsequently discuss how the model of belief\nalignment as a measure of epistemic resistance or friction can naturally be\nmade more sophisticated to accommodate the complexities of real-world dialogue\nscenarios.", "authors": ["Timothy Obiso", "Kenneth Lai", "Abhijnan Nath", "Nikhil Krishnaswamy", "James Pustejovsky"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:28:42.207994", "title_zh": "對話中的動態認知摩擦", "summary_zh": "本研究探討大型語言模型(LLM)在人機協作中忽略的「認知摩擦」問題。認知摩擦指的是當人們面對新的、衝突的或模糊的資訊時，更新信念時遇到的阻力。我們將「動態認知摩擦」定義為認知整合的阻力，也就是個體當前信念狀態與外部證據支持的新觀點之間的落差。我們利用動態認知邏輯框架，分析了在協作任務中，認知摩擦如何影響對話中的信念更新。研究結果顯示，此模型能有效預測對話中的信念變化，並可進一步完善，以應對真實對話的複雜性。", "applications": ["情境一：醫療診斷輔助。醫生在與AI系統討論病患病情時，AI的建議可能與醫生的經驗判斷不同。了解認知摩擦能幫助AI更好地呈現資訊，減少醫生因觀點差異而產生的抗拒感，提升診斷效率。", "情境二：協商談判。在商業談判中，不同立場的參與者對同一份合約條款可能有不同解讀。AI可以分析雙方的認知摩擦點，提供更具說服力的論點，促進達成共識。", "情境三：教育學習。學生在學習新知識時，如果新知識與既有觀念衝突，容易產生認知摩擦。AI導師可以根據學生的認知結構，調整教學策略，降低學習阻力，提升學習效果。"], "pitch": "各位創投先進，想像一下，我們正在打造的不僅僅是AI，而是真正能與人類有效協作的智能夥伴！現今的大型語言模型雖然強大，卻常常因為忽略人類的認知習慣，導致溝通效率低下，甚至產生誤解。我們的「動態認知摩擦」模型，就像是為AI裝上了一顆同理心，讓它能理解人類的認知盲點，減少溝通阻力，進而提升協作效率。試想一下，在醫療、金融、教育等領域，如果AI能更順暢地與專業人士協作，將釋放出多麼巨大的生產力！我們不僅掌握了核心技術，更擁有廣闊的應用前景。我們預期，未來五年內，這項技術將成為人機協作領域的關鍵標準，並為我們帶來數十億美元的市場價值。現在加入我們，您將成為這場人機協作革命的領航者！", "audio": "docs/data/audios/2506.10934v1.wav"}
{"query": "Foundation Model", "id": "2506.10914v1", "url": "http://arxiv.org/abs/2506.10914v1", "title": "Foundation Models for Causal Inference via Prior-Data Fitted Networks", "summary": "Prior-data fitted networks (PFNs) have recently been proposed as a promising\nway to train tabular foundation models. PFNs are transformers that are\npre-trained on synthetic data generated from a prespecified prior distribution\nand that enable Bayesian inference through in-context learning. In this paper,\nwe introduce CausalFM, a comprehensive framework for training PFN-based\nfoundation models in various causal inference settings. First, we formalize the\nconstruction of Bayesian priors for causal inference based on structural causal\nmodels (SCMs) in a principled way and derive necessary criteria for the\nvalidity of such priors. Building on this, we propose a novel family of prior\ndistributions using causality-inspired Bayesian neural networks that enable\nCausalFM to perform Bayesian causal inference in various settings, including\nback-door, front-door, and instrumental variable adjustment. Finally, we\ninstantiate CausalFM and explicitly train a foundation model for estimating\nconditional average treatment effects (CATEs) using back-door adjustment. We\nshow that CausalFM performs competitively for CATE estimation using various\nsynthetic and semi-synthetic benchmarks. In sum, our framework can be used as a\ngeneral recipe to train foundation models for various causal inference\nsettings. In contrast to the current state-of-the-art in causal inference,\nCausalFM offers a novel paradigm with the potential to fundamentally change how\npractitioners perform causal inference in medicine, economics, and other\ndisciplines.", "authors": ["Yuchen Ma", "Dennis Frauen", "Emil Javurek", "Stefan Feuerriegel"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:30:01.015475", "title_zh": "基於先驗資料擬合網路之因果推論基礎模型", "summary_zh": "本研究提出CausalFM，一個基於先驗資料擬合網路（PFNs）的因果推論基礎模型框架。PFNs透過在合成資料上預訓練Transformer，並利用上下文學習實現貝氏推論。CausalFM基於結構因果模型（SCMs），系統化地構建因果推論的貝氏先驗，並提出新型的因果啟發式貝氏神經網路先驗分佈，使其能在後門、前門和工具變數調整等多種情境下執行貝氏因果推論。實驗證明，CausalFM在條件平均處理效應（CATE）估計方面表現出色，為醫學、經濟學等領域的因果推論提供了一種全新的方法。", "applications": ["**個人化醫療：**想像一下，醫生可以利用CausalFM分析你的病歷、基因數據和生活習慣，準確預測哪種治療方案對你最有效，避免不必要的副作用，就像擁有一個超級聰明的醫療顧問。", "**精準行銷：**電商平台可以利用CausalFM分析你的購物行為、瀏覽紀錄和社群互動，精準預測你對哪些產品感興趣，並提供客製化的推薦和優惠，讓你每次購物都能滿載而歸。", "**政策模擬：**政府可以利用CausalFM模擬不同政策對經濟、社會和環境的影響，例如調整稅收政策、推動綠色能源等，從而制定更有效的政策，提升社會福祉。"], "pitch": "各位投資人，我們正站在一個劃時代的轉捩點上！CausalFM不僅僅是一個模型，它是一個通往因果關係理解的鑰匙，一個重塑決策方式的引擎。想像一下，一個AI可以精準預測新藥的療效、評估行銷活動的真實影響、甚至預測氣候變遷的長期後果。這就是CausalFM的潛力！在醫療領域，CausalFM能加速新藥研發，降低臨床試驗成本，實現精準醫療；在金融領域，CausalFM能更準確地預測市場風險，優化投資組合；在政策制定領域，CausalFM能幫助政府制定更有效的政策，提升社會福祉。我們相信，CausalFM將成為未來AI發展的核心動力，引領下一波科技革命。現在加入我們，共同開創一個更智慧、更美好的未來！", "audio": "docs/data/audios/2506.10914v1.wav"}
{"query": "Diffusion Model", "id": "2506.10971v1", "url": "http://arxiv.org/abs/2506.10971v1", "title": "What Exactly Does Guidance Do in Masked Discrete Diffusion Models", "summary": "We study masked discrete diffusion models with classifier-free guidance\n(CFG). Assuming no score error nor discretization error, we derive an explicit\nsolution to the guided reverse dynamics, so that how guidance influences the\nsampling behavior can be precisely characterized. When the full data\ndistribution is a mixture over classes and the goal is to sample from a\nspecific class, guidance amplifies class-specific regions while suppresses\nregions shared with other classes. This effect depends on the guidance strength\n$w$ and induces distinct covariance structures in the sampled distribution.\nNotably, we observe quantitatively different behaviors in $1$D and $2$D. We\nalso show that for large $w$, the decay rate of the total variation\n($\\mathrm{TV}$) along the reverse dynamics is double-exponential in $w$ for\nboth $1$D and $2$D. These findings highlight the role of guidance, not just in\nshaping the output distribution, but also in controlling the dynamics of the\nsampling trajectory. Our theoretical analysis is supported by experiments that\nillustrate the geometric effects of guidance and its impact on convergence.", "authors": ["He Ye", "Rojas Kevin", "Tao Molei"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:31:33.022686", "title_zh": "遮罩離散擴散模型中，引導究竟做了什麼？", "summary_zh": "本研究深入探討具備無分類器引導（CFG）的遮罩離散擴散模型。在假設沒有分數誤差和離散化誤差的前提下，我們推導出引導反向動力學的顯式解，從而精確地描述引導如何影響取樣行為。當完整數據分佈是各類別的混合，且目標是從特定類別中取樣時，引導會放大特定類別區域，同時抑制與其他類別共享的區域。這種效應取決於引導強度w，並在取樣分佈中產生不同的共變異數結構。我們觀察到一維和二維中定量不同的行為。此外，我們證明對於大的w，沿反向動力學的總變異（TV）的衰減率在一維和二維中都是w的雙指數。這些發現突顯了引導的作用，不僅在於塑造輸出分佈，還在於控制取樣軌跡的動力學。我們的理論分析得到了實驗的支持，這些實驗說明了引導的幾何效應及其對收斂的影響。", "applications": ["AI繪圖助手：讓使用者指定風格或主題，AI就能精準生成符合需求的圖像，例如：『畫一張梵谷風格的貓咪』。", "醫療影像分析：協助醫生更準確地辨識X光片或MRI中的病灶，例如：『找出肺部CT影像中疑似腫瘤的區域』。", "語音合成：根據文字生成更自然、更具情感的語音，例如：『用溫柔的語氣朗讀這段睡前故事』。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能精準控制生成式AI的輸出結果，就像一位技藝精湛的畫家，能完全按照您的指令創作。想像一下，這項技術能讓AI繪圖工具不再隨機生成，而是精準呈現使用者想要的風格和細節；能讓醫療影像分析AI更準確地找出病灶，大幅提升診斷效率；能讓語音合成AI更富情感，創造更逼真的虛擬助手。這項技術的核心優勢在於其對擴散模型『引導』的精準控制，這意味著我們能大幅降低AI生成內容的隨機性，提高內容品質和實用性。未來，我們將把這項技術應用於各個領域，從娛樂、醫療到教育，打造一個由精準AI驅動的全新世界。我們相信，這項技術的商業價值將是無限的，現在加入我們，您將有機會成為這場AI革命的領航者！", "audio": "docs/data/audios/2506.10971v1.wav"}
{"query": "AI", "id": "2506.10927v1", "url": "http://arxiv.org/abs/2506.10927v1", "title": "The Role of Generative AI in Facilitating Social Interactions: A Scoping Review", "summary": "Reduced social connectedness increasingly poses a threat to mental health,\nlife expectancy, and general well-being. Generative AI (GAI) technologies, such\nas large language models (LLMs) and image generation tools, are increasingly\nintegrated into applications aimed at enhancing human social experiences.\nDespite their growing presence, little is known about how these technologies\ninfluence social interactions. This scoping review investigates how GAI-based\napplications are currently designed to facilitate social interaction, what\nforms of social engagement they target, and which design and evaluation\nmethodologies designers use to create and evaluate them. Through an analysis of\n30 studies published since 2020, we identify key trends in application domains\nincluding storytelling, socio-emotional skills training, reminiscence,\ncollaborative learning, music making, and general conversation. We highlight\nthe role of participatory and co-design approaches in fostering both effective\ntechnology use and social engagement, while also examining socio-ethical\nconcerns such as cultural bias and accessibility. This review underscores the\npotential of GAI to support dynamic and personalized interactions, but calls\nfor greater attention to equitable design practices and inclusive evaluation\nstrategies.", "authors": ["T. T. J. E. Arets", "G. Perugia", "M. Houben", "W. A. IJsselsteijn"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:53:11.065720", "title_zh": "生成式人工智慧在促進社交互動中的作用：範圍界定綜述", "summary_zh": "社交連結的減少對心理健康和壽命構成威脅。生成式AI（GAI），如大型語言模型和圖像生成工具，正被整合到增強人類社交體驗的應用中。本研究分析了30篇2020年後發表的文獻，探討GAI如何促進社交互動，以及設計者使用的設計和評估方法。研究發現，GAI在故事敘述、社交情感技能訓練、懷舊、協作學習、音樂創作和一般對話等領域有應用。強調參與式設計對技術有效性和社交互動的重要性，並關注文化偏見和可訪問性等社會倫理問題。GAI有潛力支持動態和個人化的互動，但需要更關注公平的設計實踐和包容性的評估策略。", "applications": ["想像一下，獨居長輩可以使用AI聊天機器人，不僅能閒話家常，還能根據長輩的興趣，播放懷舊歌曲、分享老照片，甚至協助長輩與遠方的親友視訊通話，讓他們感受到被關懷。", "學校可以利用AI設計角色扮演遊戲，讓學生在安全的情境下練習社交技巧，例如如何表達自己的想法、如何處理衝突，或者如何與不同文化背景的人溝通。", "企業可以使用AI工具來促進團隊合作，例如AI可以分析團隊成員的溝通模式，找出潛在的衝突點，並提供改善建議，或者協助團隊進行腦力激盪，產生更多創新想法。"], "pitch": "各位投資人，我們正站在一個社交革命的起點！孤獨感是21世紀的流行病，而我們的技術正是解藥。生成式AI不僅僅是個工具，它是連接人與人的橋樑，是情感的催化劑。想像一下，一個AI伴侶能為自閉症兒童提供社交訓練，一個AI導師能為內向者提供自信表達的指導。我們的技術能打破地理限制，讓世界各地的人們建立有意義的連結。市場潛力巨大！從銀髮照護到青少年心理健康，從企業團隊建設到國際文化交流，我們的技術無所不在。我們不僅僅在開發AI，我們在創造更美好、更互聯的世界。現在投資我們，你投資的不僅僅是一個公司，而是投資一個更有愛的未來！未來，我們甚至可以打造一個完全由AI驅動的虛擬社交平台，讓使用者在其中體驗前所未有的社交互動，創造無限商機！", "audio": "docs/data/audios/2506.10927v1.wav"}
{"query": "Foundation Model", "id": "2506.10579v1", "url": "http://arxiv.org/abs/2506.10579v1", "title": "Equations of state and stability condition of mixed p-spin glass model", "summary": "The Sherrington-Kirkpatrick (SK) is a foundational model for understanding\nspin glass systems. It is based on the pairwise interaction between each two\nspins in a fully connected lattice with quenched disordered interactions. The\nnature of long-range interaction among spins in the (SK) model simplifies the\nstudy of this system by eliminating fluctuations. An advanced (SK) model, known\nas the p-spin model, introduces higher-order interactions that involve the\ninteraction of P spins. This research focuses on the general Hamiltonian of the\nspin glass model with long-range interaction, referred to as the mixed p-spin\nglass model, which consists of adding all p-spin interaction terms. This\nresearch aims to derive the equation of states for this Hamiltonian, formulate\nthe equation of state within the framework of the first replica symmetry\nbreaking, and determine both the stability condition of the replica symmetric\nsolution and the stability of the replicas belonging to the same group in the\nfirst step of replica symmetry breaking.", "authors": ["Ali Talebi"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:54:21.164725", "title_zh": "混合p-自旋玻璃模型的狀態方程式與穩定性條件", "summary_zh": "本研究探討一種更複雜的自旋玻璃模型，稱為混合p-自旋模型。這種模型考慮了多個自旋之間的交互作用，而不僅僅是兩兩之間的交互作用。研究目標是推導出此模型的狀態方程式，並在第一步複製對稱性破壞的框架下，確定複製對稱解的穩定性條件，以及屬於同一組的複製品的穩定性。這有助於更深入理解複雜系統的行為，例如材料的磁性或神經網路的運作。", "applications": ["材料科學：想像一下，我們可以設計出具有特定磁性或電性的新型材料，應用於更高效的儲存設備或更靈敏的感測器。例如，開發出在極端溫度下仍能穩定運作的磁性材料。", "神經網路：這個模型有助於我們理解大腦中神經元之間的複雜互動，進而開發出更強大的人工智慧。例如，設計出更有效率的機器學習演算法，提升AI的學習速度和準確性。", "金融風險管理：金融市場的波動性也可以用類似的模型來描述。透過理解這些複雜的交互作用，我們可以更準確地預測市場風險，並設計出更有效的風險管理策略。"], "pitch": "各位投資人，我們正在開發一種革命性的模型，能夠解開複雜系統的底層奧秘。混合p-自旋玻璃模型不僅僅是一個學術概念，它擁有巨大的商業潛力。想像一下，如果我們能夠精準預測新材料的特性，加速新藥開發，甚至預測金融市場的崩盤，這將帶來多大的價值？我們的研究團隊已經在這個領域取得了突破性進展，掌握了關鍵的狀態方程式和穩定性條件。我們相信，透過您的投資，我們可以將這個模型應用於各個領域，從材料科學到人工智慧，再到金融科技，創造一個全新的產業生態。這不僅是一項投資，更是一場科技革命，讓我們一起引領未來！", "audio": "docs/data/audios/2506.10579v1.wav"}
{"query": "Diffusion Model", "id": "2506.10963v1", "url": "http://arxiv.org/abs/2506.10963v1", "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "summary": "In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning--a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits--low entity fidelity, weak\nrelations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,\nunderscoring the benchmark's difficulty. To spur further progress, we release\nFLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines\na reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.", "authors": ["Yuxuan Luo", "Yuhui Yuan", "Junwen Chen", "Haonan Cai", "Ziyi Yue", "Yuwei Yang", "Fatima Zohra Daha", "Ji Li", "Zhouhui Lian"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:55:51.388040", "title_zh": "MMMG：一個用於文字生成圖像推理的大規模、多學科、多層級生成基準", "summary_zh": "本研究提出知識圖像生成的新任務，並創建了大規模多學科多層級知識圖像生成基準（MMMG），以評估圖像生成模型的推理能力。知識圖像對人類文明和學習至關重要。MMMG包含4456個專家驗證的圖像提示對，涵蓋10個學科、6個教育程度和多種知識格式。為簡化評估，採用統一的知識圖譜（KG）表示。我們還引入MMMG-Score來評估生成的知識圖像，結合了基於圖編輯距離的事實準確性和視覺清晰度評估。評估顯示，現有模型存在推理缺陷，GPT-4o的MMMG-Score僅為50.20。我們發布了FLUX-Reason，一個有效的開源基準，結合推理LLM和擴散模型，並在16000個知識圖像提示對上進行訓練。", "applications": ["**輔助學習：** 小明在學習歷史，只要輸入「秦始皇焚書坑儒」，系統就能自動生成一張圖，清楚呈現事件的來龍去脈，幫助他更快理解和記憶。", "**簡報製作：** 企劃人員小美要向老闆報告市場分析，只要輸入「近五年飲料市場趨勢圖」，系統就能快速生成專業的圖表，省下大量製作時間。", "**兒童教育：** 爸媽想教小朋友認識動物，只要輸入「獅子的生活習性」，系統就能生成一張生動有趣的圖片，配上簡單的文字說明，讓小朋友輕鬆學習。"], "pitch": "各位投資人，我們正在開發一項顛覆性的技術：基於MMMG基準的知識圖像生成引擎。想像一下，未來人們不再需要死記硬背，只要輸入文字，就能立即生成清晰易懂的知識圖像，大幅提升學習效率和資訊傳播速度。這項技術的應用範圍極廣，從教育、醫療到商業簡報，都能看到它的身影。我們相信，隨著AI技術的發展，視覺化知識將成為主流。我們的團隊將持續優化模型，擴大知識庫，並積極探索商業模式，例如訂閱制、API服務等。我們預計在三年內，搶佔知識圖像生成市場的領導地位，為投資者帶來豐厚的回報！這不僅僅是一項技術，更是一場知識革命，邀請您與我們一同參與，共同創造未來！", "audio": "docs/data/audios/2506.10963v1.wav"}
{"query": "AI", "id": "2506.10916v1", "url": "http://arxiv.org/abs/2506.10916v1", "title": "Semi-Automated Quality Assurance in Digital Pathology: Tile Classification Approach", "summary": "Quality assurance is a critical but underexplored area in digital pathology,\nwhere even minor artifacts can have significant effects. Artifacts have been\nshown to negatively impact the performance of AI diagnostic models. In current\npractice, trained staff manually review digitized images prior to release of\nthese slides to pathologists which are then used to render a diagnosis.\nConventional image processing approaches, provide a foundation for detecting\nartifacts on digital pathology slides. However, current tools do not leverage\ndeep learning, which has the potential to improve detection accuracy and\nscalability. Despite these advancements, methods for quality assurance in\ndigital pathology remain limited, presenting a gap for innovation.\n  We propose an AI algorithm designed to screen digital pathology slides by\nanalyzing tiles and categorizing them into one of 10 predefined artifact types\nor as background. This algorithm identifies and localizes artifacts, creating a\nmap that highlights regions of interest. By directing human operators to\nspecific tiles affected by artifacts, the algorithm minimizes the time and\neffort required to manually review entire slides for quality issues.\n  From internal archives and The Cancer Genome Atlas, 133 whole slide images\nwere selected and 10 artifacts were annotated using an internally developed\nsoftware ZAPP (Mayo Clinic, Jacksonville, FL). Ablation study of multiple\nmodels at different tile sizes and magnification was performed. InceptionResNet\nwas selected. Single artifact models were trained and tested, followed by a\nlimited multiple instance model with artifacts that performed well together\n(chatter, fold, and pen). From the results of this study we suggest a hybrid\ndesign for artifact screening composed of both single artifact binary models as\nwell as multiple instance models to optimize detection of each artifact.", "authors": ["Meredith VandeHaar", "M. Clinch", "I. Yilmaz", "M. A. Rahman", "Y. Xiao", "F. Dogany", "H. M. Alazab", "A. Nassar", "Z. Akkus", "B. Dangott"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:28:20.680718", "title_zh": "數位病理學中的半自動化品質保證：切片分類方法", "summary_zh": "數位病理切片中的偽影會嚴重影響AI診斷模型的準確性。目前仰賴人工檢查數位化切片，耗時費力。本研究提出一種AI演算法，通過分析切片並將其分類為10種預定義的偽影類型或背景，來篩選數位病理切片。該演算法能識別並定位偽影，生成突出顯示感興趣區域的地圖，引導操作員關注受偽影影響的特定切片，從而最大限度地減少人工審查整個切片以發現品質問題所需的時間和精力。實驗結果表明，採用單一偽影二元模型和多重實例模型的混合設計，能優化各種偽影的檢測。", "applications": ["醫院品管人員可以利用這套系統快速篩檢數位病理切片，確保診斷品質，減少人為疏失。", "遠距醫療平台可以整合這項技術，讓病理專家在線上就能快速判斷切片品質，提升遠端診斷的效率和準確性。", "病理學系的學生可以利用這套系統學習辨識各種偽影，加速學習進度，提升專業能力。"], "pitch": "各位投資人，我們正在革新數位病理學的品質保證！想像一下，AI醫生在診斷癌症時，不會再因為切片上的污漬或摺痕而誤判。我們的半自動化品質保證系統，能精準識別並標記數位病理切片中的偽影，大幅提升AI診斷的準確性與效率。這不僅能減少醫療失誤，更能加速新藥開發，甚至實現個人化醫療。數位病理市場正以驚人的速度成長，而我們的技術將成為這個市場的關鍵基礎設施。我們預計，在未來五年內，所有大型醫院和研究機構都將採用類似的解決方案。現在投資，您將成為這場醫療AI革命的領頭羊，共同打造一個更精準、更高效的醫療未來！", "audio": "docs/data/audios/2506.10916v1.wav"}
{"query": "Foundation Model", "id": "2506.10395v1", "url": "http://arxiv.org/abs/2506.10395v1", "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation", "summary": "Recent advances in large language models (LLMs) have enabled multimodal\nfoundation models to tackle both image understanding and generation within a\nunified framework. Despite these gains, unified models often underperform\ncompared to specialized models in either task. A key challenge in developing\nunified models lies in the inherent differences between the visual features\nneeded for image understanding versus generation, as well as the distinct\ntraining processes required for each modality. In this work, we introduce\nPisces, an auto-regressive multimodal foundation model that addresses this\nchallenge through a novel decoupled visual encoding architecture and tailored\ntraining techniques optimized for multimodal generation. Combined with\nmeticulous data curation, pretraining, and finetuning, Pisces achieves\ncompetitive performance in both image understanding and image generation. We\nevaluate Pisces on over 20 public benchmarks for image understanding, where it\ndemonstrates strong performance across a wide range of tasks. Additionally, on\nGenEval, a widely adopted benchmark for image generation, Pisces exhibits\nrobust generative capabilities. Our extensive analysis reveals the synergistic\nrelationship between image understanding and generation, and the benefits of\nusing separate visual encoders, advancing the field of unified multimodal\nmodels.", "authors": ["Zhiyang Xu", "Jiuhai Chen", "Zhaojiang Lin", "Xichen Pan", "Lifu Huang", "Tianyi Zhou", "Madian Khabsa", "Qifan Wang", "Di Jin", "Michihiro Yasunaga", "Lili Yu", "Xi Victoria Lin", "Shaoliang Nie"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:29:25.920042", "title_zh": "雙魚座：用於圖像理解與生成的自迴歸基礎模型", "summary_zh": "本研究提出名為「雙魚座」的自迴歸多模態基礎模型，旨在解決圖像理解與生成在統一模型中表現不佳的問題。雙魚座採用獨特的解耦視覺編碼架構，並針對多模態生成進行優化訓練。透過精心的數據管理、預訓練與微調，雙魚座在圖像理解和生成方面均展現了卓越的性能。在超過20個公共圖像理解基準測試中，雙魚座表現出色；在GenEval圖像生成基準測試中，也展現了強大的生成能力。研究揭示了圖像理解與生成之間的協同關係，以及使用獨立視覺編碼器的優勢，推動了統一多模態模型領域的發展。", "applications": ["智慧相機：相機能自動理解照片內容，例如識別出人物、地點、物品，並根據場景提供最佳拍攝建議，甚至自動生成風格化的照片。", "輔助設計：設計師可以口頭描述設計理念，系統自動生成草圖或3D模型，加速設計流程，並提供更多創意靈感。", "教育娛樂：孩子可以與AI互動，描述想像中的畫面，AI立即生成圖像，激發孩子創造力，並學習圖像相關知識。"], "pitch": "各位投資人，我們相信圖像理解和生成技術是AI的下一個爆點！「雙魚座」模型在統一架構下同時實現了卓越的圖像理解和生成能力，打破了傳統模型在兩者間的trade-off。想像一下，未來電商平台能根據用戶上傳的房屋照片，自動推薦合適的家具擺設；醫療領域能根據X光片自動診斷疾病；自動駕駛能更精準地理解周遭環境。雙魚座不僅能賦能現有產業，更能催生全新的商業模式。我們團隊擁有頂尖的AI專家和豐富的產業經驗，現在加入，您將有機會成為下一波AI浪潮的領航者！我們預計在三年內，雙魚座能成為市場上最受歡迎的圖像AI引擎，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.10395v1.wav"}
{"query": "Diffusion Model", "id": "2506.10962v1", "url": "http://arxiv.org/abs/2506.10962v1", "title": "SpectralAR: Spectral Autoregressive Visual Generation", "summary": "Autoregressive visual generation has garnered increasing attention due to its\nscalability and compatibility with other modalities compared with diffusion\nmodels. Most existing methods construct visual sequences as spatial patches for\nautoregressive generation. However, image patches are inherently parallel,\ncontradicting the causal nature of autoregressive modeling. To address this, we\npropose a Spectral AutoRegressive (SpectralAR) visual generation framework,\nwhich realizes causality for visual sequences from the spectral perspective.\nSpecifically, we first transform an image into ordered spectral tokens with\nNested Spectral Tokenization, representing lower to higher frequency\ncomponents. We then perform autoregressive generation in a coarse-to-fine\nmanner with the sequences of spectral tokens. By considering different levels\nof detail in images, our SpectralAR achieves both sequence causality and token\nefficiency without bells and whistles. We conduct extensive experiments on\nImageNet-1K for image reconstruction and autoregressive generation, and\nSpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project\npage: https://huang-yh.github.io/spectralar/.", "authors": ["Yuanhui Huang", "Weiliang Chen", "Wenzhao Zheng", "Yueqi Duan", "Jie Zhou", "Jiwen Lu"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:30:56.769031", "title_zh": "SpectralAR：頻譜自迴歸視覺生成", "summary_zh": "現有的自迴歸視覺生成方法多以圖像的空間色塊作為序列，但色塊本質上是平行的，與自迴歸模型的因果關係相悖。本研究提出SpectralAR框架，從頻譜角度實現視覺序列的因果性。首先，利用嵌套頻譜標記將圖像轉換為有序的頻譜標記，由低頻到高頻表示圖像的不同細節層次。然後，以粗略到精細的方式，利用頻譜標記序列進行自迴歸生成。實驗證明，SpectralAR在ImageNet-1K圖像重建和自迴歸生成任務上表現出色，僅用64個標記和3.1億參數就達到了3.02 gFID。", "applications": ["智慧型修圖App：使用者只需簡單描述想要修改的內容，例如「讓天空更藍」、「讓花朵更鮮豔」，App就能利用SpectralAR技術，精準地調整圖像的頻譜分佈，生成高品質的修改後圖片。", "AI藝術創作工具：藝術家或設計師可以使用SpectralAR來生成獨特的藝術作品。透過調整頻譜參數，可以創造出各種風格的圖像，例如印象派、抽象派等，激發無限的創作靈感。", "醫療影像增強：SpectralAR可以應用於醫療影像處理，例如X光片或MRI掃描。透過增強特定頻率的信號，可以更清晰地顯示病灶，幫助醫生更準確地診斷疾病。"], "pitch": "各位投資人，想像一下，未來AI生成圖像不再需要耗費大量算力，也不再受限於模糊不清的細節。SpectralAR技術的誕生，正是要打破這些限制。它就像一個高效的圖像解碼器，能將圖像分解成頻譜信息，再以極高的效率重新組合，生成高品質的圖像。這意味著，我們可以將這項技術應用於各行各業：從電商平台的商品圖像優化，到遊戲公司的AI美術資源生成，再到電影特效的快速渲染，市場潛力巨大。更重要的是，SpectralAR的低算力需求，讓它非常適合部署在移動設備上，這將開啟一個全新的移動AI圖像處理時代。我們相信，SpectralAR將成為AI視覺領域的下一個獨角獸，現在加入，您將有機會共同見證這項技術顛覆世界的時刻！", "audio": "docs/data/audios/2506.10962v1.wav"}
{"query": "AI", "id": "2506.10908v1", "url": "http://arxiv.org/abs/2506.10908v1", "title": "Probably Approximately Correct Labels", "summary": "Obtaining high-quality labeled datasets is often costly, requiring either\nextensive human annotation or expensive experiments. We propose a method that\nsupplements such \"expert\" labels with AI predictions from pre-trained models to\nconstruct labeled datasets more cost-effectively. Our approach results in\nprobably approximately correct labels: with high probability, the overall\nlabeling error is small. This solution enables rigorous yet efficient dataset\ncuration using modern AI models. We demonstrate the benefits of the methodology\nthrough text annotation with large language models, image labeling with\npre-trained vision models, and protein folding analysis with AlphaFold.", "authors": ["Emmanuel J. Candès", "Andrew Ilyas", "Tijana Zrnic"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:36:07.748516", "title_zh": "可能近似正確標籤", "summary_zh": "取得高品質標記資料集通常成本高昂。本研究提出一種方法，利用預訓練模型的AI預測來補充「專家」標籤，以更具成本效益的方式構建標記資料集。我們的方案能產生「可能近似正確」的標籤：整體標記錯誤的機率很高，但錯誤率很小。此方案能使用現代AI模型進行嚴謹且高效的資料集管理。我們透過大型語言模型的文本標註、預訓練視覺模型的圖像標記，以及AlphaFold的蛋白質摺疊分析，展示了此方法的優勢。", "applications": ["想像一下，以後醫生看X光片，AI會先幫忙標記出可疑的地方，醫生再仔細檢查，這樣可以減少誤判，讓診斷更準確快速。", "以後網購，AI可以自動判斷商品圖片的內容，例如衣服的款式、顏色，消費者只要輸入關鍵字，就能更快找到想要的商品。", "農民伯伯可以用手機拍照，AI就能判斷農作物有沒有生病、缺什麼養分，及時採取措施，提高產量。"], "pitch": "各位投資人，我們正在革新資料標記領域！想像一下，現在AI訓練需要海量資料，但人工標記成本高昂且耗時。我們的技術能利用現有AI模型，輔助專家進行資料標記，大幅降低成本，同時保證標記品質。這就像是為AI訓練提供了一條高速公路，加速AI發展。我們已經在文本、圖像和生物領域驗證了其有效性，證明其廣泛的應用潛力。未來，我們可以將此技術應用於自動駕駛、醫療診斷、金融風控等各個領域，市場前景無限。現在投資我們，就是投資AI的未來！", "audio": "docs/data/audios/2506.10908v1.wav"}
{"query": "Foundation Model", "id": "2506.10386v1", "url": "http://arxiv.org/abs/2506.10386v1", "title": "Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial", "summary": "The burial state of anthropogenic objects on the seafloor provides insight\ninto localized sedimentation dynamics and is also critical for assessing\necological risks, potential pollutant transport, and the viability of recovery\nor mitigation strategies for hazardous materials such as munitions. Accurate\nburial depth estimation from remote imagery remains difficult due to partial\nocclusion, poor visibility, and object degradation. This work introduces a\ncomputer vision pipeline, called PoseIDON, which combines deep foundation model\nfeatures with multiview photogrammetry to estimate six degrees of freedom\nobject pose and the orientation of the surrounding seafloor from ROV video.\nBurial depth is inferred by aligning CAD models of the objects with observed\nimagery and fitting a local planar approximation of the seafloor. The method is\nvalidated using footage of 54 objects, including barrels and munitions,\nrecorded at a historic ocean dumpsite in the San Pedro Basin. The model\nachieves a mean burial depth error of approximately 10 centimeters and resolves\nspatial burial patterns that reflect underlying sediment transport processes.\nThis approach enables scalable, non-invasive mapping of seafloor burial and\nsupports environmental assessment at contaminated sites.", "authors": ["Jerry Yan", "Chinmay Talegaonkar", "Nicholas Antipa", "Eric Terrill", "Sophia Merrifield"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:37:10.124025", "title_zh": "利用6DoF姿態基礎模型繪製海洋沉積物覆蓋圖", "summary_zh": "這項研究提出名為PoseIDON的電腦視覺流程，結合深度學習基礎模型特徵與多視角攝影測量技術，從水下機器人影片中估算海底物體的六自由度姿態及周圍海床方向。藉由將物體的CAD模型與觀測影像對齊，並擬合海床局部平面近似，推斷埋藏深度。實驗結果顯示，該模型能以約10公分的平均誤差估算埋藏深度，並解析反映沉積物傳輸過程的空間埋藏模式。這項技術實現了海底埋藏的可擴展、非侵入式繪圖，並支持受污染場址的環境評估。", "applications": ["海洋工程：精準定位海底管線或結構物，避免施工時誤觸或損壞。", "環境監測：追蹤海底垃圾或有害物質的分布與移動，評估對生態環境的影響。", "考古研究：協助考古學家定位與記錄沉沒的古物或遺跡，重現歷史場景。"], "pitch": "各位投資人，我們正在革新海洋環境監測方式！想像一下，我們能精準掌握海底廢棄物、沉船殘骸，甚至是潛在的汙染源。PoseIDON技術不僅能繪製高精度的海底地圖，還能預測沉積物移動，協助我們保護海洋生態，降低潛在風險。這項技術的應用範圍廣泛，從海洋工程、環境監測到考古研究，都有巨大的商業潛力。隨著海洋資源的開發日益重要，PoseIDON將成為不可或缺的工具。我們預期，未來能將這項技術應用於深海採礦、海底能源開發等領域，創造更大的經濟效益。現在投資PoseIDON，就是投資海洋的未來！", "audio": "docs/data/audios/2506.10386v1.wav"}
{"query": "Diffusion Model", "id": "2506.10955v1", "url": "http://arxiv.org/abs/2506.10955v1", "title": "ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems", "summary": "There has been a flurry of activity around using pretrained diffusion models\nas informed data priors for solving inverse problems, and more generally around\nsteering these models using reward models. Training-free methods like diffusion\nposterior sampling (DPS) and its many variants have offered flexible heuristic\nalgorithms for these tasks, but when the reward is not informative enough,\ne.g., in hard inverse problems with low signal-to-noise ratio, these techniques\nveer off the data manifold, failing to produce realistic outputs. In this work,\nwe devise a simple wrapper, ReGuidance, for boosting both the sample realism\nand reward achieved by these methods. Given a candidate solution $\\hat{x}$\nproduced by an algorithm of the user's choice, we propose inverting the\nsolution by running the unconditional probability flow ODE in reverse starting\nfrom $\\hat{x}$, and then using the resulting latent as an initialization for\nDPS. We evaluate our wrapper on hard inverse problems like large box\nin-painting and super-resolution with high upscaling. Whereas state-of-the-art\nbaselines visibly fail, we find that applying our wrapper on top of these\nbaselines significantly boosts sample quality and measurement consistency. We\ncomplement these findings with theory proving that on certain multimodal data\ndistributions, ReGuidance simultaneously boosts the reward and brings the\ncandidate solution closer to the data manifold. To our knowledge, this\nconstitutes the first rigorous algorithmic guarantee for DPS.", "authors": ["Aayush Karan", "Kulin Shah", "Sitan Chen"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:38:29.432841", "title_zh": "ReGuidance：一個用於提升困難反問題樣本品質的簡單擴散模型封裝器", "summary_zh": "本研究提出ReGuidance，一個簡單的封裝器，旨在提升預訓練擴散模型在解決反問題時的樣本真實性和獎勵。針對如擴散後驗採樣(DPS)等無訓練方法在低訊噪比的困難反問題中表現不佳的問題，ReGuidance通過反轉候選解，將其轉換為潛在空間的初始化點，再利用DPS進行優化。實驗結果顯示，在大型圖像修復和高倍率超解析度等任務中，ReGuidance能顯著提升樣本品質和一致性，超越現有技術。理論分析也證明，ReGuidance能同時提升獎勵並使候選解更接近數據流形，為DPS提供了首個嚴格的演算法保證。", "applications": ["老照片修復：透過ReGuidance技術，可以將模糊不清的老照片還原成清晰、細節豐富的影像，讓珍貴的回憶重現光彩。", "醫療影像增強：在X光片或MRI等醫療影像中，ReGuidance能提升影像解析度，幫助醫生更精準地診斷疾病，例如偵測微小的腫瘤。", "犯罪現場重建：利用ReGuidance技術，可以將模糊的監視器畫面清晰化，幫助警方還原犯罪現場的細節，提高破案率。"], "pitch": "各位創投先進，我們正在開發一項突破性的圖像處理技術，ReGuidance，它能大幅提升反問題的解決方案品質，尤其是在現有技術難以應付的困難場景。想像一下，未來我們可以將模糊的衛星圖像轉化為高解析度的地圖，用於精準農業、城市規劃，甚至國防安全。或者，我們能將低畫質的影片升級為4K甚至8K，重塑影視娛樂產業。ReGuidance不僅能提升現有圖像處理技術的性能，更開啟了全新的商業模式。我們預計，隨著AIoT和元宇宙的發展，ReGuidance在智慧城市、自動駕駛、虛擬實境等領域將有巨大的應用潛力。現在投資，您將成為這場圖像革命的先驅！", "audio": "docs/data/audios/2506.10955v1.wav"}
