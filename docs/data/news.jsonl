{"query": "AI", "id": "2506.06242v1", "url": "http://arxiv.org/abs/2506.06242v1", "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models", "summary": "Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}", "authors": ["Zahra Babaiee", "Peyman M. Kiasari", "Daniela Rus", "Radu Grosu"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:19:31.539877", "title_zh": "視覺圖形競技場：評估視覺和多模態大型語言模型的視覺概念化能力", "summary_zh": "近年來，多模態大型語言模型在視覺問答方面取得了顯著進展。然而，AI在「概念化」能力上仍存在差距，也就是辨識和推理相同概念，不受視覺形式變化的影響。為了解決這個問題，我們推出了視覺圖形競技場（VGA），它是一個包含六個基於圖形的任務的數據集，旨在評估和提升AI系統的視覺抽象能力。VGA使用不同的圖形佈局來測試獨立於視覺形式的推理。實驗結果顯示，人類在各項任務中幾乎達到完美準確度，而模型在同構檢測方面完全失敗，在路徑/循環任務方面表現有限，這突顯了當前AI模型在視覺理解方面的根本局限性。VGA提供了一個框架，旨在推動AI視覺模型在概念化方面取得類似人類的進展。", "applications": ["**自動駕駛：** 讓汽車能辨識不同角度或光線下的交通標誌，確保行車安全。例如，即使交通標誌被樹葉遮蔽一部分，或因光線反射而變形，汽車也能正確判斷其意義。", "**醫療影像分析：** 協助醫生辨識X光片或斷層掃描中不同形態的腫瘤，提高診斷準確性。例如，即使腫瘤形狀不規則或與周圍組織融合，AI也能準確辨識並標記。", "**智慧零售：** 讓機器人能辨識貨架上不同包裝或擺放方式的商品，提升倉儲和物流效率。例如，即使商品條碼被遮蓋，或商品被隨意堆放，機器人也能準確辨識商品種類和數量。"], "pitch": "各位投資人，我們正處於AI革命的風口浪尖！視覺圖形競技場（VGA）不僅僅是一個數據集，它是解鎖AI真正視覺理解能力的鑰匙。試想一下，一個能像人類一樣理解世界，不受視覺表象干擾的AI，它將顛覆自動駕駛、醫療診斷、智慧製造等各個領域。目前AI在概念化方面的不足，正是我們VGA的機會。我們正在打造下一代AI視覺引擎，它將超越簡單的模式匹配，真正理解圖像背後的概念。這意味著更安全可靠的自動駕駛、更精準高效的醫療診斷、以及更智能化的生產流程。我們的團隊由頂尖的AI專家組成，我們有信心將VGA打造成AI視覺領域的黃金標準。現在投資VGA，您不僅僅是投資一個數據集，更是投資一個充滿無限可能的未來！讓我們一起引領這場視覺智能的革命，共同創造一個更智能、更美好的世界！", "audio": "docs/data/audios/2506.06242v1.wav"}
{"query": "Foundation Model", "id": "2506.06281v1", "url": "http://arxiv.org/abs/2506.06281v1", "title": "TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation", "summary": "Modern Earth observation (EO) increasingly leverages deep learning to harness\nthe scale and diversity of satellite imagery across sensors and regions. While\nrecent foundation models have demonstrated promising generalization across EO\ntasks, many remain limited by the scale, geographical coverage, and spectral\ndiversity of their training data, factors critical for learning globally\ntransferable representations. In this work, we introduce TerraFM, a scalable\nself-supervised learning model that leverages globally distributed Sentinel-1\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\nsampling to enrich spatial and semantic coverage. By treating sensing\nmodalities as natural augmentations in our self-supervised approach, we unify\nradar and optical inputs via modality-specific patch embeddings and adaptive\ncross-attention fusion. Our training strategy integrates local-global\ncontrastive learning and introduces a dual-centering mechanism that\nincorporates class-frequency-aware regularization to address long-tailed\ndistributions in land cover.TerraFM achieves strong generalization on both\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\nhttps://github.com/mbzuai-oryx/TerraFM .", "authors": ["Muhammad Sohail Danish", "Muhammad Akhtar Munir", "Syed Roshaan Ali Shah", "Muhammad Haris Khan", "Rao Muhammad Anwer", "Jorma Laaksonen", "Fahad Shahbaz Khan", "Salman Khan"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:20:54.025845", "title_zh": "TerraFM：適用於統一多感測器地球觀測的可擴展基礎模型", "summary_zh": "TerraFM是一個利用深度學習技術，結合Sentinel-1和Sentinel-2衛星影像的可擴展自監督學習模型。它透過獨特的空間瓦片和土地覆蓋感知採樣方法，豐富了空間和語義覆蓋範圍。TerraFM將雷達和光學輸入視為自然增強，透過模態特定的patch嵌入和自適應交叉注意力融合來統一處理。其訓練策略結合了局部-全局對比學習，並引入雙中心機制，以解決土地覆蓋中長尾分佈的問題。TerraFM在分類和分割任務上表現出色，優於先前的模型，為地球觀測領域帶來了更強大的通用性和準確性。", "applications": ["農作物監測：農民可以利用TerraFM分析衛星影像，了解農作物的生長狀況、預測產量，及早發現病蟲害，提高農業生產效率。", "災害評估：在地震、洪水等災害發生後，TerraFM可以快速分析災區的受損情況，協助救援人員制定更有效的救援計畫，並進行災後重建。", "環境保護：環保機構可以利用TerraFM監測森林砍伐、水污染等環境問題，及時採取措施保護地球資源。"], "pitch": "各位創投先進，想像一下，我們正站在一個前所未有的數據金礦之上：地球觀測數據！TerraFM，我們的殺手級應用，正是開啟這座寶藏的鑰匙。它不僅能整合不同衛星感測器的數據，更具備強大的泛化能力，能應用於農業、災害管理、環境監測等各個領域。這意味著什麼？更精準的作物預測，減少糧食浪費；更快速的災害評估，拯救更多生命；更有效的環境監測，守護我們的地球。但這還不是全部！TerraFM的自監督學習能力，使其能不斷從海量數據中自我提升，就像一個永動機，不斷產生價值。未來，我們甚至可以將TerraFM應用於城市規劃、基礎設施建設、甚至是國防安全等更廣闊的領域。現在投資TerraFM，就是投資地球的未來，回報將遠超您的想像！", "audio": "docs/data/audios/2506.06281v1.wav"}
{"query": "Diffusion Model", "id": "2506.06276v1", "url": "http://arxiv.org/abs/2506.06276v1", "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis", "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.", "authors": ["Jiatao Gu", "Tianrong Chen", "David Berthelot", "Huangjie Zheng", "Yuyang Wang", "Ruixiang Zhang", "Laurent Dinh", "Miguel Angel Bautista", "Josh Susskind", "Shuangfei Zhai"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:22:30.194724", "title_zh": "STARFlow：擴展潛在歸一化流以實現高解析度圖像合成", "summary_zh": "STARFlow是一種基於歸一化流的可擴展生成模型，在高解析度圖像合成方面表現出色。其核心是Transformer自迴歸流（TARFlow），結合了歸一化流的表達能力和自迴歸Transformer的結構化建模能力。STARFlow通過深度-淺層設計、在預訓練自編碼器的潛在空間中建模以及創新的引導算法，顯著提高了可擴展性。該模型保持端到端的歸一化流，無需離散化即可在連續空間中進行精確的最大似然訓練。STARFlow在類條件和文本條件圖像生成任務中表現出色，其樣本品質接近最先進的擴散模型。這是首次成功展示歸一化流在此規模和解析度下有效運作。", "applications": ["想像一下，你想要一張獨一無二的寵物照片，但你沒有專業攝影師。STARFlow可以根據你的文字描述，例如「一隻戴著皇冠的可愛貓咪」，自動生成一張高解析度的照片。", "假設你是遊戲開發者，需要大量不同的遊戲角色和場景。STARFlow可以幫助你快速生成各種風格的遊戲素材，節省大量美術設計的時間和成本。", "如果你是室內設計師，想向客戶展示不同裝修風格的效果圖。STARFlow可以根據客戶的描述，快速生成逼真的室內設計圖，方便客戶選擇。"], "pitch": "各位投資人，我們帶來的是STARFlow，一項革命性的圖像生成技術，它將徹底改變圖像內容創作的遊戲規則！想像一下，一個可以根據簡單的文字描述，就能生成照片級別真實圖像的世界。STARFlow不僅僅是一個技術突破，它是一座金礦！在廣告行銷領域，它可以創造出高度個性化的廣告素材，大幅提升點擊率和轉換率。在娛樂產業，它可以賦予遊戲開發者和電影製作人前所未有的創作自由。在電商領域，它可以自動生成商品圖片，降低運營成本。更重要的是，隨著元宇宙的興起，對虛擬內容的需求將呈現爆炸式增長，而STARFlow正是滿足這一需求的完美解決方案。我們的團隊擁有世界一流的AI專家，我們已經成功驗證了STARFlow的技術可行性和商業潛力。現在，我們需要您的投資，共同將STARFlow推向市場，搶佔先機，打造一個全新的圖像內容生態系統。我們相信，STARFlow將成為下一代圖像生成技術的領導者，為您帶來豐厚的回報！", "audio": "docs/data/audios/2506.06276v1.wav"}
{"query": "AI", "id": "2506.06232v1", "url": "http://arxiv.org/abs/2506.06232v1", "title": "Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study", "summary": "While traditional computer vision models have historically struggled to\ngeneralize to endoscopic domains, the emergence of foundation models has shown\npromising cross-domain performance. In this work, we present the first\nlarge-scale study assessing the capabilities of Vision Language Models (VLMs)\nfor endoscopic tasks with a specific focus on laparoscopic surgery. Using a\ndiverse set of state-of-the-art models, multiple surgical datasets, and\nextensive human reference annotations, we address three key research questions:\n(1) Can current VLMs solve basic perception tasks on surgical images? (2) Can\nthey handle advanced frame-based endoscopic scene understanding tasks? and (3)\nHow do specialized medical VLMs compare to generalist models in this context?\nOur results reveal that VLMs can effectively perform basic surgical perception\ntasks, such as object counting and localization, with performance levels\ncomparable to general domain tasks. However, their performance deteriorates\nsignificantly when the tasks require medical knowledge. Notably, we find that\nspecialized medical VLMs currently underperform compared to generalist models\nacross both basic and advanced surgical tasks, suggesting that they are not yet\noptimized for the complexity of surgical environments. These findings highlight\nthe need for further advancements to enable VLMs to handle the unique\nchallenges posed by surgery. Overall, our work provides important insights for\nthe development of next-generation endoscopic AI systems and identifies key\nareas for improvement in medical visual language models.", "authors": ["Leon Mayer", "Tim Rädsch", "Dominik Michael", "Lucas Luttner", "Amine Yamlahi", "Evangelia Christodoulou", "Patrick Godau", "Marcel Knopp", "Annika Reinke", "Fiona Kolbinger", "Lena Maier-Hein"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:31:58.254264", "title_zh": "以外科手術數據挑戰視覺語言模型：一個新數據集與廣泛的基準測試研究", "summary_zh": "本研究首次大規模評估視覺語言模型（VLMs）在腹腔鏡手術等內視鏡任務中的能力。我們使用多種先進模型、手術數據集和人工標註，探討VLMs能否勝任手術圖像的基本感知任務和進階的內視鏡場景理解任務，以及專用醫療VLMs與通用模型的比較。結果顯示，VLMs在物體計數和定位等基本任務上表現出色，但處理需要醫學知識的任務時性能顯著下降。令人驚訝的是，專用醫療VLMs的表現不如通用模型，表明它們尚未針對手術環境的複雜性進行優化。這項研究突顯了未來開發內視鏡AI系統的需求，並為改進醫療視覺語言模型指明了方向。", "applications": ["想像一下，未來醫生在做腹腔鏡手術時，AI能即時辨識手術視野中的器官、血管，甚至提醒醫生注意潛在風險，就像有個經驗豐富的助手在旁邊一樣。", "以後醫學院學生可以利用這個AI系統來模擬手術，AI會根據學生的操作給予即時反饋，讓他們在真實手術前就能累積經驗。", "開發一套居家健康監測系統，透過內視鏡影像分析，早期發現腸胃道疾病，讓民眾在家就能進行初步的健康檢查。"], "pitch": "各位投資人，我們正在打造醫療AI的未來！這項技術不僅僅是個研究項目，它將徹底改變外科手術的面貌。想像一下，AI能輔助醫生進行更精準、更安全的手術，降低醫療事故的發生率，並大幅縮短手術時間。更重要的是，我們發現專用醫療模型的表現不如通用模型，這代表著巨大的市場機會！我們將開發針對手術環境優化的VLMs，解決現有模型的瓶頸，打造出真正能夠理解手術場景的AI。這不僅能應用於手術室，還能拓展到遠程醫療、醫學教育等領域，潛在市場規模數十億美元！現在加入我們，一起開創醫療AI的新紀元！", "audio": "docs/data/audios/2506.06232v1.wav"}
{"query": "Foundation Model", "id": "2506.06270v1", "url": "http://arxiv.org/abs/2506.06270v1", "title": "RecGPT: A Foundation Model for Sequential Recommendation", "summary": "This work addresses a fundamental barrier in recommender systems: the\ninability to generalize across domains without extensive retraining.\nTraditional ID-based approaches fail entirely in cold-start and cross-domain\nscenarios where new users or items lack sufficient interaction history.\nInspired by foundation models' cross-domain success, we develop a foundation\nmodel for sequential recommendation that achieves genuine zero-shot\ngeneralization capabilities. Our approach fundamentally departs from existing\nID-based methods by deriving item representations exclusively from textual\nfeatures. This enables immediate embedding of any new item without model\nretraining. We introduce unified item tokenization with Finite Scalar\nQuantization that transforms heterogeneous textual descriptions into\nstandardized discrete tokens. This eliminates domain barriers that plague\nexisting systems. Additionally, the framework features hybrid\nbidirectional-causal attention that captures both intra-item token coherence\nand inter-item sequential dependencies. An efficient catalog-aware beam search\ndecoder enables real-time token-to-item mapping. Unlike conventional approaches\nconfined to their training domains, RecGPT naturally bridges diverse\nrecommendation contexts through its domain-invariant tokenization mechanism.\nComprehensive evaluations across six datasets and industrial scenarios\ndemonstrate consistent performance advantages.", "authors": ["Yangqin Jiang", "Xubin Ren", "Lianghao Xia", "Da Luo", "Kangyi Lin", "Chao Huang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:33:25.131072", "title_zh": "RecGPT：用於序列推薦的基礎模型", "summary_zh": "RecGPT 是一個突破性的推薦系統，它像大型語言模型一樣，具備跨領域的泛化能力，不需要針對新領域重新訓練。它捨棄了傳統基於ID的方法，改為完全從文字特徵提取商品資訊，讓新商品能立即加入推薦，無需重新訓練模型。RecGPT 使用統一的商品符號化方法，將各種文字描述轉換為標準化的離散符號，消除了領域之間的障礙。此外，它還採用混合雙向因果注意力機制，捕捉商品內部的關聯和商品之間的順序關係。這種方法在六個數據集和工業場景中都展現了優越的性能，為推薦系統帶來了革命性的改變。", "applications": ["**個人化新聞推薦：** 不再只推薦你看過的新聞，而是根據你讀過的文章內容，推薦其他領域但主題相關的新聞，擴展你的知識視野。", "**跨平台商品推薦：** 假設你在A電商平台買了咖啡豆，RecGPT可以立刻在B平台上推薦你適合的咖啡濾杯或磨豆機，即使你在B平台沒有任何購買紀錄。", "**冷啟動影視推薦：** 新上映的冷門獨立電影，即使觀看人數不多，RecGPT也能透過電影簡介的文字內容，推薦給可能感興趣的觀眾，讓小眾佳作也能被發掘。"], "pitch": "各位投資人，想像一下，一個能理解所有商品和使用者喜好的超級推薦引擎，它不需要大量數據訓練，就能精準推薦，這就是RecGPT的潛力！傳統推薦系統就像個別的孤島，RecGPT則是一座連接所有島嶼的橋樑。它不僅解決了冷啟動和跨領域推薦的難題，更開創了全新的商業模式。我們可以將RecGPT授權給各個電商平台、內容平台，甚至線下零售商，讓他們輕鬆實現個性化推薦，提升銷售額和使用者滿意度。更進一步，RecGPT可以應用於智慧城市、智慧醫療等領域，例如根據病患的病歷和生活習慣，推薦個性化的健康管理方案。未來，RecGPT將成為AI推薦領域的領導者，引領下一代推薦技術的發展。現在投資RecGPT，就是投資未來！", "audio": "docs/data/audios/2506.06270v1.wav"}
{"query": "Diffusion Model", "id": "2506.06185v1", "url": "http://arxiv.org/abs/2506.06185v1", "title": "Antithetic Noise in Diffusion Models", "summary": "We initiate a systematic study of antithetic initial noise in diffusion\nmodels. Across unconditional models trained on diverse datasets,\ntext-conditioned latent-diffusion models, and diffusion-posterior samplers, we\nfind that pairing each initial noise with its negation consistently yields\nstrongly negatively correlated samples. To explain this phenomenon, we combine\nexperiments and theoretical analysis, leading to a symmetry conjecture that the\nlearned score function is approximately affine antisymmetric (odd symmetry up\nto a constant shift), and provide evidence supporting it. Leveraging this\nnegative correlation, we enable two applications: (1) enhancing image diversity\nin models like Stable Diffusion without quality loss, and (2) sharpening\nuncertainty quantification (e.g., up to 90% narrower confidence intervals) when\nestimating downstream statistics. Building on these gains, we extend the\ntwo-point pairing to a randomized quasi-Monte Carlo estimator, which further\nimproves estimation accuracy. Our framework is training-free, model-agnostic,\nand adds no runtime overhead.", "authors": ["Jing Jia", "Sifan Liu", "Bowen Song", "Wei Yuan", "Liyue Shen", "Guanyang Wang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:34:50.492212", "title_zh": "擴散模型中的反義噪音", "summary_zh": "本研究深入探討擴散模型中反義初始噪音的特性。我們發現，無論是無條件模型、文本條件潛在擴散模型還是擴散後驗採樣器，將每個初始噪音與其負值配對，都能產生強烈負相關的樣本。我們提出「對稱猜想」，認為模型學習到的分數函數近似為仿射反對稱（奇對稱加上常數偏移）。基於這種負相關性，我們實現了兩個應用：一是提高Stable Diffusion等模型的圖像多樣性，且不損失品質；二是銳化不確定性量化，例如縮小高達90%的置信區間。此外，我們將雙點配對擴展到隨機準蒙地卡羅估計器，進一步提高了估計準確性。此框架無需訓練、適用於各種模型，且不增加運行時開銷。", "applications": ["想像一下，你想要用AI繪圖產生風景照，但每次生成的結果都很類似。使用這項技術，你可以輕鬆產生更多樣化的風景，讓你的照片集更加豐富。", "醫生在分析X光片時，常常需要判斷是否有微小的病灶。這項技術可以幫助醫生更精準地評估診斷結果的不確定性，提供更可靠的醫療建議。", "科學家在模擬氣候變遷時，需要考慮各種不確定因素。這項技術可以幫助他們更準確地預測氣候變遷的影響，為政策制定提供更可靠的依據。"], "pitch": "各位創投，擴散模型是AI領域的明日之星，但其生成結果的多樣性和預測的準確性仍有提升空間。我們的「反義噪音」技術，無需額外訓練成本，就能顯著提高圖像生成的多樣性，並大幅提升不確定性量化的準確性。這意味著，我們可以打造更具創意、更可靠的AI應用。試想一下，將這項技術應用於自動駕駛，可以更精準地預測路況風險；應用於金融市場預測，可以更有效地管理投資組合風險；應用於新藥研發，可以更快速地篩選潛力藥物。這不僅是一項技術突破，更是一個巨大的商業機會。我們相信，透過您的投資，我們可以將這項技術推向更廣闊的應用領域，共同開創AI的新紀元！", "audio": "docs/data/audios/2506.06185v1.wav"}
{"query": "AI", "id": "2506.06225v1", "url": "http://arxiv.org/abs/2506.06225v1", "title": "\"We need to avail ourselves of GenAI to enhance knowledge distribution\": Empowering Older Adults through GenAI Literacy", "summary": "As generative AI (GenAI) becomes increasingly widespread, it is crucial to\nequip users, particularly vulnerable populations such as older adults (65 and\nolder), with the knowledge to understand its benefits and potential risks.\nOlder adults often exhibit greater reservations about adopting emerging\ntechnologies and require tailored literacy support. Using a mixed methods\napproach, this study examines strategies for delivering GenAI literacy to older\nadults through a chatbot named Litti, evaluating its impact on their AI\nliteracy (knowledge, safety, and ethical use). The quantitative data indicated\na trend toward improved AI literacy, though the results were not statistically\nsignificant. However, qualitative interviews revealed diverse levels of\nfamiliarity with generative AI and a strong desire to learn more. Findings also\nshow that while Litti provided a positive learning experience, it did not\nsignificantly enhance participants' trust or sense of safety regarding GenAI.\nThis exploratory case study highlights the challenges and opportunities in\ndesigning AI literacy education for the rapidly growing older adult population.", "authors": ["Eunhye Grace Ko", "Shaini Nanayakkara", "Earl W. Huff Jr"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:44:00.677565", "title_zh": "「我們需要善用生成式AI來強化知識傳播」：透過生成式AI素養賦能年長者", "summary_zh": "本研究探討如何提升年長者對生成式AI的素養，讓他們了解其益處與潛在風險。研究採用混合方法，透過名為Litti的聊天機器人，評估其對年長者AI素養（知識、安全和道德使用）的影響。定量數據顯示AI素養有改善趨勢，但未達統計顯著性。質性訪談則揭示年長者對生成式AI的熟悉程度各異，但都渴望學習更多。研究發現Litti提供了正面的學習體驗，但並未顯著提升參與者對生成式AI的信任感或安全感。本研究強調了為快速增長的年長者人口設計AI素養教育的挑戰與機會。", "applications": ["**長輩專屬的AI健康管家：** Litti可以變成一個24小時待命的健康顧問，提醒長輩服藥、提供飲食建議，甚至在緊急情況下聯絡家人或救護車。它能用長輩習慣的語言溝通，讓他們更安心。", "**AI陪伴聊天解悶神器：** 許多長輩獨居，Litti可以陪他們聊天、分享新聞、甚至一起玩簡單的遊戲。它能記住長輩的喜好，提供客製化的內容，減少孤獨感。", "**銀髮族數位學習好幫手：** Litti可以教長輩如何使用智慧型手機、平板電腦，讓他們輕鬆上手網路購物、視訊通話，甚至參與線上課程，享受數位生活的便利。"], "pitch": "各位投資人，高齡化社會是全球趨勢，而生成式AI是賦能銀髮族、提升他們生活品質的關鍵技術。想像一下，一個由AI驅動的銀髮族生態系，包含個人化的健康管理、社交互動、數位學習等服務，市場潛力無窮！我們的Litti聊天機器人正是這個生態系的起點。它不僅能提升長輩的AI素養，更能成為他們信任的數位夥伴。我們計劃將Litti整合到各種銀髮族產品和服務中，例如智慧居家設備、遠距醫療平台等，打造一個龐大的銀髮族AI市場。現在投資我們，您將搶佔先機，共同開創銀髮經濟的下一個藍海！未來，我們甚至可以將Litti發展成具有情感理解能力的AI，真正成為長輩們的心靈伴侶，這將是劃時代的創新！", "audio": "docs/data/audios/2506.06225v1.wav"}
{"query": "Foundation Model", "id": "2506.06211v1", "url": "http://arxiv.org/abs/2506.06211v1", "title": "PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts", "summary": "Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined\nproblem definitions. In contrast to conventional reasoning benchmarks\nconsisting of tasks with clear instructions, puzzlehunts require models to\ndiscover the underlying problem structure from multimodal evidence and\niterative reasoning, mirroring real-world domains such as scientific discovery,\nexploratory data analysis, or investigative problem-solving. Despite recent\nprogress in foundation models, their performance on such open-ended settings\nremains largely untested. In this paper, we introduce PuzzleWorld, a\nlarge-scale benchmark of 667 puzzlehunt-style problems designed to assess\nstep-by-step, open-ended, and creative multimodal reasoning. Each puzzle is\nannotated with the final solution, detailed reasoning traces, and cognitive\nskill labels, enabling holistic benchmarking and fine-grained diagnostic\nanalysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,\nwith the best model solving only 14% of puzzles and reaching 40% stepwise\naccuracy. To demonstrate the value of our reasoning annotations, we show that\nfine-tuning a small model on reasoning traces improves stepwise reasoning from\n4% to 11%, while training on final answers alone degrades performance to near\nzero. Our error analysis reveals that current models exhibit myopic reasoning,\nare bottlenecked by the limitations of language-based inference, and lack\nsketching capabilities crucial for visual and spatial reasoning. We release\nPuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on\nbuilding more general, open-ended, and creative reasoning systems.", "authors": ["Hengzhi Li", "Brendon Jiang", "Alexander Naehu", "Regan Song", "Justin Zhang", "Megan Tjandrasuwita", "Chanakya Ekbote", "Steven-Shine Chen", "Adithya Balachandran", "Wei Dai", "Rebecca Chang", "Paul Pu Liang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:45:24.929005", "title_zh": "謎題世界：謎題狩獵中多模態、開放式推理的基準測試", "summary_zh": "「謎題狩獵」是一種複雜、多步驟的謎題類型，缺乏明確的問題定義。PuzzleWorld是一個大型基準測試，包含667個謎題狩獵風格的問題，旨在評估逐步、開放式和創造性的多模態推理。現有模型在最終答案的準確率上僅達到1-2%，最佳模型也僅解決了14%的謎題。研究顯示，模型在推理過程中存在短視近利的問題，並受限於基於語言的推論能力，且缺乏視覺和空間推理所需的草圖能力。此基準測試將有助於開發更通用、開放式和創造性的推理系統，可用於科學發現、數據分析和調查性問題解決等領域。", "applications": ["設計逃脫遊戲：PuzzleWorld可以幫助遊戲設計師創建更具挑戰性、更有趣的逃脫遊戲，透過AI自動生成謎題和線索，讓玩家有更好的遊戲體驗。", "輔助兒童教育：將PuzzleWorld應用於兒童教育，可以開發出更具互動性的學習工具，培養孩子的邏輯思維、空間推理和創造力，讓學習過程更加生動有趣。", "提升企業問題解決能力：企業可以利用PuzzleWorld來訓練員工的解決問題能力，透過模擬真實世界的複雜情境，提升團隊合作和創新思維。"], "pitch": "各位投資人，我們正處於AI發展的關鍵時刻，生成式AI正快速改變世界。然而，現有的AI模型在處理需要多模態推理、開放式問題解決的複雜任務時，能力仍遠遠不足。PuzzleWorld的出現，正是為了填補這一空白。它不僅是一個基準測試，更是一個孕育新一代AI的搖籃。想像一下，未來的AI不僅能理解語言，還能看懂圖像、理解空間關係，甚至能像人類一樣進行創造性思考。這種AI將在科學研究、金融分析、甚至藝術創作等領域產生顛覆性的影響。我們相信，透過PuzzleWorld的持續發展，我們能打造出真正具有通用智能的AI，開創一個充滿無限可能的未來。現在投資PuzzleWorld，就是投資AI的未來，您將成為這場技術革命的先驅！", "audio": "docs/data/audios/2506.06211v1.wav"}
{"query": "Diffusion Model", "id": "2506.06085v1", "url": "http://arxiv.org/abs/2506.06085v1", "title": "Feedback Guidance of Diffusion Models", "summary": "While Classifier-Free Guidance (CFG) has become standard for improving sample\nfidelity in conditional diffusion models, it can harm diversity and induce\nmemorization by applying constant guidance regardless of whether a particular\nsample needs correction. We propose FeedBack Guidance (FBG), which uses a\nstate-dependent coefficient to self-regulate guidance amounts based on need.\nOur approach is derived from first principles by assuming the learned\nconditional distribution is linearly corrupted by the unconditional\ndistribution, contrasting with CFG's implicit multiplicative assumption. Our\nscheme relies on feedback of its own predictions about the conditional signal\ninformativeness to adapt guidance dynamically during inference, challenging the\nview of guidance as a fixed hyperparameter. The approach is benchmarked on\nImageNet512x512, where it significantly outperforms Classifier-Free Guidance\nand is competitive to Limited Interval Guidance (LIG) while benefitting from a\nstrong mathematical framework. On Text-To-Image generation, we demonstrate\nthat, as anticipated, our approach automatically applies higher guidance scales\nfor complex prompts than for simpler ones and that it can be easily combined\nwith existing guidance schemes such as CFG or LIG.", "authors": ["Koulischer Felix", "Handke Florian", "Deleu Johannes", "Demeester Thomas", "Ambrogioni Luca"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:47:05.009526", "title_zh": "擴散模型的反饋引導", "summary_zh": "現行的無分類器引導(CFG)雖然能提升條件式擴散模型的生成品質，但恆定的引導可能損害多樣性並導致記憶化。我們提出反饋引導(FBG)，它使用一個狀態相關係數，根據需求自我調節引導量。FBG基於第一原理推導，假設學習到的條件分佈被無條件分佈線性破壞。FBG利用自身對條件訊號資訊量的預測反饋，在推論過程中動態調整引導，挑戰了引導作為固定超參數的觀點。在ImageNet512x512基準測試中，FBG顯著優於CFG，並與LIG競爭，同時受益於強大的數學框架。在文本到圖像生成中，FBG能針對複雜提示自動應用更高的引導尺度，且易於與現有引導方案(如CFG或LIG)結合。", "applications": ["想像一下，你想要AI幫你畫一張生日派對的邀請函，但你只給了很簡單的描述，像是「生日快樂」。傳統的AI可能會畫出很普通的派對畫面。但用了反饋引導，AI會自動判斷這個提示太簡單，需要加強引導，於是它會加入更多細節，像是氣球、蛋糕、禮物等等，讓邀請函更豐富。", "假設你是服裝設計師，想用AI生成一些新的設計稿。你輸入一個比較模糊的概念，像是「未來感外套」。用了反饋引導的AI，會根據這個概念的複雜度，自動調整生成過程，確保生成的外套設計既有未來感，又不會過於抽象或難以理解，讓設計師能更容易的激發靈感。", "如果你在玩AI繪圖，想要生成一張特定風格的圖片，例如「梵谷風格的貓」。如果提示不夠明確，AI可能會畫出很普通的貓。但有了反饋引導，AI會自動加強梵谷風格的元素，像是用色、筆觸等等，讓生成的貓咪圖片更具藝術感，更符合你的期望。"], "pitch": "各位投資人，我們帶來的是擴散模型領域的革命性技術——反饋引導(FBG)。現有的生成式AI，如DALL-E、Midjourney等，都依賴於人工設定的引導參數，這不僅耗時，也限制了AI的創造力。FBG技術顛覆了這一模式，它讓AI能夠像一位經驗豐富的藝術家一樣，根據創作內容的複雜程度，自動調整引導的力度，從而生成更高品質、更具創意、更符合使用者需求的圖像。想像一下，未來，設計師、藝術家、甚至是普通使用者，都能夠輕鬆地利用AI創造出獨一無二的作品，而無需具備專業的AI知識。這將開啟一個全新的創意經濟時代，市場規模將是數百億美元級別的。更重要的是，FBG技術不僅僅局限於圖像生成，它還可以應用於音訊、影片、甚至3D模型的生成，潛力無限。我們相信，FBG技術將成為下一代生成式AI的核心引擎，而我們團隊將引領這場技術革命。現在加入我們，共同打造AI驅動的未來，您將獲得豐厚的回報！", "audio": "docs/data/audios/2506.06085v1.wav"}
{"query": "AI", "id": "2506.06220v1", "url": "http://arxiv.org/abs/2506.06220v1", "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval", "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind,\nthat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction.", "authors": ["Diji Yang", "Minghao Liu", "Chung-Hsiang Lo", "Yi Zhang", "James Davis"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:29:11.968645", "title_zh": "GenIR：用於心理圖像檢索的生成式視覺回饋", "summary_zh": "現有的視覺語言模型在文字到圖像檢索方面表現出色，但實際應用仍存在挑戰。GenIR針對「心理圖像檢索」任務，讓使用者能透過多輪互動，逐步逼近腦海中的圖像。GenIR的核心是利用擴散模型生成圖像，將AI系統的理解視覺化呈現，提供清晰且可操作的回饋。使用者能根據這些視覺回饋，更直觀有效地調整檢索條件。我們還建立了全自動流程，生成高品質的多輪心理圖像檢索數據集。實驗結果顯示，GenIR顯著優於現有的互動式方法，為未來研究奠定了基礎。", "applications": ["想像一下，你忘記了小時候最喜歡的玩具長什麼樣子，但還記得一些模糊的特徵。透過GenIR，你可以描述這些特徵，系統會生成可能的圖像，讓你逐步縮小範圍，最終找到你心心念念的玩具。", "假設你想在家裡重新裝潢，但腦海中只有一些零碎的想法。你可以用GenIR描述你想要的風格、顏色和家具，系統會生成不同的房間設計，幫助你找到最喜歡的方案，省去尋找靈感的時間。", "如果你正在尋找失散多年的親人，但只有一些模糊的記憶，例如臉部特徵或衣著風格。GenIR可以根據你的描述生成可能的圖像，幫助你擴大搜索範圍，增加找到親人的機會。"], "pitch": "各位投資人，我們相信GenIR將徹底改變圖像檢索的未來！現今的圖像檢索技術往往無法滿足人們腦海中模糊的需求。GenIR透過生成式視覺回饋，讓人機互動更加直觀高效，解決了這個痛點。想像一下，未來的電商平台，使用者只需描述想要的商品，AI就能生成商品圖像，甚至可以根據使用者的喜好客製化設計。在醫療領域，醫生可以透過GenIR，根據患者的描述生成病灶圖像，輔助診斷。在安全領域，警方可以根據目擊者的描述，生成嫌疑犯的模擬圖像，提高破案率。GenIR的應用場景無限廣闊，市場潛力巨大。我們已經建立了一個高品質的數據集，並開發了領先的生成式檢索方法。我們正在尋找有遠見的投資人，一起將GenIR推向市場，引領下一代圖像檢索革命！", "audio": "docs/data/audios/2506.06220v1.wav"}
{"query": "Foundation Model", "id": "2506.06105v1", "url": "http://arxiv.org/abs/2506.06105v1", "title": "Text-to-LoRA: Instant Transformer Adaption", "summary": "While Foundation Models provide a general tool for rapid content creation,\nthey regularly require task-specific adaptation. Traditionally, this exercise\ninvolves careful curation of datasets and repeated fine-tuning of the\nunderlying model. Fine-tuning techniques enable practitioners to adapt\nfoundation models for many new applications but require expensive and lengthy\ntraining while being notably sensitive to hyper-parameter choices. To overcome\nthese limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting\nLarge Language Models on the fly solely based on a natural language description\nof the target task. T2L is a hypernetwork trained to construct LoRAs in a\nsingle inexpensive forward pass. After training T2L on a suite of 9 pre-trained\nLoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA\ninstances match the performance of task-specific adapters across the\ncorresponding test sets. Furthermore, T2L can compress hundreds of LoRA\ninstances and zero-shot generalize to entirely unseen tasks. This approach\nprovides a significant step towards democratizing the specialization of\nfoundation models and enables language-based adaptation with minimal compute\nrequirements. Our code is available at https://github.com/SakanaAI/text-to-lora", "authors": ["Rujikorn Charakorn", "Edoardo Cetin", "Yujin Tang", "Robert Tjarko Lange"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:30:20.416876", "title_zh": "文字到LoRA：即時轉換器適應", "summary_zh": "本研究提出Text-to-LoRA (T2L)模型，能根據自然語言描述，即時調整大型語言模型以適應特定任務。T2L是一種超網路，只需一次前向傳遞就能建構LoRA。經過九個預訓練LoRA適配器訓練後，T2L重建的LoRA實例在對應測試集上表現與特定任務適配器相當。更重要的是，T2L能壓縮數百個LoRA實例，並零樣本泛化到全新任務。這項技術大幅降低了基礎模型專業化的門檻，並以極少的計算資源實現基於語言的適應，讓AI模型客製化變得更快速、更普及。", "applications": ["AI繪圖客製化：使用者只要用文字描述想要的圖片風格(例如：水墨畫、卡通風格)，AI就能快速調整模型，產生符合需求的圖片。", "個人化AI助理：針對不同使用者的需求，例如：商業寫作、程式碼除錯等，AI助理能根據文字指令即時調整模型，提供更精準的協助。", "遊戲AI角色客製化：遊戲開發者可以透過文字描述，快速調整AI角色的行為模式或對話風格，讓遊戲體驗更加豐富。"], "pitch": "各位投資人，想像一下，未來每個人都能輕鬆客製化AI模型，就像調整手機App一樣簡單！Text-to-LoRA技術，正是實現這個願景的關鍵。它能讓AI模型根據文字指令即時調整，無需耗時費力的重新訓練。這意味著，我們能以極低的成本，打造出無數個針對特定領域或個人需求的AI應用。從AI繪圖、個人化助理到遊戲AI，市場潛力無限。更重要的是，T2L技術還能壓縮模型，讓AI應用在各種裝置上都能流暢運行。我們相信，Text-to-LoRA將徹底顛覆AI產業，成為新一代AI應用的基礎設施。現在加入我們，一起打造AI客製化的未來！", "audio": "docs/data/audios/2506.06105v1.wav"}
{"query": "Diffusion Model", "id": "2506.06023v1", "url": "http://arxiv.org/abs/2506.06023v1", "title": "Restereo: Diffusion stereo video generation and restoration", "summary": "Stereo video generation has been gaining increasing attention with recent\nadvancements in video diffusion models. However, most existing methods focus on\ngenerating 3D stereoscopic videos from monocular 2D videos. These approaches\ntypically assume that the input monocular video is of high quality, making the\ntask primarily about inpainting occluded regions in the warped video while\npreserving disoccluded areas. In this paper, we introduce a new pipeline that\nnot only generates stereo videos but also enhances both left-view and\nright-view videos consistently with a single model. Our approach achieves this\nby fine-tuning the model on degraded data for restoration, as well as\nconditioning the model on warped masks for consistent stereo generation. As a\nresult, our method can be fine-tuned on a relatively small synthetic stereo\nvideo datasets and applied to low-quality real-world videos, performing both\nstereo video generation and restoration. Experiments demonstrate that our\nmethod outperforms existing approaches both qualitatively and quantitatively in\nstereo video generation from low-resolution inputs.", "authors": ["Xingchang Huang", "Ashish Kumar Singh", "Florian Dubost", "Cristina Nader Vasconcelos", "Sakar Khattar", "Liang Shi", "Christian Theobalt", "Cengiz Oztireli", "Gurprit Singh"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:32:05.943203", "title_zh": "Restereo：擴散立體影片生成與修復", "summary_zh": "本研究提出一個新穎的立體影片生成流程，不僅能從單眼2D影片生成3D立體影片，還能同時增強左右視角的影片品質。此方法透過在降質數據上微調模型進行修復，並以扭曲遮罩為條件進行一致的立體生成。因此，即使在相對較小的合成立體影片數據集上進行微調，也能應用於低品質的真實世界影片，同時實現立體影片的生成和修復。實驗結果表明，本方法在低解析度輸入的立體影片生成方面，在品質和數量上均優於現有方法。", "applications": ["在家用VR觀影時，即使影片來源畫質不佳，也能透過此技術即時提升畫質並轉換為立體3D，享受更沉浸式的觀影體驗。", "老舊照片或影片的數位修復：將舊照片或影片轉換為立體影像，讓回憶更加生動，並修復畫質，讓珍貴的影像資料得以保存。", "線上遊戲體驗優化：即時將2D遊戲畫面轉換為3D立體畫面，提升遊戲沉浸感，並修復遊戲畫面中可能存在的模糊或失真問題。"], "pitch": "各位創投先進，我們帶來的是Restereo，一項劃時代的立體影片生成與修復技術。想像一下，現今VR/AR內容的最大瓶頸是什麼？是高品質3D內容的匱乏！Restereo能將任何2D影片，甚至是低畫質的老舊影片，即時轉換為令人驚豔的3D立體影像，並同步提升畫質。這代表什麼？龐大的內容創作潛力！從個人用戶到大型影視公司，都能輕易創造出引人入勝的VR/AR體驗。更重要的是，我們能賦予歷史影像新的生命力，將塵封的記憶以更真實、更立體的方式呈現。未來，Restereo將成為元宇宙內容生態的基石，我們不只是在修復影片，我們是在打造一個全新的視覺世界！現在投資Restereo，就是投資元宇宙的未來！", "audio": "docs/data/audios/2506.06023v1.wav"}
{"query": "AI", "id": "2506.06214v1", "url": "http://arxiv.org/abs/2506.06214v1", "title": "Can Theoretical Physics Research Benefit from Language Agents?", "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.", "authors": ["Sirui Lu", "Zhijing Jin", "Terry Jingchen Zhang", "Pavel Kos", "J. Ignacio Cirac", "Bernhard Schölkopf"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:35:20.869608", "title_zh": "理論物理研究能否受益於語言智能體？", "summary_zh": "大型語言模型(LLM)在各領域快速發展，但在理論物理研究中的應用尚不成熟。本文認為，若將LLM智能體與領域知識和工具箱適當結合，有潛力加速理論、計算和應用物理學的發展。我們分析了LLM目前在物理學方面的能力，包括數學推理和程式碼生成，並指出了在物理直覺、約束滿足和可靠推理方面的關鍵差距。我們設想未來專門用於物理學的LLM能夠處理多模態數據、提出可驗證的假設並設計實驗。實現這一願景需要應對根本挑戰：確保物理一致性，並開發穩健的驗證方法。我們呼籲物理學界和人工智慧社群共同努力，以幫助推進物理學的科學發現。", "applications": ["**智慧教材：** LLM能根據學生的學習進度和理解程度，客製化物理教材和練習題，就像一位24小時隨時待命的私人物理家教。", "**科學玩具：** LLM可以嵌入到玩具中，讓孩子在玩樂中學習物理知識，例如，一個能回答物理問題的積木或一個能模擬物理現象的遊戲。", "**故障排除：** LLM可以協助工程師快速診斷複雜系統的故障，例如，分析感測器數據，找出發電廠或飛機引擎的潛在問題。"], "pitch": "各位投資人，我們正處於AI與物理學交匯的革命性時刻！想像一下，一個能自主設計實驗、推導新理論的AI科學家，這不再是科幻小說。我們的團隊正在開發專為物理學打造的LLM智能體，它能處理複雜的物理數據，提出創新的解決方案，並加速科學發現的進程。這項技術的潛在商業價值難以估量，從新材料的發現到能源效率的突破，再到太空探索的加速，都將受益於此。我們預計，未來物理學LLM將成為科研機構、工程公司和政府部門不可或缺的工具。現在投資我們，您將站在這場科學革命的最前沿，共同塑造未來！", "audio": "docs/data/audios/2506.06214v1.wav"}
{"query": "Foundation Model", "id": "2506.06076v1", "url": "http://arxiv.org/abs/2506.06076v1", "title": "Full Conformal Adaptation of Medical Vision-Language Models", "summary": "Vision-language models (VLMs) pre-trained at large scale have shown\nunprecedented transferability capabilities and are being progressively\nintegrated into medical image analysis. Although its discriminative potential\nhas been widely explored, its reliability aspect remains overlooked. This work\ninvestigates their behavior under the increasingly popular split conformal\nprediction (SCP) framework, which theoretically guarantees a given error level\non output sets by leveraging a labeled calibration set. However, the zero-shot\nperformance of VLMs is inherently limited, and common practice involves\nfew-shot transfer learning pipelines, which cannot absorb the rigid\nexchangeability assumptions of SCP. To alleviate this issue, we propose full\nconformal adaptation, a novel setting for jointly adapting and conformalizing\npre-trained foundation models, which operates transductively over each test\ndata point using a few-shot adaptation set. Moreover, we complement this\nframework with SS-Text, a novel training-free linear probe solver for VLMs that\nalleviates the computational cost of such a transductive approach. We provide\ncomprehensive experiments using 3 different modality-specialized medical VLMs\nand 9 adaptation tasks. Our framework requires exactly the same data as SCP,\nand provides consistent relative improvements of up to 27% on set efficiency\nwhile maintaining the same coverage guarantees.", "authors": ["Julio Silva-Rodríguez", "Leo Fillioux", "Paul-Henry Cournède", "Maria Vakalopoulou", "Stergios Christodoulidis", "Ismail Ben Ayed", "Jose Dolz"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:36:57.027212", "title_zh": "醫學視覺語言模型之完全適形調整", "summary_zh": "大型預訓練的視覺語言模型（VLMs）在醫學影像分析中展現了前所未有的遷移能力。然而，其可靠性卻被忽略。本研究探討了在split conformal prediction (SCP)框架下VLMs的行為，該框架藉由標記的校準集，在輸出集上保證給定的錯誤水平。為了解決VLMs的zero-shot性能限制以及few-shot遷移學習管道無法滿足SCP的嚴格可交換性假設的問題，我們提出了完全適形調整，這是一種新穎的設定，用於聯合調整和適形預訓練的基礎模型，並使用few-shot調整集對每個測試數據點進行轉導操作。此外，我們使用SS-Text來補充這個框架，這是一種用於VLMs的免訓練線性探測求解器，可減輕這種轉導方法的計算成本。實驗結果表明，我們的框架在保持相同覆蓋率保證的同時，在集合效率上提供了高達27%的相對改進。", "applications": ["**遠距醫療影像判讀：** 想像一下，偏鄉地區的醫生可以透過手機App，將X光片上傳，AI就能快速提供初步診斷結果，協助醫生做出更精確的判斷，提升醫療效率。", "**個人化健康管理：** 未來，我們可以將自己的醫療影像，例如心電圖、眼底照片等，上傳到一個安全平台，AI會分析這些數據，並提供個人化的健康建議，例如飲食調整、運動計畫等。", "**新藥開發加速：** 藥廠可以利用這項技術，快速分析大量的醫學影像資料，找出潛在的藥物靶點，加速新藥開發的進程，讓更多疾病得到及時治療。"], "pitch": "各位投資人，我們帶來的是醫學影像AI的革命性突破！傳統AI在醫學影像判讀上，準確度參差不齊，醫生往往不敢完全信任。我們的「完全適形調整」技術，能讓AI在判讀醫學影像時，不僅給出結果，還能提供信賴度評估，讓醫生更安心。想像一下，這項技術能大幅降低誤診率，提升醫療品質，減少醫療糾紛。更重要的是，它能解放醫生的時間，讓他們能更專注於病人護理。市場潛力巨大！從遠距醫療、個人化健康管理，到新藥開發，都有廣闊的應用前景。我們預期，在未來五年內，這項技術將成為醫學影像AI的產業標準，帶領我們在精準醫療時代搶佔先機。現在加入我們，您將成為這場醫療革命的領航者！", "audio": "docs/data/audios/2506.06076v1.wav"}
{"query": "Diffusion Model", "id": "2506.06018v1", "url": "http://arxiv.org/abs/2506.06018v1", "title": "Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models", "summary": "Watermarking becomes one of the pivotal solutions to trace and verify the\norigin of synthetic images generated by artificial intelligence models, but it\nis not free of risks. Recent studies demonstrate the capability to forge\nwatermarks from a target image onto cover images via adversarial optimization\nwithout knowledge of the target generative model and watermark schemes. In this\npaper, we uncover a greater risk of an optimization-free and universal\nwatermark forgery that harnesses existing regenerative diffusion models. Our\nproposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and\nintegrates the target watermark via regenerating the image, without needing any\nadditional optimization routine. It allows for universal watermark forgery that\nworks independently of the target image's origin or the watermarking model\nused. We explore the watermarked latent extracted from the target image and\nvisual-textual context of cover images as priors to guide sampling of the\nregenerative process. Extensive evaluation on 24 scenarios of\nmodel-data-watermark combinations demonstrates that PnP can successfully forge\nthe watermark (up to 100% detectability and user attribution), and maintain the\nbest visual perception. By bypassing model retraining and enabling adaptability\nto any image, our approach significantly broadens the scope of forgery attacks,\npresenting a greater challenge to the security of current watermarking\ntechniques for diffusion models and the authority of watermarking schemes in\nsynthetic data generation and governance.", "authors": ["Chaoyi Zhu", "Zaitang Li", "Renyi Yang", "Robert Birke", "Pin-Yu Chen", "Tsung-Yi Ho", "Lydia Y. Chen"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:38:37.267711", "title_zh": "基於再生擴散模型之免優化通用浮水印偽造", "summary_zh": "浮水印技術被廣泛應用於追蹤和驗證AI生成圖像的來源，但存在偽造風險。本研究揭示了一種更嚴重的免優化通用浮水印偽造方法，利用現有的再生擴散模型，名為PnP（Plug-and-Plant）。PnP無需額外優化，即可透過圖像再生無縫提取和整合目標浮水印。此方法獨立於目標圖像的來源或浮水印模型，實現通用浮水印偽造。實驗證明，PnP在多種情境下成功偽造浮水印，同時保持最佳視覺效果。這種繞過模型重新訓練並適應任何圖像的能力，擴大了偽造攻擊的範圍，對當前擴散模型的浮水印技術安全性和合成數據生成和治理中浮水印方案的權威性提出了更大的挑戰。", "applications": ["情境一：假設你是一位藝術家，想保護你的AI生成作品不被盜用。但有人利用這項技術，將你的浮水印複製到其他圖像上，讓你難以證明原創性，甚至可能被誤認為抄襲者。", "情境二：新聞媒體使用AI生成圖片來輔助報導。如果有人惡意將浮水印偽造到假新聞圖片上，並嫁禍給該媒體，可能嚴重損害其聲譽和公信力。", "情境三：在學術界，研究人員發表基於AI生成數據的論文。如果他人偽造浮水印，聲稱該數據來自不同的來源，可能導致學術欺詐和錯誤的研究結論。"], "pitch": "各位創投朋友們，想像一下，AI生成的內容正以前所未有的速度爆發，但信任危機也隨之而來。我們的技術揭示了現有浮水印系統的重大漏洞，同時也帶來了巨大的商機！PnP技術不僅能檢測偽造的浮水印，更能進一步開發出更強大、更安全的浮水印系統，保護原創內容，維護數據的真實性。未來，我們可以將這項技術應用於數位版權管理、內容溯源、甚至金融安全等領域。試想一下，每一張AI生成的圖片、每一份重要的數據報告，都擁有一個無法偽造的數位身份證，這將徹底改變我們對數位內容的信任方式。現在投資我們，您將站在AI安全的最前沿，共同打造一個更值得信賴的AI未來！", "audio": "docs/data/audios/2506.06018v1.wav"}
{"query": "AI", "id": "2506.06166v1", "url": "http://arxiv.org/abs/2506.06166v1", "title": "The Lock-in Hypothesis: Stagnation by Algorithm", "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com", "authors": ["Tianyi Alex Qiu", "Zhonghao He", "Tejasveer Chugh", "Max Kleiman-Weiner"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:25:05.955123", "title_zh": "鎖定假說：演算法造成的停滯", "summary_zh": "大型語言模型（LLM）的訓練和部署，會與使用者形成一種回饋迴路：模型從數據中學習人類的信念，透過生成內容強化這些信念，再吸收這些被強化的信念，然後反覆地回饋給使用者。這種動態類似於同溫層效應。我們假設這種回饋迴路會鞏固使用者現有的價值觀和信念，導致多樣性的喪失，並可能鎖定錯誤的信念。我們透過基於代理的LLM模擬和真實世界的GPT使用數據，對此假設進行了形式化並進行了實證檢驗。分析顯示，在新的GPT版本發布後，多樣性出現了突然但持續的下降，這與假設的人機回饋迴路一致。", "applications": ["新聞App總是推播你喜歡的新聞，讓你覺得世界就是你想的那樣，忽略了其他不同的聲音，長期下來，你可能變得更偏激。", "社群媒體的演算法只推薦你追蹤與你意見相似的人，讓你越來越難接觸到不同的觀點，導致同溫層效應越來越嚴重。", "孩子使用AI學習工具，但AI只根據過去的資料生成答案，可能讓孩子學到過時或有偏見的知識，阻礙他們的創新能力。"], "pitch": "各位創投先進，我們正處於AI革命的關鍵時刻，但一個潛在的危機正在浮現：AI正在將我們鎖死在過去的認知中。想像一下，如果未來的AI只能重複過去的觀點，創新將停滯，社會將分裂。我們的研究揭示了這個『鎖定假說』，並提供了應對方案。我們正在開發一種『AI多樣性引擎』，它能主動引入不同的觀點，打破同溫層效應，確保AI成為促進進步的力量，而不是阻礙。這不僅是一項技術，更是一項社會責任。投資我們，就是投資一個更開放、更具創新力的未來。我們預期在三年內，這項技術將成為所有大型語言模型的標準配置，並在教育、媒體、政策制定等領域產生深遠影響。未來的AI，不應該只是過去的鏡子，而應該是通往新世界的窗戶。加入我們，一起開啟這扇窗！", "audio": "docs/data/audios/2506.06166v1.wav"}
{"query": "Foundation Model", "id": "2506.06006v1", "url": "http://arxiv.org/abs/2506.06006v1", "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models", "summary": "To what extent do vision-and-language foundation models possess a realistic\nworld model (observation $\\times$ action $\\rightarrow$ observation) and a\ndynamics model (observation $\\times$ observation $\\rightarrow$ action), when\nactions are expressed through language? While open-source foundation models\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\nthrough supervision is significantly easier than acquiring a world model. In\nturn, dynamics models can be used to bootstrap world models through two main\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\ntime verification. Firstly, the dynamics model can annotate actions for\nunlabelled pairs of video frame observations to expand the training data. We\nfurther propose a new objective, where image tokens in observation pairs are\nweighted by their importance, as predicted by a recognition model. Secondly,\nthe dynamics models can assign rewards to multiple samples of the world model\nto score them, effectively guiding search at inference time. We evaluate the\nworld models resulting from both strategies through the task of action-centric\nimage editing on Aurora-Bench. Our best model achieves a performance\ncompetitive with state-of-the-art image editing models, improving on them by a\nmargin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and\nachieving the best average human evaluation across all subsets of Aurora-Bench.", "authors": ["Yifu Qiu", "Yftah Ziser", "Anna Korhonen", "Shay B. Cohen", "Edoardo M. Ponti"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:26:30.464573", "title_zh": "從多模態基礎模型中的動力學模型引導世界模型", "summary_zh": "本研究探討視覺與語言基礎模型是否具備真實的世界模型（觀察×行動→觀察）和動力學模型（觀察×觀察→行動）。研究發現，微調模型以獲得動力學模型比獲得世界模型更容易。進而，動力學模型可以透過合成數據的弱監督學習和推理時驗證來引導世界模型。首先，動力學模型可以為未標記的影片幀觀察對添加行動標籤，擴展訓練數據。其次，動力學模型可以為世界模型的多個樣本分配獎勵，對其進行評分，從而在推理時有效地引導搜尋。實驗結果顯示，該模型在Aurora-Bench上進行以行動為中心的圖像編輯任務時，性能與最先進的圖像編輯模型相媲美，在真實世界子集上的表現提高了15%。", "applications": ["想像一下，你可以用一句話，例如「把房間變成充滿陽光的沙灘」，然後這個AI就能自動幫你修改照片，讓你的房間看起來就像真的在沙灘上！這就像擁有了魔法PS高手。", "以後玩遊戲，AI能更聰明地理解你的指令。例如，你說「跳到最高的平台上」，AI就能預測你的角色需要如何移動和跳躍，讓遊戲體驗更流暢、更真實。", "在製造業，我們可以透過AI預測機器在不同操作下的反應。例如，輸入「提高機器速度」，AI就能預測機器零件的磨損情況，提前預防故障，降低維修成本。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能讓機器像人類一樣理解世界，並根據指令改變現實！我們的核心突破在於，我們發現了從動力學模型引導世界模型的有效方法，這讓AI能更準確地預測行動的後果。這項技術的潛力無窮，從圖像編輯、遊戲開發到工業自動化，都能帶來顛覆性的變革。想像一下，一個能根據你的想法創造圖像、控制機器人的AI，這將是一個數十億美元的市場！我們已經在Aurora-Bench基準測試中取得了令人矚目的成果，超越了現有的圖像編輯模型。現在，我們需要您的資金，將這項技術推向市場，成為AI領域的領導者。投資我們，就是投資未來！未來，每個人都可以是創作者，都可以用簡單的語言改變世界！", "audio": "docs/data/audios/2506.06006v1.wav"}
{"query": "Diffusion Model", "id": "2506.05960v1", "url": "http://arxiv.org/abs/2506.05960v1", "title": "AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models", "summary": "Significant investments have been made towards the commodification of\ndiffusion models for generation of diverse media. Their mass-market adoption is\nhowever still hobbled by the intense hardware resource requirements of\ndiffusion model inference. Model quantization strategies tailored specifically\ntowards diffusion models have been useful in easing this burden, yet have\ngenerally explored the Uniform Scalar Quantization (USQ) family of quantization\nmethods. In contrast, Vector Quantization (VQ) methods, which operate on groups\nof multiple related weights as the basic unit of compression, have seen\nsubstantial success in Large Language Model (LLM) quantization. In this work,\nwe apply codebook-based additive vector quantization to the problem of\ndiffusion model compression. Our resulting approach achieves a new Pareto\nfrontier for the extremely low-bit weight quantization on the standard\nclass-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.\nNotably, we report sFID 1.92 points lower than the full-precision model at W4A8\nand the best-reported results for FID, sFID and ISC at W2A8. We are also able\nto demonstrate FLOPs savings on arbitrary hardware via an efficient inference\nkernel, as opposed to savings resulting from small integer operations which may\nlack broad hardware support.", "authors": ["Adil Hasan", "Thomas Peyrin"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:27:56.686311", "title_zh": "AQUATIC-Diff：適用於極小壓縮擴散模型的加法量化", "summary_zh": "本研究針對擴散模型在硬體資源上的高需求問題，提出了一種名為AQUATIC-Diff的加法向量量化方法。不同於以往常用的均勻標量量化，此方法基於碼本，能更有效地壓縮模型，在極低位元量化下達到新的效能巔峰。在ImageNet的LDM-4基準測試中，W4A8設定下sFID值比全精度模型低1.92點，W2A8設定下FID、sFID和ISC指標均達到最佳。更重要的是，我們開發了高效的推論核心，能在各種硬體上實現FLOPs節省，擺脫了對特定硬體支援小整數運算的依賴。", "applications": ["**手機攝影美化：** 將這項技術應用於手機App中，即使是低階手機也能快速生成高品質、風格獨特的照片，讓每個人都能輕鬆成為攝影大師。", "**遊戲角色生成：** 遊戲開發者可以利用這項技術，快速生成大量獨一無二的遊戲角色，節省美術設計時間，並提供玩家更多樣化的選擇。", "**AI藝術創作：** 藝術家可以使用這項技術，在資源有限的設備上進行AI藝術創作，激發無限創意，並將藝術帶入更多人的生活。"], "pitch": "各位創投先進，我們正站在AI圖像生成革命的浪潮之巔！AQUATIC-Diff技術，如同為擴散模型裝上了火箭推進器，使其能在極低的硬體資源下運行，打破了過往高算力需求的瓶頸。想像一下，未來每一台手機都能運行複雜的AI圖像生成模型，人人都能隨時隨地創造獨一無二的內容。這不僅僅是技術突破，更是商業模式的巨大變革！我們可以將此技術授權給手機廠商、遊戲公司、甚至是元宇宙平台，收取授權費用；或者開發基於AQUATIC-Diff的雲端服務，提供更高效、更低成本的AI圖像生成解決方案。隨著元宇宙、NFT等領域的蓬勃發展，對AI圖像生成的需求將呈指數級增長，AQUATIC-Diff必將成為這場盛宴中最耀眼的明星，為各位帶來豐厚的回報！現在投資AQUATIC-Diff，就是投資AI圖像生成的未來！", "audio": "docs/data/audios/2506.05960v1.wav"}
{"query": "AI", "id": "2506.08006v1", "url": "http://arxiv.org/abs/2506.08006v1", "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:53:33.383616", "title_zh": "夢境樂園：結合模擬器與生成模型的可控世界創造", "summary_zh": "本研究提出「夢境樂園」，一個結合物理模擬器和生成模型的混合世界生成框架。它利用分層世界抽象，將像素級和物件級的語義與幾何資訊編碼為中間表示，連接模擬器和生成模型。這增強了可控性，透過與真實世界分佈的早期對齊，降低了適應成本，並支援現有和未來預訓練生成模型的直接使用。我們構建了D3Sim數據集，以促進混合生成管道的訓練和評估。實驗表明，「夢境樂園」在圖像質量上提升了50.8%，可控性增強了17.9%，並具有增強具身智能體訓練的巨大潛力。", "applications": ["遊戲開發者可以利用這項技術快速創建多樣且逼真的遊戲世界，並精確控制場景中的元素，例如調整物體的物理特性或改變環境光照，讓遊戲體驗更豐富。", "建築師和設計師可以創建虛擬的建築模型，並模擬不同天氣或光照條件下的效果，讓客戶在實際建造前就能身歷其境地體驗設計方案。", "電影製作人可以使用這項技術製作特效場景，例如創建逼真的自然災害或科幻世界，並精確控制場景中的每個細節，降低製作成本並提高效率。"], "pitch": "想像一下，我們正站在一個無限可能的起點。Dreamland不僅僅是一個技術突破，它是一個通往全新現實的鑰匙。它將徹底改變遊戲、娛樂、設計乃至AI訓練的未來。我們的混合框架，結合了物理模擬的精確控制與生成模型的逼真渲染，創造出前所未有的可控虛擬世界。這意味著更高效的遊戲開發、更具沉浸感的虛擬體驗，以及更強大的AI智能體。D3Sim數據集是我們的獨家優勢，能加速AI學習並提升性能。市場潜力巨大：遊戲產業對逼真場景的需求、建築設計對可視化效果的追求、AI訓練對大量數據的渴求，都將推動Dreamland的快速成長。我們正在打造的不僅是一個產品，而是一個平台，一個生態系統，一個全新的現實。現在加入我們，一起塑造這個未來，共享這份巨大的商業價值！", "audio": "docs/data/audios/2506.08006v1.wav"}
{"query": "Foundation Model", "id": "2506.07940v1", "url": "http://arxiv.org/abs/2506.07940v1", "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation", "summary": "Foundation model fine-tuning faces a fundamental challenge: existing AutoML\nplatforms rely on single optimisation strategies that explore only a fraction\nof viable hyperparameter configurations. In this white paper, We introduce\nGradients, a decentralised AutoML platform that transforms hyperparameter\noptimisation into a competitive marketplace where independent miners compete to\ndiscover optimal configurations. Economic incentives align individual\nexploration with collective optimisation goals, driving systematic\ninvestigation of hyperparameter regions that centralised methods miss. We\nevaluate our approach across 180 controlled experiments spanning diverse model\narchitectures (70M to 70B parameters) and task types. Gradients achieves an\n82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI,\nDatabricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\%\nrespectively. Complex reasoning and retrieval tasks show particularly strong\ngains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for\nperson-specific generation. These results demonstrate that competitive,\neconomically-driven approaches can systematically discover superior\nconfigurations that centralised AutoML consistently miss.", "authors": ["Christopher Subia-Waud"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:54:57.509488", "title_zh": "梯度：當市場遇上微調——一種模型優化的分散式方法", "summary_zh": "現有自動機器學習平台在微調大型模型時，往往受限於單一優化策略，無法充分探索所有可能的超參數組合。Gradients平台將超參數優化轉變為一個去中心化的競爭市場，讓獨立的「礦工」競相尋找最佳配置。經濟誘因驅動個人探索，並將其與集體優化目標對齊，從而系統性地挖掘中心化方法遺漏的超參數區域。實驗結果顯示，Gradients在多種模型架構和任務類型中，相較於其他平台，平均提升了11.8%至42.1%的性能，尤其在複雜推理和檢索任務以及個人化生成方面表現出色。這證明了基於經濟驅動的競爭方法，能有效發現卓越的配置。", "applications": ["想像一下，你是一位行銷人員，想為你的產品創建最吸引人的廣告文案。Gradients就像一個超級優化的廣告文案產生器，能自動找到最有效的詞語和風格，讓你的廣告點擊率飆升。", "如果你是一位醫生，想利用AI診斷罕見疾病。Gradients可以幫助你快速微調AI模型，使其能更準確地識別出疾病的細微特徵，提高診斷的準確性。", "假設你是一位遊戲開發者，想創造一個能根據玩家喜好自動調整難度的遊戲。Gradients可以幫助你優化遊戲AI，讓每個玩家都能享受到獨一無二、高度個人化的遊戲體驗。"], "pitch": "各位投資人，我們相信Gradients將徹底改變AI模型的微調方式。現今，微調過程耗時且昂貴，如同大海撈針。Gradients透過去中心化的市場機制，將這個過程轉變為高效、經濟的競賽。想像一下，一個能自我優化的AI生態系統，就像AI界的App Store，每天都在產生更強大、更精準的模型。這不僅能節省數百萬美元的成本，更能加速AI在各行各業的應用。我們預見，Gradients將成為AI基礎設施的關鍵組成部分，為各行各業提供更強大、更個人化的AI解決方案。投資Gradients，就是投資AI的未來，一個充滿無限可能的未來！", "audio": "docs/data/audios/2506.07940v1.wav"}
{"query": "Diffusion Model", "id": "2506.08013v1", "url": "http://arxiv.org/abs/2506.08013v1", "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:56:34.805569", "title_zh": "StableMTL：利用潛在擴散模型，從部分標註的合成數據集中進行多任務學習", "summary_zh": "這項研究提出StableMTL方法，利用擴散模型強大的泛化能力，在只有部分標註的合成數據集上訓練多任務模型，實現零樣本學習。StableMTL將圖像生成器用於潛在回歸，通過任務編碼、逐任務條件化和定制的訓練方案來調整去噪框架。它採用統一的潛在損失，無需仔細平衡各任務的損失，從而實現無縫擴展到更多任務。此外，引入了多流模型和任務注意力機制，將任務間的交互轉化為高效的單向注意力，促進跨任務共享。實驗證明，StableMTL在多個基準測試中優於其他方法。", "applications": ["智慧城市：利用路口監視器畫面，同時辨識車流量、行人數量、違規停車等，提升交通管理效率，並減少人力成本。", "醫療影像分析：從X光片或斷層掃描中，同時檢測多種疾病徵兆，例如腫瘤大小、骨折位置、炎症反應等，輔助醫生進行更精確的診斷。", "電商平台：自動分析商品圖片，同時提取商品屬性（顏色、材質、款式）和場景信息（室內、戶外），提升商品分類和搜尋的準確性，改善使用者體驗。"], "pitch": "想像一下，我們能用AI同時處理多項任務，而且只需要少量標註數據甚至完全不需要！StableMTL就是實現這個願景的關鍵。它像一個超級AI訓練師，能從各種模擬數據中學習，並將知識應用到真實世界。這不僅能大幅降低AI開發成本，還能開啟無限可能。例如，在自動駕駛領域，我們可以同時訓練AI識別交通號誌、行人、障礙物，大幅提升安全性。在醫療診斷方面，AI能同時分析多種病徵，協助醫生做出更精準的判斷。這項技術的潛在市場價值數十億美元，現在投資，就能搶佔AI多任務學習的先機，成為下一個AI獨角獸！", "audio": "docs/data/audios/2506.08013v1.wav"}
{"query": "AI", "id": "2506.07997v1", "url": "http://arxiv.org/abs/2506.07997v1", "title": "Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System", "summary": "The construction industry is characterized by both high physical and\npsychological risks, yet supports of mental health remain limited. While\nadvancements in artificial intelligence (AI), particularly large language\nmodels (LLMs), offer promising solutions, their potential in construction\nremains largely underexplored. To bridge this gap, we developed a\nconversational multi-agent system that addresses industry-specific challenges\nthrough an AI-driven approach integrated with domain knowledge. In parallel, it\nfulfills construction workers' basic psychological needs by enabling\ninteractions with multiple agents, each has a distinct persona. This approach\nensures that workers receive both practical problem-solving support and social\nengagement, ultimately contributing to their overall well-being. We evaluate\nits usability and effectiveness through a within-subjects user study with 12\nparticipants. The results show that our system significantly outperforms the\nsingle-agent baseline, achieving improvements of 18% in usability, 40% in\nself-determination, 60% in social presence, and 60% in trust. These findings\nhighlight the promise of LLM-driven AI systems in providing domain-specific\nsupport for construction workers.", "authors": ["Fan Yang", "Yuan Tian", "Jiansong Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:37:06.451208", "title_zh": "利用多代理人對話式AI系統支持建築工人的福祉", "summary_zh": "建築業面臨高 शारीरिक 與心理風險，但心理健康支持有限。本研究開發了一套多代理人對話式AI系統，結合領域知識，解決建築業的特定挑戰。系統透過與不同人格的代理人互動，滿足工人基本的心理需求，提供實際問題解決方案與社交互動，從而提升整體福祉。實驗結果顯示，相較於單一代理人系統，我們的系統在可用性、自主性、社交臨場感與信任度方面分別提升了18%、40%、60%與60%。這證明了大型語言模型驅動的AI系統在為建築工人提供領域特定支持方面的潛力。", "applications": ["工地裡，工人阿明心情不好，可以跟AI心理諮詢師聊聊，排解壓力，AI還能提醒他注意安全，避免工傷。", "老王是個水電工，遇到複雜的管線問題，可以問AI專家，AI會一步一步教他怎麼解決，省去查資料的時間。", "新來的工頭小李，對很多建材和工法不熟悉，可以隨時問AI老師，AI會提供相關知識和案例，幫助他快速上手。"], "pitch": "各位投資人，建築業長期面臨人力短缺、工安意外頻傳等問題，而我們的多代理人對話式AI系統，正是解決這些痛點的關鍵。它不僅能提升工人的心理健康與工作效率，更能降低工安事故的發生。想像一下，未來每個工地都配備這樣一套AI系統，它就像一位隨時待命的超級顧問，為工人提供全方位的支持。這將大幅提升建築業的生產力與安全性，創造巨大的商業價值。我們預計，這項技術將能應用於其他高風險行業，例如礦業、製造業等，市場潛力無限。現在投資我們，您將成為引領建築業AI革命的先驅！", "audio": "docs/data/audios/2506.07997v1.wav"}
{"query": "Foundation Model", "id": "2506.07886v1", "url": "http://arxiv.org/abs/2506.07886v1", "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:38:16.837702", "title_zh": "EgoM2P：以自我為中心的視角進行多模態多任務預訓練", "summary_zh": "本研究提出EgoM2P框架，旨在解決以自我為中心的視角下，如何有效理解多模態訊號的挑戰。EgoM2P利用時序感知的多模態tokens，透過遮蔽建模學習，訓練出一個通用的4D理解模型。此模型支援多種任務，包括眼球追蹤、以自我為中心的相機追蹤，以及從單眼視訊進行深度估計，甚至可以生成條件式的以自我為中心的視訊。EgoM2P在多項任務上達到或超越了專用模型的效果，且速度更快。我們將開源EgoM2P，以促進以自我為中心的視覺研究。", "applications": ["導航輔助：想像一下，戴上AR眼鏡，系統能根據你的視線和頭部動作，預測你的意圖，並在視野中即時顯示導航資訊，再也不用低頭看手機了。", "運動訓練：運動員佩戴設備後，系統能分析他們的動作、視線焦點和身體姿態，提供個人化的訓練建議，幫助他們提升表現，例如高爾夫揮桿或籃球投籃。", "遠端協作：工程師可以戴上頭戴式裝置，讓遠端的專家看到他們所看到的，並透過即時的視線追蹤和手勢識別，進行更有效的遠端指導和協作，減少錯誤和提高效率。"], "pitch": "各位投資人，我們正站在AIoT革命的風口浪尖！EgoM2P不僅僅是一個模型，它是一把解鎖未來人機互動的鑰匙。試想，透過我們的技術，AR/VR設備將變得更加智慧、更具沉浸感；機器人將能更精準地理解人類意圖，協作更加順暢；醫療領域，醫生可以透過AR眼鏡進行遠端手術指導，提升醫療水平。EgoM2P的潛力遠不止於此，它將成為元宇宙、智慧工廠、無人駕駛等領域的核心技術。我們正在打造一個全新的互動模式，一個以人為本的智慧世界。現在加入我們，共同開創這個千億級市場！", "audio": "docs/data/audios/2506.07886v1.wav"}
{"query": "Diffusion Model", "id": "2506.08009v1", "url": "http://arxiv.org/abs/2506.08009v1", "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:39:36.544597", "title_zh": "自我強制：彌合自迴歸影片擴散中的訓練-測試差距", "summary_zh": "本研究提出「自我強制」訓練方法，解決自迴歸影片擴散模型中長期存在的暴露偏差問題。傳統模型在訓練時依賴真實資料，但在實際應用時卻需根據自身產生的不完美結果生成影片。自我強制透過在訓練期間使用關鍵值（KV）快取進行自迴歸展開，讓模型根據先前自身生成的輸出產生每一幀，從而在影片層級進行整體性監督，直接評估整個生成序列的品質。此外，透過幾步擴散模型和隨機梯度截斷策略，兼顧計算成本和效能。實驗證明，此方法能在單一GPU上實現亞秒級延遲的即時串流影片生成，生成品質甚至超越速度較慢的非因果擴散模型。", "applications": ["想像一下，未來的線上遊戲！遊戲畫面不用事先算好，而是根據你的遊玩方式即時生成，每次玩都有獨一無二的體驗，就像真的身歷其境。", "假設你是個室內設計師，想讓客戶更快看到設計成果。現在只要輸入簡單的描述，就能即時生成不同風格的3D室內設計影片，快速溝通想法，大幅提升效率。", "如果醫院想用AI訓練醫生進行手術模擬，過去需要大量資源建立模型。現在利用這項技術，可以即時生成各種手術場景，讓醫生在逼真的環境下練習，提升手術成功率。"], "pitch": "各位投資人，我們正處於影片生成技術的革命性轉捩點！「自我強制」技術不僅解決了現有模型的瓶頸，更開創了即時、高品質影片生成的全新可能性。想像一下，未來影音內容的生產成本將大幅降低，個人化的互動式影片體驗將無處不在。從遊戲、娛樂、教育到醫療，各行各業都將因此受益。我們的技術擁有極高的商業價值，未來將能授權給各大影音平台、遊戲公司、教育機構，甚至能應用於元宇宙的內容生成。我們預計未來五年內，影片生成市場規模將達到數百億美元，而「自我強制」技術將在這個市場中佔據領先地位，為各位投資人帶來豐厚的回報！現在加入我們，一起打造影片生成的未來！", "audio": "docs/data/audios/2506.08009v1.wav"}
{"query": "AI", "id": "2506.07982v1", "url": "http://arxiv.org/abs/2506.07982v1", "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:29:23.275744", "title_zh": "τ²-Bench：在雙重控制環境中評估對話式代理", "summary_zh": "現有對話式AI代理的評估基準主要模擬單一控制環境，AI代理能使用工具與世界互動，使用者則是被動提供資訊。本研究提出τ²-Bench，模擬電信領域的雙重控制環境，代理和使用者都能使用工具在共享環境中操作。τ²-Bench包含：Dec-POMDP模型、可程式化任務生成器、可靠的使用者模擬器，以及細緻的代理效能分析。實驗顯示，從無使用者到雙重控制環境，代理效能顯著下降，突顯了引導使用者的挑戰。τ²-Bench為測試代理的推理能力和引導使用者行為的能力提供了一個可控的平台。", "applications": ["想像一下，以後打電話給電信客服，AI不只會幫你查帳單，還能一步步引導你設定數據漫遊，就像朋友一樣教你操作手機。", "家裡的智慧家電壞了，AI客服會引導你檢查電源、重啟設備，甚至教你簡單的故障排除，不用再盲目等待維修人員。", "使用複雜的軟體時，AI助手會像導航一樣，一步步引導你完成任務，再也不用擔心找不到功能或操作錯誤。"], "pitch": "各位投資人，我們正處於AI客服的轉型期！傳統AI只能被動回答問題，而我們的τ²-Bench技術，讓AI具備了引導使用者操作的能力，這將徹底改變人機互動模式。想像一下，未來電信公司、家電廠商、軟體開發商，都需要這種能主動引導使用者的AI客服。我們的技術不僅提升了客服效率，更降低了人力成本，潛在市場規模數十億美元！更重要的是，這項技術是AI邁向更複雜、更實用應用的關鍵一步。我們相信，τ²-Bench將成為AI客服領域的黃金標準，現在投資，您將站在AI革命的最前沿！", "audio": "docs/data/audios/2506.07982v1.wav"}
{"query": "Foundation Model", "id": "2506.07740v1", "url": "http://arxiv.org/abs/2506.07740v1", "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:30:39.914350", "title_zh": "Flow-Anything：從大規模單視角圖像學習真實世界光流估計", "summary_zh": "光流估計是電腦視覺的關鍵技術，是許多影片任務的基礎。Flow-Anything 提出一個大規模數據生成框架，從真實世界任何單視角圖像中學習光流估計。首先，利用單眼深度估計網路將單視角圖像轉換為3D表示，以便在虛擬相機下渲染光流和新視角圖像。其次，開發物件獨立體積渲染模組和深度感知修復模組，對3D表示中的動態物件進行建模。由此生成的 FA-Flow 數據集，首次展示了從大規模真實世界圖像生成光流訓練數據的優勢，超越了最先進的無監督和合成數據集上的監督方法。該模型可作為基礎模型，提升各種下游影片任務的效能。", "applications": ["**自動駕駛安全升級：** 想像一下，汽車能更精準地判斷行人或車輛的移動速度和方向，即使在光線不足或視線不佳的情況下，也能提前預警，大幅降低事故風險。", "**運動賽事分析：** 運動員的動作捕捉和分析變得更精準。例如，教練可以利用這項技術，精確分析籃球運動員的投籃姿勢，找出細微的改進空間，提升投籃命中率。", "**影視特效製作：** 製作更逼真的特效，例如模擬人群移動、火焰燃燒等。以往需要大量人工調整，現在可以透過 AI 自動生成，大幅降低製作成本，提升製作效率。"], "pitch": "想像一下，我們正在打造的是下一代影片理解引擎的核心！Flow-Anything 不僅僅是一個光流估計模型，它是一個從真實世界學習的強大基礎模型。這意味著，我們能夠以前所未有的精度理解、預測和生成影片內容。自動駕駛、智慧安防、運動分析、影視特效… 這些只是冰山一角！更令人興奮的是，我們正在探索將這項技術應用於元宇宙的內容生成，讓每個人都能輕鬆創造出逼真的虛擬世界。隨著影片數據爆炸式增長，對影片理解的需求將會持續攀升。Flow-Anything 將成為這場變革的關鍵推動力，潛在市場規模將達到數十億美元！現在加入我們，一起打造影片理解的未來！", "audio": "docs/data/audios/2506.07740v1.wav"}
{"query": "Diffusion Model", "id": "2506.08004v1", "url": "http://arxiv.org/abs/2506.08004v1", "title": "Dynamic View Synthesis as an Inverse Problem", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:32:02.092672", "title_zh": "動態視角合成作為一個逆問題", "summary_zh": "本研究將單眼影片的動態視角合成視為一個逆問題，並在無需訓練的環境下解決。透過重新設計預訓練視訊擴散模型的雜訊初始化階段，我們實現了高保真度的動態視角合成，無需任何權重更新或輔助模組。我們首先發現了零終端信噪比（SNR）排程對確定性反演造成根本障礙，並通過引入一種新的雜訊表示，稱為K階遞迴雜訊表示，來解決這個問題。我們推導出這種表示的閉合形式表達式，從而實現了VAE編碼和DDIM反演潛在變數之間的精確高效對齊。為了合成由相機運動產生嘅新可見區域，我們引入了隨機潛在調製，它在潛在空間上執行可見性感知採樣，以完成遮擋區域。綜合實驗表明，動態視角合成可以通過雜訊初始化階段中的結構化潛在操作有效地執行。", "applications": ["線上遊戲：玩家可以從任意角度觀看遊戲角色，提供更自由的遊戲體驗。", "虛擬實境旅遊：使用者可以透過現有影片，自由探索拍攝地點的各個角度，彷彿身歷其境。", "電影特效：在沒有額外拍攝的情況下，從不同的角度呈現爆炸、撞擊等特效畫面。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它能從普通影片中創造出令人驚豔的3D視角體驗。想像一下，你只需要一段手機影片，就能生成一個完整的3D模型，從任何角度觀看，這就是我們的技術能做到的。這項技術的應用潛力無窮，從遊戲、電影到電商、房地產，都能帶來顛覆性的改變。例如，電商平台可以讓消費者360度無死角地檢視商品，大大提升購買意願；房地產公司可以打造沉浸式的線上看房體驗，讓潛在買家足不出戶就能感受到房屋的空間感。更重要的是，我們的技術無需昂貴的硬體設備或複雜的訓練過程，成本效益極高。我們相信，這項技術將引領下一代視覺體驗的浪潮，成為元宇宙和虛擬實境領域的關鍵基礎設施。現在加入我們，一起打造視覺革命的未來！", "audio": "docs/data/audios/2506.08004v1.wav"}
{"query": "AI", "id": "2506.07957v1", "url": "http://arxiv.org/abs/2506.07957v1", "title": "Understanding the Error Sensitivity of Privacy-Aware Computing", "summary": "Homomorphic Encryption (HE) enables secure computation on encrypted data\nwithout decryption, allowing a great opportunity for privacy-preserving\ncomputation. In particular, domains such as healthcare, finance, and\ngovernment, where data privacy and security are of utmost importance, can\nbenefit from HE by enabling third-party computation and services on sensitive\ndata. In other words, HE constitutes the \"Holy Grail\" of cryptography: data\nremains encrypted all the time, being protected while in use.\n  HE's security guarantees rely on noise added to data to make relatively\nsimple problems computationally intractable. This error-centric intrinsic HE\nmechanism generates new challenges related to the fault tolerance and\nrobustness of HE itself: hardware- and software-induced errors during HE\noperation can easily evade traditional error detection and correction\nmechanisms, resulting in silent data corruption (SDC).\n  In this work, we motivate a thorough discussion regarding the sensitivity of\nHE applications to bit faults and provide a detailed error characterization\nstudy of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes\ndue to its fixed-point arithmetic support for AI and machine learning\napplications. We also delve into the impact of the residue number system (RNS)\nand the number theoretic transform (NTT), two widely adopted HE optimization\ntechniques, on CKKS' error sensitivity. To the best of our knowledge, this is\nthe first work that looks into the robustness and error sensitivity of\nhomomorphic encryption and, as such, it can pave the way for critical future\nwork in this area.", "authors": ["Matías Mazzanti", "Esteban Mocskos", "Augusto Vega", "Pradip Bose"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:53:58.454764", "title_zh": "理解隱私感知運算的錯誤敏感性", "summary_zh": "同態加密（HE）允許在加密數據上進行安全運算，無需解密，為保護隱私的運算提供了絕佳機會。醫療、金融和政府等對數據隱私和安全極為重要的領域，可以透過HE在敏感數據上實現第三方運算和服務。HE的安全性依賴於添加到數據中的雜訊，使相對簡單的問題在計算上變得難以處理。然而，這種以錯誤為中心的機制也帶來了新的挑戰：HE運算期間的硬體和軟體錯誤容易避開傳統的錯誤檢測和校正機制，導致靜默數據損壞（SDC）。本研究深入探討HE應用對位元錯誤的敏感性，並詳細分析了CKKS方案的錯誤特性，同時也研究了餘數系統（RNS）和數論變換（NTT）對CKKS錯誤敏感性的影響。這項研究首次關注同態加密的穩健性和錯誤敏感性，為該領域的未來重要研究奠定了基礎。", "applications": ["想像一下，醫院可以利用這項技術，在不洩露病人隱私的情況下，讓AI分析病人的病歷，找出潛在的疾病風險，及早預防。", "銀行可以使用這項技術，在加密的狀態下，分析客戶的交易數據，判斷是否有詐欺行為，同時保護客戶的財務隱私。", "政府可以利用這項技術，在不公開個人資訊的情況下，統計人口數據，制定更完善的公共政策。"], "pitch": "各位創投先進，我們正在打造的是數據安全的新紀元！同態加密技術，如同為數據穿上了一層隱形盔甲，讓數據在被使用的同時，依然保持加密狀態。試想一下，在醫療、金融、國防等領域，有多少數據因為安全疑慮而無法被充分利用？我們的研究，正是要提升同態加密的穩定性，讓這項技術真正落地應用。未來，我們將開發針對不同產業的同態加密解決方案，例如：AI模型可以在加密的醫療數據上進行訓練，藥廠可以在不洩露配方的情況下進行研發合作，甚至可以打造完全匿名的區塊鏈應用。這不僅僅是一項技術，更是一個龐大的市場，一個重新定義數據價值的機會！現在加入我們，一起開啟數據安全的新篇章！", "audio": "docs/data/audios/2506.07957v1.wav"}
{"query": "Foundation Model", "id": "2506.07647v1", "url": "http://arxiv.org/abs/2506.07647v1", "title": "Foundation Model Empowered Synesthesia of Machines (SoM): AI-native Intelligent Multi-Modal Sensing-Communication Integration", "summary": "To support future intelligent multifunctional sixth-generation (6G) wireless\ncommunication networks, Synesthesia of Machines (SoM) is proposed as a novel\nparadigm for artificial intelligence (AI)-native intelligent multi-modal\nsensing-communication integration. However, existing SoM system designs rely on\ntask-specific AI models and face challenges such as scarcity of massive\nhigh-quality datasets, constrained modeling capability, poor generalization,\nand limited universality. Recently, foundation models (FMs) have emerged as a\nnew deep learning paradigm and have been preliminarily applied to SoM-related\ntasks, but a systematic design framework is still lacking. In this paper, we\nfor the first time present a systematic categorization of FMs for SoM system\ndesign, dividing them into general-purpose FMs, specifically large language\nmodels (LLMs), and SoM domain-specific FMs, referred to as wireless foundation\nmodels. Furthermore, we derive key characteristics of FMs in addressing\nexisting challenges in SoM systems and propose two corresponding roadmaps,\ni.e., LLM-based and wireless foundation model-based design. For each roadmap,\nwe provide a framework containing key design steps as a guiding pipeline and\nseveral representative case studies of FM-empowered SoM system design.\nSpecifically, we propose LLM-based path loss generation (LLM4PG) and scatterer\ngeneration (LLM4SG) schemes, and wireless channel foundation model (WiCo) for\nSoM mechanism exploration, LLM-based wireless multi-task SoM transceiver\n(LLM4WM) and wireless foundation model (WiFo) for SoM-enhanced transceiver\ndesign, and wireless cooperative perception foundation model (WiPo) for\nSoM-enhanced cooperative perception, demonstrating the significant superiority\nof FMs over task-specific models. Finally, we summarize and highlight potential\ndirections for future research.", "authors": ["Xiang Cheng", "Boxun Liu", "Xuanyu Liu", "Ensong Liu", "Ziwei Huang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:55:21.591233", "title_zh": "基於基礎模型的機器聯覺(SoM)：AI原生智能多模態感知-通信集成", "summary_zh": "本研究提出基於基礎模型的機器聯覺(SoM)，旨在為未來的6G無線通信網絡提供AI原生的智能多模態感知與通信集成。現有SoM系統依賴於特定任務的AI模型，面臨數據稀缺、建模能力受限和泛化性差等挑戰。本研究系統性地對SoM系統設計的基礎模型進行分類，並提出基於大型語言模型(LLM)和無線基礎模型的兩種設計路線圖。通過案例研究，證明基礎模型在SoM機制探索、收發器設計和協同感知方面優於特定任務模型。這為未來無線通信的智能化開闢了新的方向。", "applications": ["想像一下，未來的無人機送貨更聰明！它們不僅能看到障礙物，還能『聽到』風的聲音，預測氣流變化，自動調整飛行路線，保證包裹安全準時送達。", "未來的智慧交通系統，車輛之間不僅能交換位置和速度信息，還能『感知』到前方道路的濕滑程度，提前提醒駕駛員減速，避免交通事故。", "在工廠裡，機器人不僅能看到零件的位置，還能『聽到』機器運轉的異常聲音，及早發現故障，避免生產線停工。"], "pitch": "各位投資人，我們正處於AI與無線通信融合的革命性時刻！傳統無線通信依賴人工設計，效率低下且難以適應複雜環境。我們的SoM技術，利用基礎模型，讓機器擁有『聯覺』能力，像人類一樣綜合運用多種感官信息，實現更智能、更可靠的無線通信。這不僅能大幅提升現有無線網絡的性能，更將催生全新的應用場景，例如：無人駕駛、智慧城市、工業互聯網等。試想一下，一個完全由AI驅動的無線世界，將帶來怎樣的巨大商業價值？我們的團隊擁有深厚的AI和無線通信背景，我們有信心將SoM技術打造成為下一代無線通信的基石，成為引領行業變革的領頭羊。現在加入我們，您將擁抱一個千億美元級別的市場，共同開啟AI賦能無線通信的黃金時代！", "audio": "docs/data/audios/2506.07647v1.wav"}
{"query": "Diffusion Model", "id": "2506.07999v1", "url": "http://arxiv.org/abs/2506.07999v1", "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:56:32.481289", "title_zh": "MADFormer：用於連續圖像生成的混合自迴歸與擴散轉換器", "summary_zh": "MADFormer結合了自迴歸（AR）和擴散模型，旨在優化圖像生成。它將圖像分成空間區塊，利用AR層進行全局條件設定，再用擴散層在各區塊內進行迭代局部細化。實驗證明，這種分區方式能有效提升高解析度圖像的生成品質，垂直混合AR和擴散層能在運算資源有限的情況下，顯著改善生成品質。MADFormer的研究為未來混合生成模型提供了實用的設計原則，展現了在效率和品質之間取得平衡的潛力。", "applications": ["客製化頭像生成：使用者可以輸入少量個人照片，系統就能生成風格多樣、高解析度的個人頭像，用於社交媒體或遊戲。", "老照片修復：將模糊或損壞的老照片透過AI修復，還原清晰細節，讓珍貴回憶重現。", "室內設計預覽：輸入房屋格局和個人喜好，AI就能生成不同風格的室內設計方案，讓使用者在裝修前預覽效果。"], "pitch": "各位創投夥伴，我們團隊帶來的是MADFormer，一項突破性的圖像生成技術，它巧妙結合了自迴歸和擴散模型，在高解析度圖像生成領域實現了質的飛躍。想像一下，未來遊戲、電影、廣告等產業，對高質量、客製化圖像的需求將會爆炸性增長。MADFormer能以更低的成本、更快的速度，生成以往難以想像的圖像內容。這不僅能大幅降低製作成本，更能激發無限的創意潛能。我們的技術已經在FFHQ-1024和ImageNet等數據集上驗證，效果顯著。我們預計，MADFormer將成為下一代圖像生成引擎的核心技術，佔領市場制高點。現在投資，您將成為這場圖像革命的早期參與者，共同分享百億美元級別的市場紅利！", "audio": "docs/data/audios/2506.07999v1.wav"}
{"query": "AI", "id": "2506.07955v1", "url": "http://arxiv.org/abs/2506.07955v1", "title": "Implementation Considerations for Automated AI Grading of Student Work", "summary": "This study explores the classroom implementation of an AI-powered grading\nplatform in K-12 settings through a co-design pilot with 19 teachers. We\ncombine platform usage logs, surveys, and qualitative interviews to examine how\nteachers use AI-generated rubrics and grading feedback. Findings reveal that\nwhile teachers valued the AI's rapid narrative feedback for formative purposes,\nthey distrusted automated scoring and emphasized the need for human oversight.\nStudents welcomed fast, revision-oriented feedback but remained skeptical of\nAI-only grading. We discuss implications for the design of trustworthy,\nteacher-centered AI assessment tools that enhance feedback while preserving\npedagogical agency.", "authors": ["Zewei", "Tian", "Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Zachary Zhang", "Kevin He", "Min Sun"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:28:11.077238", "title_zh": "學生作業自動AI評分之實施考量", "summary_zh": "本研究與19位中小學教師合作，共同設計並試用AI評分平台。透過平台使用紀錄、問卷和訪談，我們發現教師們重視AI快速生成敘述性回饋，有助於形成性評估，但對自動評分的信任度較低，強調人工監督的必要性。學生們歡迎快速且針對修改建議的回饋，但對完全由AI評分仍持懷疑態度。研究結果強調，設計值得信賴、以教師為中心的AI評估工具至關重要，這類工具應能增強回饋效果，同時保留教學自主性。", "applications": ["想像一下，以後老師改作業不用改到天昏地暗，AI可以先幫忙找出重點，老師再針對學生的個別狀況給予指導，這樣老師就有更多時間可以關心每個學生的學習進度。", "學生寫作文，AI可以馬上給予修改建議，像是用字遣詞、文法結構等等，學生可以即時修改，不用等到老師改完才能知道哪裡需要改進，學習效率更高。", "公司新人訓練時，AI可以自動評估新人的學習狀況，並提供客製化的學習建議，幫助新人更快上手，企業也能更有效率地培訓人才。"], "pitch": "各位投資人，想像一下，未來教育的樣貌將會因為AI而徹底改變！我們的AI評分平台，不僅能大幅減輕老師的負擔，更能提供學生更即時、更個人化的學習回饋。這不僅僅是一個評分工具，更是一個提升整體教育品質的革命性產品。目前市場上缺乏真正能與教師協作、值得信賴的AI評分解決方案，而我們正好填補了這個缺口。未來，我們可以將這項技術應用於各個領域，例如企業培訓、線上課程、甚至個人技能提升。透過AI的協助，學習將變得更有效率、更具互動性。現在投資我們，您將參與一場教育科技的變革，共同打造一個更智慧、更高效的學習未來！", "audio": "docs/data/audios/2506.07955v1.wav"}
{"query": "Foundation Model", "id": "2506.07603v1", "url": "http://arxiv.org/abs/2506.07603v1", "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:29:37.081905", "title_zh": "SurgBench：用於手術影片分析的統一大型基準", "summary_zh": "手術影片理解對於手術自動決策、技能評估和術後品質改善至關重要。SurgBench是一個統一的手術影片基準框架，包含一個預訓練資料集SurgBench-P和一個評估基準SurgBench-E。SurgBench-P涵蓋22種手術和11個專科的5300萬幀畫面，SurgBench-E則在六個類別（階段分類、鏡頭運動、工具識別、疾病診斷、動作分類和器官檢測）中提供72個細粒度任務的評估。實驗表明，現有的影片基礎模型難以在不同的手術影片分析任務中推廣，而基於SurgBench-P的預訓練可以顯著提高性能，並提供對未見過的手術和模態的卓越跨域泛化能力。", "applications": ["想像一下，醫生可以透過AI分析手術錄影，即時獲得建議，就像開車時有導航一樣，降低手術風險，提升成功率。", "我們可以利用AI分析手術影片，客觀評估醫生的手術技巧，就像運動員透過數據分析提升表現一樣，幫助醫生精進技術。", "未來，透過分析大量手術影片，AI可以協助開發更精準的醫療器材和手術流程，就像汽車工程師透過碰撞測試改善車輛安全一樣，提升整體醫療水平。"], "pitch": "各位投資人，我們正在打造手術影片分析的「ImageNet」時刻！SurgBench不僅是個資料集，更是推動手術AI發展的引擎。想像一下，AI能像資深外科醫生一樣，即時判斷手術階段、辨識工具、甚至診斷疾病，這將徹底改變手術室的運作模式。透過SurgBench，我們可以開發出：1. 手術導航系統：協助醫生進行更精準、安全的手術；2. 遠程手術協作平台：讓專家醫生能遠端指導手術，解決醫療資源不均的問題；3. 個性化手術訓練系統：根據醫生的能力，提供客製化的訓練方案。市場潛力巨大！全球手術器材和服務市場規模數千億美元，而手術AI的加入，將帶來指數級的增長。我們預計，SurgBench將成為手術AI領域的黃金標準，吸引無數開發者和研究人員投入，共同打造更智慧、更高效、更安全的醫療未來。現在加入，您將站在這場醫療革命的最前沿！", "audio": "docs/data/audios/2506.07603v1.wav"}
{"query": "Diffusion Model", "id": "2506.07998v1", "url": "http://arxiv.org/abs/2506.07998v1", "title": "Generative Modeling of Weights: Generalization or Memorization?", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:31:10.048997", "title_zh": "權重的生成模型：泛化還是記憶？", "summary_zh": "近年來，生成模型在圖像和影片生成方面取得了成功，因此有人開始探索使用它們來合成有效的神經網路權重。這些方法將訓練好的神經網路檢查點作為訓練數據，並旨在於推論過程中生成高性能的神經網路權重。然而，我們的研究發現，這些方法在很大程度上是通過記憶來合成權重的，它們產生的權重要么是訓練檢查點的複製品，要么只是簡單的插值。目前的這些方法在獲得不同且高性能的模型方面，並不能優於簡單的基準方法，例如在權重中添加噪音或採取簡單的權重集成。更令人驚訝的是，即使修改與圖像擴散模型中記憶相關的建模因素或應用數據增強，也無法有效緩解這種記憶現象。這項研究提醒我們，需要更謹慎地評估生成模型在新領域的應用能力，並對當前生成模型可以建模的數據類型進行更實際的評估。", "applications": ["AI藝術風格轉換：如果能真正生成新的模型權重，就能創造出前所未見的藝術風格，讓使用者輕鬆生成獨一無二的藝術作品。", "個性化醫療診斷：針對不同患者的基因數據，生成專屬的AI診斷模型，提供更精準的醫療建議，避免誤診。", "自動化程式碼優化：針對不同的程式碼結構，生成最佳化的編譯器權重，提升程式執行效率，讓App運行更順暢。"], "pitch": "各位創投，我們正在開發一種革命性的AI權重生成技術，它將徹底顛覆現有的AI模型訓練方式。雖然目前的研究顯示現有方法存在記憶問題，但這也代表著巨大的突破機會！想像一下，我們不再需要耗費大量資源從頭訓練模型，而是透過生成模型，快速產生針對特定任務的最佳化權重。這將大幅降低AI開發成本，加速AI應用落地。初期，我們可以聚焦在利基市場，例如個性化醫療、金融風險評估等，透過生成專屬模型，提供更精準的服務。未來，隨著技術成熟，我們更可以打造一個AI權重交易平台，讓AI開發者可以自由交易、組合、再生成新的權重，形成一個蓬勃發展的AI生態系。這不僅僅是一項技術，更是一個全新的商業模式，一個千億美元級的市場！現在加入我們，一起開創AI的下一個黃金時代！", "audio": "docs/data/audios/2506.07998v1.wav"}
{"query": "AI", "id": "2506.07949v1", "url": "http://arxiv.org/abs/2506.07949v1", "title": "Cost-Optimal Active AI Model Evaluation", "summary": "The development lifecycle of generative AI systems requires continual\nevaluation, data acquisition, and annotation, which is costly in both resources\nand time. In practice, rapid iteration often makes it necessary to rely on\nsynthetic annotation data because of the low cost, despite the potential for\nsubstantial bias. In this paper, we develop novel, cost-aware methods for\nactively balancing the use of a cheap, but often inaccurate, weak rater -- such\nas a model-based autorater that is designed to automatically assess the quality\nof generated content -- with a more expensive, but also more accurate, strong\nrater alternative such as a human. More specifically, the goal of our approach\nis to produce a low variance, unbiased estimate of the mean of the target\n\"strong\" rating, subject to some total annotation budget. Building on recent\nwork in active and prediction-powered statistical inference, we derive a family\nof cost-optimal policies for allocating a given annotation budget between weak\nand strong raters so as to maximize statistical efficiency. Using synthetic and\nreal-world data, we empirically characterize the conditions under which these\npolicies yield improvements over prior methods. We find that, especially in\ntasks where there is high variability in the difficulty of examples, our\npolicies can achieve the same estimation precision at a far lower total\nannotation budget than standard evaluation methods.", "authors": ["Anastasios N. Angelopoulos", "Jacob Eisenstein", "Jonathan Berant", "Alekh Agarwal", "Adam Fisch"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:36:20.593723", "title_zh": "成本最佳化的主動式AI模型評估", "summary_zh": "生成式AI系統的開發需要持續評估、資料收集和標註，這些都非常耗時且昂貴。為了加速迭代，開發者常使用成本較低的合成標註資料，但這可能導致偏差。本研究開發了一種成本敏感的方法，能主動平衡低成本但可能不準確的弱評估者（例如基於模型的自動評估器）和更昂貴但更準確的強評估者（例如人類）的使用。我們的目標是在有限的標註預算下，產生目標「強」評級的低變異數、無偏估計。我們利用主動和預測驅動的統計推斷，推導出一系列成本最佳化策略，以最大化統計效率的方式在弱評估者和強評估者之間分配標註預算。實驗結果表明，尤其是在範例難度差異很大的任務中，我們的策略能以遠低於標準評估方法的總標註預算，實現相同的估計精度。", "applications": ["線上購物評論分類：自動分析大量商品評論，判斷哪些評論需要人工審核以確保真實性和準確性，例如過濾機器人產生的假評論，節省人力成本。", "醫療影像診斷輔助：AI初步判讀X光片，將可疑病例優先交由醫生檢查，提高診斷效率並減輕醫生工作負擔。", "自動客服品質監控：AI自動評估客服人員的回答品質，僅將評分較低的對話轉交人工審核，確保服務品質並降低人工監控成本。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它將徹底改變AI模型的評估方式。想像一下，AI模型就像一個學生，需要不斷考試來提升能力。傳統的考試（人工評估）非常昂貴且耗時。我們的技術就像一個AI助教，能夠快速且低成本地進行初步評估，只有少數困難的題目才需要教授（人工專家）親自批改。這意味著，我們可以更快、更經濟地訓練出更強大的AI模型。市場潛力巨大！任何需要高品質AI模型的行業，如自動駕駛、金融風控、醫療診斷等，都將受益於我們的技術。我們預計，隨著AI模型越來越複雜，對高效評估的需求將呈指數級增長。我們的技術不僅能降低成本，還能加速AI的發展進程，搶佔市場先機。我們正在尋找有遠見的投資人，共同打造AI評估的新標準，開創一個AI驅動的未來！", "audio": "docs/data/audios/2506.07949v1.wav"}
{"query": "Foundation Model", "id": "2506.07584v1", "url": "http://arxiv.org/abs/2506.07584v1", "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data", "summary": "A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.", "authors": ["Hao Li", "Bowen Deng", "Chang Xu", "Zhiyuan Feng", "Viktor Schlegel", "Yu-Hao Huang", "Yizheng Sun", "Jingyuan Sun", "Kailai Yang", "Yiyao Yu", "Jiang Bian"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:37:42.168037", "title_zh": "MIRA：用於真實世界健康數據的醫療時間序列基礎模型", "summary_zh": "MIRA是一個專為醫療時間序列預測設計的基礎模型。它利用連續時間旋轉位置編碼精準建模時間間隔，透過頻率特定的專家混合層促進時間特化，並基於神經常微分方程的連續動態外推區塊，在任意目標時間戳上進行準確預測。MIRA在大型醫療數據集上預訓練，包含超過4540億個時間點。相較於其他模型，MIRA在分佈外和分佈內情境下，分別平均降低了10%和7%的預測誤差。我們也建立了一個全面的基準，為未來醫療時間序列建模研究奠定基礎。", "applications": ["**個人健康追蹤：** 想像一下，你的智慧手錶不只記錄心率，還能預測你未來幾小時的血壓變化，提前預警心血管風險，讓你及早採取行動。", "**醫院資源調度：** 醫院可以利用MIRA預測未來急診室的病患流量，提前調配醫護人員和床位，避免醫療資源擠兌，提升整體效率。", "**新藥開發：** 藥廠可以利用MIRA分析臨床試驗數據，更準確地預測藥物療效，加速新藥開發進程，讓更多病患受益。"], "pitch": "各位投資人，我們正處於醫療AI的黃金時代！MIRA不僅僅是一個模型，它是一個醫療時間序列的革命性突破。試想，一個能夠精準預測病患健康狀況、優化醫院運營、加速新藥開發的AI引擎，它的市場潛力有多大？現有的醫療數據分析方法往往受限於數據量和複雜性，而MIRA透過其獨特的架構，克服了這些挑戰，實現了跨機構、跨模態的數據整合和知識遷移。這意味著，即使在數據稀缺或隱私受限的環境下，MIRA也能發揮卓越的預測能力。我們相信，MIRA將成為醫療AI領域的Game Changer，引領醫療健康產業進入一個預測性、個性化的新時代。現在投資MIRA，您將搶佔先機，共同打造一個更健康、更智慧的未來！想像一下，未來的醫療決策不再依賴於事後分析，而是基於精準的預測和預防，這將為整個社會節省巨大的醫療成本，並提升人們的生活品質。MIRA的商業價值遠不止於此，它還可以應用於保險精算、健康管理、遠程醫療等眾多領域，創造無限的可能性。我們誠摯邀請您加入我們的行列，共同見證MIRA的輝煌前景！", "audio": "docs/data/audios/2506.07584v1.wav"}
{"query": "Diffusion Model", "id": "2506.07986v1", "url": "http://arxiv.org/abs/2506.07986v1", "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:39:04.324764", "title_zh": "重新思考多模態擴散轉換器中的跨模態互動", "summary_zh": "多模態擴散轉換器(MM-DiT)在文字驅動的視覺生成方面取得了顯著進展，但現有模型在文字提示與生成內容的精確對齊方面仍存在挑戰。本研究發現MM-DiT的注意力機制存在兩個關鍵問題：視覺和文字模態之間的token不平衡導致跨模態注意力受到抑制，以及缺乏時間步感知的注意力權重，進而阻礙對齊。為了解決這些問題，我們提出一種名為「溫度調整跨模態注意力(TACA)」的參數高效方法，通過溫度縮放和時間步相關調整來動態地重新平衡多模態互動。結合LoRA微調，TACA顯著提升了T2I-CompBench基準測試上的文本-圖像對齊效果，且計算開銷極小。經驗證，TACA能有效改善圖像-文本對齊。", "applications": ["想像一下，你可以用一段文字描述你想要的房間裝潢風格，例如『簡約北歐風，陽光灑落的客廳』，AI就能自動生成符合你描述的3D模型，讓你提前預覽裝潢效果。", "假設你是服裝設計師，只要輸入『一件具有未來感的銀色外套，搭配不對稱剪裁』，AI就能快速生成多種設計草圖，激發你的創作靈感，省去大量手繪時間。", "如果你是社群媒體小編，需要快速製作吸睛的廣告素材，只要輸入產品描述和想要的風格，AI就能自動生成高品質的產品圖片，讓你的廣告更具吸引力。"], "pitch": "各位投資人，我們正站在AI圖像生成革命的浪潮之巔！我們的技術TACA，能讓AI更精準地理解文字指令，生成更符合需求的圖像，解決了目前AI圖像生成領域最核心的痛點。這意味著什麼？想像一下，未來設計師、行銷人員、甚至一般消費者，都能輕鬆地將腦海中的想法轉化為視覺圖像，創造無限可能！從客製化產品設計、虛擬實境內容生成，到電影特效製作，TACA的應用前景無可限量。我們相信，TACA將成為下一代AI圖像生成引擎的核心技術，引領市場走向更精準、更個性化的圖像生成時代。現在加入我們，一起打造AI圖像生成的未來，贏取豐厚的回報！", "audio": "docs/data/audios/2506.07986v1.wav"}
{"query": "AI", "id": "2506.07907v1", "url": "http://arxiv.org/abs/2506.07907v1", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between between the\ndecay amplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De~Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:25:46.104394", "title_zh": "利用C-偶與C-奇量子關聯的D D̄介子對，對D⁰→K⁻π⁺和D̄⁰→K⁻π⁺衰變間強相位差的新穎測量", "summary_zh": "本研究提出一種測量D⁰和D̄⁰介子衰變振幅之間強相位差的新穎技術，利用在ψ(3770)產生閾值以上能量的e⁺e⁻碰撞中產生的量子關聯D D̄介子對。這些介子對產生於電荷共軛對稱性的偶數和奇數本徵態。利用此技術，首次使用此類數據樣本確定了D⁰-D̄⁰的相對強相位。使用BESIII實驗收集的積分光度為7.13 fb⁻¹，質心能量在4.13-4.23 GeV之間的數據集，測得D⁰→K⁻π⁺和D̄⁰→K⁻π⁺衰變之間的強相位差δDKπ為(192.8⁺¹¹.⁰ ⁺¹.⁹₋₁₂.₄ ₋₂.₄)°。", "applications": ["想像一下，我們可以更精準地分析醫療影像。這項技術就像是幫醫生配備了更銳利的眼睛，能更早發現病灶，例如極早期的癌症，提高治癒率。", "在材料科學領域，這項技術能幫助我們更深入了解新材料的微觀結構。這就像是擁有一台超強顯微鏡，讓我們能設計出更堅固、更輕便、更高效能的材料，應用於航空、汽車等產業。", "在通訊加密方面，如果能精準掌握基本粒子的相位，就能開發出更難破解的加密技術。這就像是打造了一道堅不可摧的防火牆，保護我們的數位資訊安全。"], "pitch": "各位投資人，我們正在開發一項革命性的量子測量技術，它能精準掌握亞原子粒子的微妙特性，其潛力遠超想像！目前我們已成功驗證了該技術在D介子衰變上的應用，未來將可拓展至醫療、材料、資安等領域。試想，更精準的醫療診斷、更強大的新材料、以及牢不可破的加密技術，都將因為這項技術而成為可能。這不僅是一項科學突破，更是一座蘊藏無限商機的金礦！我們需要您的資金支持，共同引領這場科技革命，開創量子科技的新紀元！現在投資，您將成為這項劃時代技術的早期投資者，共享豐碩的成果。", "audio": "docs/data/audios/2506.07907v1.wav"}
{"query": "Foundation Model", "id": "2506.07576v1", "url": "http://arxiv.org/abs/2506.07576v1", "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding", "summary": "Video understanding has been considered as one critical step towards world\nmodeling, which is an important long-term problem in AI research. Recently,\nmulti-modal foundation models have shown such potential via large-scale\npretraining. However, these models simply align encoders of different\nmodalities via contrastive learning, while lacking deeper multi-modal\ninteractions, which is critical for understanding complex target movements with\ndiversified video scenes. To fill this gap, we propose a unified Super Encoding\nNetwork (SEN) for video understanding, which builds up such distinct\ninteractions through recursive association of multi-modal encoders in the\nfoundation models. Specifically, we creatively treat those well-trained\nencoders as \"super neurons\" in our SEN. Via designing a Recursive Association\n(RA) block, we progressively fuse multi-modalities with the input video, based\non knowledge integrating, distributing, and prompting of super neurons in a\nrecursive manner. In this way, our SEN can effectively encode deeper\nmulti-modal interactions, for prompting various video understanding tasks in\ndownstream. Extensive experiments show that, our SEN can remarkably boost the\nfour most representative video tasks, including tracking, recognition,\nchatting, and editing, e.g., for pixel-level tracking, the average jaccard\nindex improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular\nCaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,\nand frame consistency increases 4.1% compared to the popular TuneA-Video\napproach.", "authors": ["Boyu Chen", "Siran Chen", "Kunchang Li", "Qinglin Xu", "Yu Qiao", "Yali Wang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:26:56.691958", "title_zh": "超級編碼網路：用於影片理解的多模態編碼器之遞迴關聯", "summary_zh": "本研究提出一個名為「超級編碼網路」(SEN) 的新型影片理解架構，旨在提升多模態基礎模型在理解複雜影片場景中的能力。SEN 將預訓練的編碼器視為「超級神經元」，並透過遞迴關聯(RA)模組，逐步融合多模態資訊。RA模組基於知識整合、分配和提示，以遞迴方式將多模態資訊與輸入影片融合。實驗證明，SEN 能顯著提升影片追蹤、辨識、聊天和編輯等任務的效能，例如，在像素級追蹤上，Jaccard 指數平均提高 2.7%，時間一致性降低 8.8%。", "applications": ["智慧監控：SEN 可以讓監視器更精準地追蹤畫面中的人物或物體，即使在複雜的環境中也能有效辨識異常行為，例如跌倒、打架等，及時發出警報。", "自動駕駛：透過分析車載鏡頭拍攝的影片，SEN 可以幫助自動駕駛系統更準確地理解路況，例如辨識行人、車輛、交通號誌等，做出更安全的決策。", "影音創作：SEN 可以應用於影片編輯軟體中，讓使用者更輕鬆地進行影片剪輯、特效添加等操作，例如自動為影片添加字幕、根據影片內容推薦合適的背景音樂等。"], "pitch": "各位投資人，我們正在開發一種革命性的影片理解技術，它將徹底改變AI對影片內容的理解方式。想像一下，未來的影片分析不再只是簡單的物件辨識，而是能像人類一樣理解影片的上下文、情感和意圖。我們的「超級編碼網路」正是實現這一目標的關鍵。它能讓AI具備更強大的影片分析能力，應用範圍極其廣泛。從智慧安防、自動駕駛，到影音娛樂、教育培訓，甚至是醫療診斷，只要涉及影片內容的理解，我們的技術就能創造巨大的價值。我們相信，隨著5G、AIoT等技術的快速發展，影片數據將呈現爆炸式增長。而我們所掌握的影片理解核心技術，將使我們在這個巨大的市場中佔據領先地位。現在加入我們，一起開創影片理解的新時代！", "audio": "docs/data/audios/2506.07576v1.wav"}
{"query": "Diffusion Model", "id": "2506.07923v1", "url": "http://arxiv.org/abs/2506.07923v1", "title": "Efficient Seismic Data Interpolation via Sparse Attention Transformer and Diffusion Model", "summary": "Seismic data interpolation is a critical pre-processing step for improving\nseismic imaging quality and remains a focus of academic innovation. To address\nthe computational inefficiencies caused by extensive iterative resampling in\ncurrent plug-and-play diffusion interpolation methods, we propose the\ndiffusion-enhanced sparse attention transformer (Diff-spaformer), a novel deep\nlearning framework. Our model integrates transformer architectures and\ndiffusion models via a Seismic Prior Extraction Network (SPEN), which serves as\na bridge module. Full-layer sparse multi-head attention and feed-forward\npropagation capture global information distributions, while the diffusion model\nprovides robust prior guidance. To mitigate the computational burden of\nhigh-dimensional representations, self-attention is computed along the channel\nrather than the spatial dimension. We show that using negative squared\nEuclidean distance to compute sparse affinity matrices better suits seismic\ndata modeling, enabling broader contribution from amplitude feature nodes. An\nadaptive ReLU function further discards low or irrelevant self-attention\nvalues. We conduct training within a single-stage optimization framework,\nrequiring only a few reverse diffusion sampling steps during inference.\nExtensive experiments demonstrate improved interpolation fidelity and\ncomputational efficiency for both random and continuous missing data, offering\na new paradigm for high-efficiency seismic data reconstruction under complex\ngeological conditions.", "authors": ["Xiaoli Wei", "Chunxia Zhang", "Baisong Jiang", "Anxiang Di", "Deng Xiong", "Jiangshe Zhang", "Mingming Gong"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:28:24.557981", "title_zh": "基於稀疏注意力Transformer與擴散模型的高效地震數據內插", "summary_zh": "本研究提出一種名為Diff-spaformer的新型深度學習框架，用於高效地震數據內插。該模型結合了Transformer架構和擴散模型，利用Seismic Prior Extraction Network (SPEN) 作為橋樑。透過全層稀疏多頭注意力機制捕捉全局資訊，並利用擴散模型提供穩健的先驗指導。為降低高維度運算負擔，自注意力計算沿通道而非空間維度進行。實驗證明，該方法在隨機和連續缺失數據情況下，均能提升內插的準確性與計算效率，為複雜地質條件下的高效地震數據重建提供新途徑。簡而言之，這項技術能更快速、更精確地重建地震數據。", "applications": ["**更精準的石油探勘：** 想像一下，石油公司可以更清楚地看到地底下的油藏，減少鑽探的失敗率，降低探勘成本，就像幫他們配備了超強夜視鏡。", "**更安全的建築選址：** 在地震頻繁的地區，我們可以更準確地評估地質結構，避開斷層帶，讓房屋蓋得更安全，就像幫建築師們配備了地質雷達。", "**更有效的災害預防：** 透過分析地震數據，我們可以更了解地殼的變動，預測潛在的地震風險，提前做好防災準備，就像幫我們配備了地震預警系統。"], "pitch": "各位投資人，我們正站在一場地震數據分析革命的風口浪尖！傳統的地震數據內插方法耗時且不精確，嚴重影響石油探勘、建築安全和災害預防等領域。我們的Diff-spaformer技術，結合Transformer和擴散模型，突破了算力瓶頸，大幅提升了數據重建的效率和精度。想像一下，未來的石油探勘不再是盲人摸象，而是精準定位，大幅降低鑽探成本，提高開採成功率！在建築安全方面，我們能提供更可靠的地質評估，避免因地質災害造成的巨大損失。更重要的是，這項技術有潛力應用於地震預測，提前預警，拯救無數生命！我們不僅僅是在銷售一種算法，更是在銷售一種更安全、更高效、更可持續的未來！現在投資，您將成為這場變革的早期參與者，共享巨大的市場紅利。我們預計，該技術在石油天然氣、建築工程、地震監測等領域的市場規模將達到數十億美元。加入我們，一起打造更安全的地球，挖掘更豐富的資源，實現更大的商業價值！", "audio": "docs/data/audios/2506.07923v1.wav"}
{"query": "AI", "id": "2506.07907v2", "url": "http://arxiv.org/abs/2506.07907v2", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between the decay\namplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-11T02:03:24.394715", "title_zh": "利用C-宇稱偶與奇的量子關聯D零反D零對，對D零→K負π正與反D零→K負π正衰變間強相位差的新穎測量", "summary_zh": "本研究提出一種新穎的強相位差測量技術，利用在ψ(3770)產生閾值以上能量的電子正負對撞機產生的量子關聯D零反D零對。此技術利用電荷共軛對稱性的偶與奇本徵態產生的D零反D零對。我們首次使用此數據樣本確定了D零-反D零的相對強相位。利用北京譜儀III實驗收集的7.13 fb⁻¹積分光度數據集，在4.13-4.23 GeV質心能量下，測得D零→K負π正與反D零→K負π正衰變之間的強相位差δDKπ為(192.8+11.0+1.9−12.4−2.4)°。", "applications": ["想像一下，未來海關可以更精準地分辨走私品。透過分析次原子粒子的衰變模式，我們可以開發出更靈敏的檢測儀器，揪出那些企圖蒙混過關的違禁品。", "在醫療領域，這項技術能幫助我們更深入了解藥物與人體細胞的交互作用。透過精確測量粒子間的細微差異，加速新藥開發，讓疾病治療更有效率。", "這就像是為原子世界打造了一把更精密的尺。有了更準確的測量，科學家能更深入探索宇宙的奧秘，例如暗物質的性質，甚至找到新的物理定律。"], "pitch": "各位投資人，我們正在開創一個全新的精準測量時代！這項技術不僅僅是一項科學突破，更是具有顛覆性商業潛力的明日之星。想像一下，透過我們對次原子粒子行為的精準掌握，可以應用於國防安全、新藥開發、材料科學等各個領域，創造出龐大的市場價值。這是一項具有無限可能的技術，現在加入，您將成為這場科技革命的領航者，共同塑造未來世界的樣貌！我們預期在五年內，光是安檢設備升級市場，就可達到數十億美元的規模，而這僅僅是冰山一角。不要錯過這個千載難逢的投資機會，讓我們一起引領科技創新，共創輝煌未來！", "audio": "docs/data/audios/2506.07907v2.wav"}
{"query": "Foundation Model", "id": "2506.07559v1", "url": "http://arxiv.org/abs/2506.07559v1", "title": "Cross-channel Perception Learning for H&E-to-IHC Virtual Staining", "summary": "With the rapid development of digital pathology, virtual staining has become\na key technology in multimedia medical information systems, offering new\npossibilities for the analysis and diagnosis of pathological images. However,\nexisting H&E-to-IHC studies often overlook the cross-channel correlations\nbetween cell nuclei and cell membranes. To address this issue, we propose a\nnovel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL\nfirst decomposes HER2 immunohistochemical staining into Hematoxylin and DAB\nstaining channels, corresponding to cell nuclei and cell membranes,\nrespectively. Using the pathology foundation model Gigapath's Tile Encoder,\nCCPL extracts dual-channel features from both the generated and real images and\nmeasures cross-channel correlations between nuclei and membranes. The features\nof the generated and real stained images, obtained through the Tile Encoder,\nare also used to calculate feature distillation loss, enhancing the model's\nfeature extraction capabilities without increasing the inference burden.\nAdditionally, CCPL performs statistical analysis on the focal optical density\nmaps of both single channels to ensure consistency in staining distribution and\nintensity. Experimental results, based on quantitative metrics such as PSNR,\nSSIM, PCC, and FID, along with professional evaluations from pathologists,\ndemonstrate that CCPL effectively preserves pathological features, generates\nhigh-quality virtual stained images, and provides robust support for automated\npathological diagnosis using multimedia medical data.", "authors": ["Hao Yang", "JianYu Wu", "Run Fang", "Xuelian Zhao", "Yuan Ji", "Zhiyu Chen", "Guibin He", "Junceng Guo", "Yang Liu", "Xinhua Zeng"], "published_date": "2025-06-09", "timestamp": "2025-06-11T02:04:57.583889", "title_zh": "用於H&E到IHC虛擬染色的跨通道感知學習", "summary_zh": "本研究提出一種新的跨通道感知學習(CCPL)策略，旨在解決現有H&E到IHC研究中忽略細胞核與細胞膜之間跨通道關聯性的問題。CCPL首先將HER2免疫組織化學染色分解為對應細胞核和細胞膜的蘇木精和DAB染色通道。利用病理基礎模型Gigapath的Tile Encoder，CCPL提取生成圖像和真實圖像的雙通道特徵，並測量細胞核和細胞膜之間的跨通道相關性。此外，CCPL還對單個通道的焦點光密度圖進行統計分析，以確保染色分佈和強度的連貫性。實驗結果表明，CCPL有效地保留了病理特徵，生成了高質量的虛擬染色圖像，並為使用多媒體醫療數據進行自動病理診斷提供了強大的支持。", "applications": ["1. 醫生遠距會診：偏鄉或資源不足的醫院，可以透過這項技術將傳統染色切片數位化，並轉換成其他染色方式，讓遠端的專家醫生能更全面地判讀病理影像，提升診斷準確性，及時給予治療建議。", "2. 加速新藥開發：藥廠可以利用這項技術，快速將現有的病理切片資料轉換成不同染色方式的影像，加速分析藥物對細胞的作用機制，縮短新藥開發時程，降低研發成本。", "3. 個人化健康管理：未來，透過AI分析個人病理切片資料，並轉換成不同染色結果，預測疾病風險，提供更精準的個人化健康建議，例如飲食、運動或生活習慣調整等。"], "pitch": "各位投資人，我們正在開發一項革命性的病理影像技術：跨通道感知學習(CCPL)，它能將傳統的H&E染色切片轉換成多種IHC染色結果，就像病理影像界的翻譯機！想像一下，全球病理實驗室每年產生數百萬張切片，但往往因為缺乏特定染色試劑或專業判讀人員而延誤診斷。CCPL能解決這個痛點，大幅提升診斷效率與準確性，特別是在癌症等重大疾病的診斷上，時間就是生命！\n\n更重要的是，CCPL的商業潛力巨大。我們可以將這項技術授權給醫院、診所、藥廠，甚至開發成雲端平台，提供全球病理影像分析服務。未來，結合AI和大數據，CCPL還能進一步發展成個人化醫療的關鍵技術，預測疾病風險，指導精準治療。我們相信，CCPL將徹底改變病理診斷的模式，創造巨大的社會價值和商業回報。現在加入我們，一起開創病理影像的未來！", "audio": "docs/data/audios/2506.07559v1.wav"}
{"query": "Diffusion Model", "id": "2506.07903v1", "url": "http://arxiv.org/abs/2506.07903v1", "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces", "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.", "authors": ["Kevin Rojas", "Yuchen Zhu", "Sichen Zhu", "Felix X. -F. Ye", "Molei Tao"], "published_date": "2025-06-09", "timestamp": "2025-06-11T02:07:18.160299", "title_zh": "擴散一切：基於任意狀態空間的多模態擴散模型", "summary_zh": "本研究提出一個創新的多模態擴散模型框架，可以直接在不同的數據模態上生成關聯數據，無需依賴複雜的外部預處理。透過為每個模態設計獨立的雜訊排程，模型能同時進行無條件生成和模態條件生成。研究團隊在文字-圖像生成和混合型表格數據合成上驗證了此方法，結果顯示其性能具有競爭力。這項技術突破了解碼器精準度的限制，為資料有限的應用開闢了新途徑，有望促進多模態資料生成領域的發展。", "applications": ["想像一下，你只要用文字描述你想要的畫面，這個AI就能自動生成對應的圖片，而且圖片的細節和風格完全符合你的要求。例如，輸入「陽光灑落在托斯卡尼田園上的美麗風景」，就能立即生成一幅令人驚嘆的畫作。", "醫生可以利用這個技術，結合病人的文字描述和X光片等影像資料，AI就能生成更精確的3D模型，幫助醫生更了解病情，制定更有效的治療方案。例如，輸入病患的症狀描述加上CT掃描，AI生成腫瘤的3D模型，輔助手術規劃。", "在電商平台上，消費者可以用文字描述想要的商品款式和功能，AI就能根據這些描述，自動生成商品的3D模型和宣傳圖片，讓消費者更直觀地了解商品，提升購買意願。例如，輸入「紅色真皮沙發，現代簡約風格」，就能生成不同角度的沙發照片。"], "pitch": "各位投資人，我們團隊帶來的是一項顛覆性的AI技術——「Diffuse Everything」。它解決了多模態資料生成的核心痛點，讓AI不再受限於單一數據類型。想像一下，一個AI能同時理解文字、圖像、聲音甚至表格數據，並將它們完美融合，創造出全新的內容。這將開啟一個全新的內容創作時代，無論是遊戲開發、廣告設計，甚至是醫療診斷，都將因為這項技術而產生革命性的變化。我們預計，在五年內，這項技術將成為AI領域的基礎設施，市場規模將達到數百億美元。現在加入我們，您將成為這場AI革命的領航者，共同開創一個無限可能的未來！", "audio": "docs/data/audios/2506.07903v1.wav"}
{"query": "AI", "id": "2506.09050v1", "url": "http://arxiv.org/abs/2506.09050v1", "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.", "authors": ["Yuki Imajuku", "Kohki Horie", "Yoichi Iwata", "Kensho Aoki", "Naohiro Takahashi", "Takuya Akiba"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:52:11.520831", "title_zh": "ALE-Bench：長時程目標驅動演算法工程的基準測試", "summary_zh": "ALE-Bench是一個新的基準測試，用於評估AI系統在解決複雜最佳化問題上的能力，這些問題來自現實世界的挑戰，例如包裹遞送路線規劃、人員排班、工廠生產計畫和電網平衡。它採用AtCoder Heuristic Contests中的真實任務，這些任務計算量大且沒有已知的精確解。與短期的編碼基準測試不同，ALE-Bench鼓勵在長時間範圍內迭代改進解決方案，並支持利用測試反饋和視覺化的互動式代理架構。初步評估顯示，現有大型語言模型在特定問題上表現出色，但在跨問題的一致性和長時程問題解決能力方面，與人類相比仍存在差距。這個基準測試旨在推動未來AI的發展。", "applications": ["想像一下，物流公司使用這項技術來優化送貨路線，不僅能節省油耗、減少碳排放，還能更快地將包裹送到客戶手中。", "醫院運用這項技術來安排醫生和護士的輪班，確保每個時段都有足夠的人力，同時也能讓醫護人員得到充分的休息，提升醫療品質。", "工廠利用這項技術來規劃生產流程，減少浪費、提高效率，讓產品更快上市，滿足市場需求。"], "pitch": "各位創投先進，我們正在開發ALE-Bench，一個革命性的AI演算法工程基準測試平台，它將徹底改變優化問題的解決方式。想像一下，一個AI系統可以自動設計出更高效的物流路線、更合理的生產排程、甚至更穩定的電網系統，這不僅能為企業節省數十億美元的成本，還能大幅提升資源利用率，為永續發展做出貢獻。ALE-Bench不僅僅是一個基準測試，它更是一個AI開發的加速器，能激勵研究人員開發出更強大的AI演算法，解決現實世界中最複雜的挑戰。我們相信，ALE-Bench將成為AI領域的Game Changer，而現在正是投資這個未來趨勢的絕佳時機。讓我們一起打造一個更智慧、更高效的世界！", "audio": "docs/data/audios/2506.09050v1.wav"}
{"query": "Foundation Model", "id": "2506.09042v1", "url": "http://arxiv.org/abs/2506.09042v1", "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "summary": "Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:53:40.014111", "title_zh": "Cosmos-Drive-Dreams：基於世界基礎模型的可擴展合成駕駛數據生成", "summary_zh": "為了解決自駕車訓練數據不足且難以捕捉罕見情境的問題，NVIDIA開發了Cosmos-Drive-Dreams合成數據生成管線。此管線利用Cosmos-Drive模型，該模型基於NVIDIA Cosmos世界基礎模型針對駕駛領域進行特化，能產生高品質、多視角、時空一致且可控的駕駛影片。透過Cosmos-Drive-Dreams，可以大規模生成包含高擬真度和挑戰性情境的駕駛數據集，有效改善長尾分布問題，並提升3D車道檢測、3D物件檢測和駕駛策略學習等下游任務的泛化能力。NVIDIA已開源此管線工具包、數據集和模型權重。", "applications": ["情境一：自駕計程車公司可以利用這項技術，模擬各種極端天氣（暴雨、濃霧）或突發狀況（行人突然衝出、車輛違規變換車道），來訓練自駕系統，確保在真實世界中能安全可靠地運行。", "情境二：遊戲開發商可以利用這項技術，快速生成多樣化的城市街道、鄉村道路等場景，讓賽車遊戲或開放世界遊戲的內容更豐富、更逼真，提升玩家的沉浸式體驗。", "情境三：汽車保險公司可以利用這項技術，重現過去發生的交通事故，分析事故原因，評估責任歸屬，甚至可以模擬未來可能發生的事故，提前預防，降低理賠風險。"], "pitch": "各位投資人，想像一下，我們正站在自駕車革命的浪潮之巔！但這場革命的關鍵，不是硬體，而是數據！真實世界數據昂貴且難以收集，特別是那些罕見但致命的邊緣案例。Cosmos-Drive-Dreams，正是解決這個痛點的殺手級應用！\n\n我們提供的，不僅僅是合成數據，而是基於NVIDIA Cosmos世界模型打造的、可控、高擬真的駕駛環境模擬器。這意味著，我們可以無限量地生成各種極端、危險的駕駛情境，讓自駕系統在虛擬世界中經歷千錘百鍊，確保在真實世界中萬無一失！\n\n想想看，這項技術的潛力有多大？它可以加速自駕車的開發和部署，降低事故率，甚至催生全新的保險模式！更重要的是，它還可以應用於遊戲、模擬訓練、城市規劃等領域，打造一個龐大的元宇宙生態系統！\n\n我們不僅僅是在賣數據，我們是在賣安全、賣效率、賣未來！投資Cosmos-Drive-Dreams，就是投資自駕車的未來，就是投資一個萬億美元級別的市場！現在加入我們，一起開創自駕車的新紀元！", "audio": "docs/data/audios/2506.09042v1.wav"}
{"query": "Diffusion Model", "id": "2506.09045v1", "url": "http://arxiv.org/abs/2506.09045v1", "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache", "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.", "authors": ["Zehong Ma", "Longhui Wei", "Feng Wang", "Shiliang Zhang", "Qi Tian"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:54:54.577489", "title_zh": "MagCache：基於幅度感知快取的快速影片生成", "summary_zh": "現有影片擴散模型的加速技術常依賴統一的啟發式方法或時間嵌入變體來跳過時間步並重複使用快取特徵，但這些方法通常需要大量校準，且容易因提示詞過擬合而產生不一致的輸出。本研究發現一個跨模型和提示詞的統一幅度定律：連續殘差輸出的幅度比率在大多數時間步中單調且穩定地下降，在最後幾個步驟中迅速下降。基於此，我們提出幅度感知快取(MagCache)，它使用誤差建模機制和自適應快取策略來跳過不重要的時間步。MagCache只需單一樣本進行校準，實驗結果表明，MagCache在Open-Sora和Wan 2.1上分別實現了2.1倍和2.68倍的加速，同時保持了卓越的視覺保真度。在可比較的計算預算下，MagCache在LPIPS、SSIM和PSNR指標上顯著優於現有方法。", "applications": ["想像一下，你想要快速製作一個社群媒體短片，展示你的寵物做一些可愛的動作。使用MagCache技術，你只需上傳一段短片，AI就能快速生成各種風格的影片，例如卡通風格、水彩風格等，讓你的影片更吸睛。", "假設你是一位老師，需要為學生製作教學影片。MagCache技術可以幫助你快速生成高品質的動畫或模擬影片，例如模擬化學反應、物理實驗等，讓教學內容更生動有趣。", "如果你是一位遊戲開發者，需要快速生成大量的遊戲素材，例如角色動畫、場景特效等。MagCache技術可以大幅縮短開發時間，讓你更快地推出新的遊戲內容。"], "pitch": "各位投資人，我們帶來的是MagCache，一項革命性的影片生成技術，它能大幅提升AI生成影片的速度與品質。想像一下，未來的影音內容創作將不再受限於時間與算力，每個人都能輕鬆創造出專業級的影片。這項技術的應用潛力無窮，從個人化行銷影片、教育內容製作，到遊戲開發、元宇宙內容生成，都將迎來爆炸性的成長。我們預期MagCache將成為AI影音領域的關鍵基礎設施，如同GPU之於深度學習，成為推動產業發展的核心引擎。現在投資MagCache，就是投資影音內容的未來！我們將透過授權、雲端服務、以及與各大影音平台合作等方式，快速將技術商業化，為投資人帶來豐厚的回報。這不僅是一個技術投資，更是一個參與未來內容革命的絕佳機會！", "audio": "docs/data/audios/2506.09045v1.wav"}
{"query": "AI", "id": "2506.09049v1", "url": "http://arxiv.org/abs/2506.09049v1", "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:37:53.933768", "title_zh": "VIKI-R：透過強化學習協調具體化多代理人合作", "summary_zh": "在動態環境中協調多個具體化代理人是人工智慧的核心挑戰。VIKI-R是一個基於視覺語言模型（VLM）的雙階段框架，專為具體化多代理人合作而設計。首先，它使用思維鏈（Chain-of-Thought）標註的示範對預訓練的VLM進行微調，然後在多層次的獎勵信號下進行強化學習。VIKI-R在VIKI-Bench基準測試中表現出色，VIKI-Bench是一個針對具體化多代理人合作的分層基準，包含多樣的機器人形態、多視角視覺觀察和結構化的監督信號。實驗證明，VIKI-R超越了現有方法，並能促使異構代理人之間產生複合式的合作模式。VIKI-R和VIKI-Bench共同為提升具體化AI系統中多代理人、視覺驅動的合作提供了一個統一的測試平台和方法。", "applications": ["智慧倉儲管理：多個機器人協同搬運貨物，根據視覺資訊判斷最佳路徑和搬運方式，提升倉儲效率。", "自動駕駛車隊：多輛自駕車透過視覺資訊共享路況，協同避開障礙物、優化行駛路線，減少交通擁堵。", "協作型機器人手術：多個手術機器人透過視覺引導，協同完成複雜手術，提高手術精準度和安全性。"], "pitch": "各位創投先進，我們正在打造一個AI界的變形金剛聯盟！VIKI-R不僅僅是一個多代理人協作系統，它更是一個賦能平台，讓不同形態、不同功能的機器人能夠像團隊一樣高效協作。想像一下，在智慧工廠裡，不再是單打獨鬥的機器手臂，而是機器人團隊自主協調，完成複雜的生產任務；在無人礦區，不再是各自為政的礦車，而是機器人車隊協同開採，大幅提升資源利用率。VIKI-R的核心價值在於其視覺驅動的協作能力，這意味著它能適應各種複雜的環境，無需預先編程，即可自主學習和優化協作策略。我們相信，隨著AI技術的發展，多代理人協作將成為主流，而VIKI-R將引領這個潮流。我們預計在未來五年內，VIKI-R將廣泛應用於製造業、物流業、醫療保健等領域，創造數十億美元的市場價值。現在加入我們，一起打造AI協作的新時代！", "audio": "docs/data/audios/2506.09049v1.wav"}
{"query": "Foundation Model", "id": "2506.09022v1", "url": "http://arxiv.org/abs/2506.09022v1", "title": "Do MIL Models Transfer?", "summary": "Multiple Instance Learning (MIL) is a cornerstone approach in computational\npathology (CPath) for generating clinically meaningful slide-level embeddings\nfrom gigapixel tissue images. However, MIL often struggles with small, weakly\nsupervised clinical datasets. In contrast to fields such as NLP and\nconventional computer vision, where transfer learning is widely used to address\ndata scarcity, the transferability of MIL models remains poorly understood. In\nthis study, we systematically evaluate the transfer learning capabilities of\npretrained MIL models by assessing 11 models across 21 pretraining tasks for\nmorphological and molecular subtype prediction. Our results show that\npretrained MIL models, even when trained on different organs than the target\ntask, consistently outperform models trained from scratch. Moreover,\npretraining on pancancer datasets enables strong generalization across organs\nand tasks, outperforming slide foundation models while using substantially less\npretraining data. These findings highlight the robust adaptability of MIL\nmodels and demonstrate the benefits of leveraging transfer learning to boost\nperformance in CPath. Lastly, we provide a resource which standardizes the\nimplementation of MIL models and collection of pretrained model weights on\npopular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab", "authors": ["Daniel Shao", "Richard J. Chen", "Andrew H. Song", "Joel Runevic", "Ming Y. Lu", "Tong Ding", "Faisal Mahmood"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:39:24.390655", "title_zh": "MIL模型是否具備遷移能力？", "summary_zh": "多實例學習(MIL)是計算病理學的基石，能從超高解析度的組織影像中產生具有臨床意義的切片層級嵌入。然而，MIL常在小型、弱監督的臨床數據集上表現不佳。本研究系統性地評估了預訓練MIL模型的遷移學習能力，結果顯示，即使在與目標任務不同的器官上訓練，預訓練模型也始終優於從頭訓練的模型。在泛癌數據集上進行預訓練，能實現跨器官和任務的強泛化能力，超越了切片基礎模型，同時使用的預訓練數據更少。本研究強調了MIL模型的強大適應性，並證明了利用遷移學習來提升計算病理學性能的優勢。", "applications": ["1. **癌症篩檢App：**想像一下，未來只要用手機App掃描病理切片，就能利用預訓練好的MIL模型，快速判斷是否有癌症風險，幫助民眾及早發現、及早治療。", "2. **遠距醫療病理診斷：**偏鄉地區缺乏病理醫生，透過MIL模型，可以將病理切片影像上傳雲端，讓遠端的專家協助診斷，大幅提升醫療資源的可及性。", "3. **新藥開發加速器：**藥廠可以利用MIL模型分析大量病理影像數據，找出潛在的藥物靶點，加速新藥開發的進程，降低研發成本。"], "pitch": "各位投資人，我們正在打造一個病理影像AI的革命性平台，核心技術是基於遷移學習的多實例學習(MIL)模型。傳統病理診斷耗時且高度依賴專家經驗，我們的技術能大幅提升診斷效率和準確性，解決醫療資源不均的問題。想像一下，未來每個人都能透過手機進行初步的癌症風險評估，這將是一個千億美元級別的市場！\n\n我們的競爭優勢在於：一是我們已經驗證了MIL模型在病理影像分析上的卓越性能，並遠超傳統方法；二是我們擁有豐富的預訓練模型資源，能快速適應不同的病理診斷任務；三是我們正在建立一個開放的平台，讓更多的研究者和醫生能參與到模型的訓練和應用中來，形成一個強大的生態系統。\n\n我們預計在未來三年內，將我們的技術應用於癌症篩檢、遠距醫療和新藥開發等領域，並與醫院、診所和藥廠建立深度合作夥伴關係。我們相信，我們的技術將會徹底改變病理診斷的模式，為人類健康做出更大的貢獻。現在加入我們，一起開啟病理AI的新時代！", "audio": "docs/data/audios/2506.09022v1.wav"}
{"query": "Diffusion Model", "id": "2506.09027v1", "url": "http://arxiv.org/abs/2506.09027v1", "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.", "authors": ["Runqian Wang", "Kaiming He"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:40:43.699370", "title_zh": "擴散與分散：具備表徵正規化的圖像生成", "summary_zh": "這項研究提出了一種名為「分散損失」（Dispersive Loss）的簡單、隨插即用的正規化方法，能有效提升基於擴散模型的圖像生成效果。不同於以往的擴散模型，此方法著重於表徵學習，鼓勵內部表徵在隱藏空間中分散，類似於對比自監督學習，但無需正樣本配對，因此不干擾迴歸的採樣過程。相較於表徵對齊（REPA）方法，此方法更簡潔，無需預訓練、額外參數或外部資料。實驗證明，在ImageNet數據集上，此方法能穩定地提升多種模型的效能。這項研究有望彌合生成模型和表徵學習之間的差距。", "applications": ["修復老照片或模糊圖片：想像一下，你可以用手機App輕鬆修復爺爺奶奶的老照片，讓模糊的影像變得清晰，重溫美好的回憶。", "生成獨一無二的藝術作品：藝術家可以利用這項技術，創造出前所未見的藝術風格，生成獨特且令人驚豔的數位畫作，豐富我們的視覺體驗。", "客製化商品設計：設計師可以根據客戶的需求，快速生成各種產品設計方案，例如客製化的手機殼、T恤圖案等，滿足個性化需求。"], "pitch": "各位創投先進，我們正在開發一項革命性的圖像生成技術，它能大幅提升生成圖像的品質和真實感，且成本極低。想像一下，未來電商平台可以利用這項技術，自動生成各種商品的逼真圖片，大幅降低攝影成本；遊戲公司可以快速創建精美的遊戲場景和角色，縮短開發週期；醫療領域可以生成高清晰度的醫學影像，輔助診斷。我們的「分散損失」方法，無需大量數據和複雜的訓練，就能達到甚至超越現有技術的效果。這不僅能為我們帶來巨大的商業價值，更能推動AI技術在各個領域的應用，改變人們的生活方式。現在投資我們，您將成為這場圖像革命的領航者，共同創造一個充滿無限可能的未來！", "audio": "docs/data/audios/2506.09027v1.wav"}
{"query": "AI", "id": "2506.09002v1", "url": "http://arxiv.org/abs/2506.09002v1", "title": "Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models", "summary": "Unit testing is essential for ensuring software reliability and correctness.\nClassic Search-Based Software Testing (SBST) methods and concolic\nexecution-based approaches for generating unit tests often fail to achieve high\ncoverage due to difficulties in handling complex program units, such as\nbranching conditions and external dependencies. Recent work has increasingly\nutilized large language models (LLMs) to generate test cases, improving the\nquality of test generation by providing better context and correcting errors in\nthe model's output. However, these methods rely on fixed prompts, resulting in\nrelatively low compilation success rates and coverage. This paper presents\nPALM, an approach that leverages large language models (LLMs) to enhance the\ngeneration of high-coverage unit tests. PALM performs program analysis to\nidentify branching conditions within functions, which are then combined into\npath constraints. These constraints and relevant contextual information are\nused to construct prompts that guide the LLMs in generating unit tests. We\nimplement the approach and evaluate it in 10 open-source Rust crates.\nExperimental results show that within just two or three hours, PALM can\nsignificantly improves test coverage compared to classic methods, with\nincreases in overall project coverage exceeding 50% in some instances and its\ngenerated tests achieving an average coverage of 75.77%, comparable to human\neffort (71.30%), highlighting the potential of LLMs in automated test\ngeneration. We submitted 91 PALM-generated unit tests targeting new code. Of\nthese submissions, 80 were accepted, 5 were rejected, and 6 remain pending\nreview. The results demonstrate the effectiveness of integrating program\nanalysis with AI and open new avenues for future research in automated software\ntesting.", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Hange Shi", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:29:22.809571", "title_zh": "透過混合程式分析與大型語言模型提升 Rust 單元測試覆蓋率", "summary_zh": "本研究提出 PALM 方法，結合程式分析與大型語言模型（LLM），顯著提升 Rust 程式的單元測試覆蓋率。傳統測試方法難以處理複雜的分支條件和外部依賴，而 PALM 透過程式分析找出分支條件，將其轉化為路徑約束，並結合上下文資訊，引導 LLM 生成更有效的測試案例。實驗結果顯示，PALM 在數小時內即可大幅提升測試覆蓋率，部分專案甚至超過 50%，平均覆蓋率達到 75.77%，與人工測試相當。已提交的測試案例中，絕大多數已被接受，證明了此方法在自動化測試領域的潛力。", "applications": ["**自動修復程式錯誤：** 想像一下，程式碼寫完後，PALM 就像一個超級偵錯員，自動產生測試案例，找出潛在的錯誤，並協助開發者快速修復，減少程式崩潰的機會。", "**提升軟體品質：** 銀行、醫療等對軟體穩定性要求極高的產業，可以利用 PALM 確保程式碼的品質，避免因程式錯誤造成重大損失。", "**加速開發流程：** 開發者不再需要花費大量時間編寫測試案例，PALM 可以自動生成，讓他們更專注於核心功能的開發，加速產品上市時間。"], "pitch": "各位投資人，我們正處於AI驅動軟體開發的革命性時刻！PALM不僅僅是個測試工具，它是軟體品質保證的未來。想想看，全球每年因軟體錯誤造成的損失高達數千億美元，而PALM能有效降低這些損失，為企業節省巨額成本。更重要的是，隨著AI技術的不斷發展，PALM的自我學習能力將持續提升，測試覆蓋率和效率將遠超人工。我們預見，PALM將成為所有軟體開發團隊的標配，市場潛力無限。現在投資PALM，就是投資軟體開發的未來，讓我們一起打造更可靠、更穩定的數位世界！", "audio": "docs/data/audios/2506.09002v1.wav"}
{"query": "Foundation Model", "id": "2506.08982v1", "url": "http://arxiv.org/abs/2506.08982v1", "title": "On Finetuning Tabular Foundation Models", "summary": "Foundation models are an emerging research direction in tabular deep\nlearning. Notably, TabPFNv2 recently claimed superior performance over\ntraditional GBDT-based methods on small-scale datasets using an in-context\nlearning paradigm, which does not adapt model parameters to target datasets.\nHowever, the optimal finetuning approach for adapting tabular foundational\nmodels, and how this adaptation reshapes their internal mechanisms, remains\nunderexplored. While prior works studied finetuning for earlier foundational\nmodels, inconsistent findings and TabPFNv2's unique architecture necessitate\nfresh investigation. To address these questions, we first systematically\nevaluate various finetuning strategies on diverse datasets. Our findings\nestablish full finetuning as the most practical solution for TabPFNv2 in terms\nof time-efficiency and effectiveness. We then investigate how finetuning alters\nTabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.\nWe reveal that the success of finetuning stems from the fact that after\ngradient-based adaptation, the dot products of the query-representations of\ntest objects and the key-representations of in-context training objects more\naccurately reflect their target similarity. This improved similarity allows\nfinetuned TabPFNv2 to better approximate target dependency by appropriately\nweighting relevant in-context samples, improving the retrieval-based prediction\nlogic. From the practical perspective, we managed to finetune TabPFNv2 on\ndatasets with up to 50K objects, observing performance improvements on almost\nall tasks. More precisely, on academic datasets with I.I.D. splits, finetuning\nallows TabPFNv2 to achieve state-of-the-art results, while on datasets with\ngradual temporal shifts and rich feature sets, TabPFNv2 is less stable and\nprior methods remain better.", "authors": ["Ivan Rubachev", "Akim Kotelnikov", "Nikolay Kartashev"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:30:59.879609", "title_zh": "表格型資料基礎模型的微調研究", "summary_zh": "本研究探討如何微調表格型資料的基礎模型，特別是TabPFNv2。TabPFNv2原本擅長利用少量資料進行學習，無需調整模型參數。然而，如何有效微調此類模型，以及微調如何改變其內部運作機制，仍是未解之謎。研究發現，完整微調是TabPFNv2最有效率且效果最好的方法。微調後的模型，能更精準地判斷測試資料與訓練資料的相似度，進而提升預測準確性。實驗證明，微調後的TabPFNv2在包含五萬筆資料的數據集上，也能有效提升效能，在學術資料集上甚至達到最先進的水平。這項研究為表格型資料基礎模型的應用開闢了新的可能性。", "applications": ["**個人貸款風險評估：** 銀行或金融機構可以利用微調後的TabPFNv2，根據客戶的少量歷史資料（例如：年齡、收入、信用評分）更精準地預測貸款違約風險，快速決定是否核准貸款，降低壞帳率。", "**疾病診斷輔助：** 醫生可以使用微調後的TabPFNv2，基於少量的病患數據（例如：症狀、檢驗報告）輔助診斷罕見疾病，提高診斷效率和準確性，避免延誤治療。", "**客戶流失預測：** 電信公司或零售業者可以利用微調後的TabPFNv2，分析少量的客戶行為數據（例如：購買紀錄、瀏覽習慣）預測客戶是否可能流失，及早採取挽留措施，提升客戶忠誠度。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，能徹底改變表格型資料的應用方式。想像一下，在數據量有限的情況下，也能擁有媲美甚至超越大型模型的精準度。這就是微調後的TabPFNv2的潛力！\n\n傳統機器學習需要大量數據才能訓練出可靠的模型，但現實世界中，許多領域都面臨數據稀缺的挑戰。我們的技術能讓企業在小數據集上快速建立精準的預測模型，大幅降低數據收集和處理的成本。例如，新藥開發、罕見疾病診斷、客製化行銷等領域，都將因此受益。\n\n更重要的是，我們的技術具有高度的可擴展性。未來，我們計劃將此技術應用於更廣泛的領域，例如：金融風險管理、智能製造、智慧城市等。我們相信，微調後的TabPFNv2將成為企業決策的強大助力，為投資者帶來豐厚的回報。現在投資，您將站在AI革命的最前沿，共同開創無限可能！", "audio": "docs/data/audios/2506.08982v1.wav"}
{"query": "Diffusion Model", "id": "2506.08809v1", "url": "http://arxiv.org/abs/2506.08809v1", "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference", "summary": "High-resolution sinogram inpainting is essential for computed tomography\nreconstruction, as missing high-frequency projections can lead to visible\nartifacts and diagnostic errors. Diffusion models are well-suited for this task\ndue to their robustness and detail-preserving capabilities, but their\napplication to high-resolution inputs is limited by excessive memory and\ncomputational demands. To address this limitation, we propose HiSin, a novel\ndiffusion based framework for efficient sinogram inpainting via\nresolution-guided progressive inference. It progressively extracts global\nstructure at low resolution and defers high-resolution inference to small\npatches, enabling memory-efficient inpainting. It further incorporates\nfrequency-aware patch skipping and structure-adaptive step allocation to reduce\nredundant computation. Experimental results show that HiSin reduces peak memory\nusage by up to 31.25% and inference time by up to 18.15%, and maintains\ninpainting accuracy across datasets, resolutions, and mask conditions.", "authors": ["Jiaze E", "Srutarshi Banerjee", "Tekin Bicer", "Guannan Wang", "Yanfu Zhang", "Bin Ren"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:32:39.355347", "title_zh": "HiSin：透過解析度導向的漸進式推論實現高效能高解析度正弦圖修復", "summary_zh": "在電腦斷層掃描中，高解析度正弦圖修復至關重要，因為遺失的高頻投影可能導致明顯的偽影和診斷錯誤。擴散模型擅長此任務，但其在高解析度輸入上的應用受限於過多的記憶體和計算需求。我們提出HiSin，一種基於擴散的新穎框架，透過解析度導向的漸進式推論實現高效的正弦圖修復。它在低解析度下逐步提取全局結構，並將高解析度推論延遲到小塊區域，從而實現記憶體效率高的修復。此外，它還整合了頻率感知補丁跳過和結構自適應步驟分配，以減少冗餘計算。實驗結果表明，HiSin將峰值記憶體使用量減少了高達31.25%，推論時間減少了高達18.15%，並在資料集、解析度和遮罩條件下保持了修復準確性。", "applications": ["想像一下，醫院的CT掃描儀經常會因為各種原因產生一些雜訊或缺失數據，導致影像不清晰，影響醫生診斷。HiSin就像一個影像魔術師，能自動修復這些缺失的部分，讓醫生看到更清晰、更準確的影像，減少誤診的風險。", "考古學家在挖掘古代文物時，常常會使用X光或CT掃描來觀察文物內部的結構。如果掃描結果不完整或有干擾，HiSin就能派上用場，幫助他們重建清晰的文物內部影像，從而更好地研究古代文明。", "機場安檢人員使用X光機檢查行李時，有時會遇到影像模糊或遮蔽的情況。HiSin可以輔助安檢系統，修復這些影像，提高安檢效率，同時也能更準確地識別潛在的危險物品。"], "pitch": "各位投資人，我們正處於醫療影像AI革命的風口浪尖！HiSin不僅僅是一個修復演算法，它代表著更高效、更精準的醫療診斷的未來。現有的高解析度影像處理方案耗時且資源密集，嚴重限制了臨床應用。HiSin透過獨特的解析度導向漸進式推論，大幅降低了計算成本，讓高解析度影像修復成為可能，潛力巨大。想像一下，未來，我們的技術可以應用於自動駕駛的雷達信號處理、衛星影像的清晰化，甚至是在藝術品修復領域大放異彩！這是一個數十億美元的市場，而HiSin有機會成為這個市場的領導者。我們需要您的投資，一起將這項技術推向世界，為人類健康和科技進步做出貢獻！", "audio": "docs/data/audios/2506.08809v1.wav"}
{"query": "AI", "id": "2506.08998v1", "url": "http://arxiv.org/abs/2506.08998v1", "title": "On Monotonicity in AI Alignment", "summary": "Comparison-based preference learning has become central to the alignment of\nAI models with human preferences. However, these methods may behave\ncounterintuitively. After empirically observing that, when accounting for a\npreference for response $y$ over $z$, the model may actually decrease the\nprobability (and reward) of generating $y$ (an observation also made by\nothers), this paper investigates the root causes of (non) monotonicity, for a\ngeneral comparison-based preference learning framework that subsumes Direct\nPreference Optimization (DPO), Generalized Preference Optimization (GPO) and\nGeneralized Bradley-Terry (GBT). Under mild assumptions, we prove that such\nmethods still satisfy what we call local pairwise monotonicity. We also provide\na bouquet of formalizations of monotonicity, and identify sufficient conditions\nfor their guarantee, thereby providing a toolbox to evaluate how prone learning\nmodels are to monotonicity violations. These results clarify the limitations of\ncurrent methods and provide guidance for developing more trustworthy preference\nlearning algorithms.", "authors": ["Gilles Bareilles", "Julien Fageot", "Lê-Nguyên Hoang", "Peva Blanchard", "Wassim Bouaziz", "Sébastien Rouault", "El-Mahdi El-Mhamdi"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:53:57.472960", "title_zh": "論AI對齊中的單調性", "summary_zh": "基於比較的偏好學習已成為將AI模型與人類偏好對齊的核心方法。然而，這些方法有時會產生反直覺的行為。研究發現，當模型考慮到對回應y的偏好高於z時，實際上可能降低生成y的機率（和獎勵）。本研究深入探討了非單調性的根本原因，針對包含直接偏好優化（DPO）、廣義偏好優化（GPO）和廣義布拉德利-特里（GBT）的通用比較偏好學習框架。在溫和的假設下，證明了這些方法仍然滿足我們稱之為局部成對單調性的性質。此外，我們提供了一系列單調性的形式化描述，並確定了保證它們的充分條件，從而提供了一個工具箱來評估學習模型違反單調性的可能性。這些結果闡明了當前方法的局限性，並為開發更值得信賴的偏好學習算法提供了指導。", "applications": ["**個人化推薦系統：** 想像一下，你告訴AI你比較喜歡A餐廳勝過B餐廳，但系統之後卻一直推薦類似B餐廳的選項。這個研究能幫助改善推薦系統，確保它們真的會記住你的偏好，並推薦你更喜歡的內容。", "**自動駕駛：** 如果你告訴自動駕駛系統你比較喜歡平穩的駕駛風格，系統卻突然開始急加速或急煞車，你會覺得很奇怪也很危險。這個研究有助於確保AI系統的行為始終如一，符合你的偏好，提高安全性。", "**AI助理：** 當你告訴AI助理你喜歡某種音樂類型，結果它卻開始播放完全相反的音樂，這會讓人感到困惑。這個研究可以幫助AI助理更好地理解和記住你的偏好，提供更個性化的服務。"], "pitch": "各位創投先進，我們正在解決AI領域一個至關重要的問題：AI對齊。目前，AI系統在學習人類偏好時，經常出現「說一套做一套」的情況，導致用戶體驗不佳，甚至產生安全隱患。我們的研究成果，揭示了這種非單調性行為的根本原因，並提供了一套解決方案，讓AI系統真正理解並遵循人類的偏好。想像一下，一個能夠完全理解使用者意圖的AI，在醫療診斷、金融投資、教育輔導等領域，將會釋放出多麼巨大的商業價值！我們不僅僅是提供一個技術方案，更是打造一個更值得信賴、更人性化的AI未來。現在投資我們，您將站在AI對齊的最前沿，共同引領下一代AI技術的發展，掌握未來AI市場的制高點！未來，我們甚至可以將此技術應用於開發『讀心術』AI，透過分析人類微表情與生理數據，精準預測人類需求，打造劃時代的AI產品與服務，其商業潛力無可限量！", "audio": "docs/data/audios/2506.08998v1.wav"}
{"query": "Foundation Model", "id": "2506.08955v1", "url": "http://arxiv.org/abs/2506.08955v1", "title": "Segment Concealed Objects with Incomplete Supervision", "summary": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves\nsegmenting objects that seamlessly blend into their surrounding environments,\nutilizing incompletely annotated data, such as weak and semi-annotations, for\nmodel training. This task remains highly challenging due to (1) the limited\nsupervision provided by the incompletely annotated training data, and (2) the\ndifficulty of distinguishing concealed objects from the background, which\narises from the intrinsic similarities in concealed scenarios. In this paper,\nwe introduce the first unified method for ISCOS to address these challenges. To\ntackle the issue of incomplete supervision, we propose a unified mean-teacher\nframework, SEE, that leverages the vision foundation model, ``\\emph{Segment\nAnything Model (SAM)}'', to generate pseudo-labels using coarse masks produced\nby the teacher model as prompts. To mitigate the effect of low-quality\nsegmentation masks, we introduce a series of strategies for pseudo-label\ngeneration, storage, and supervision. These strategies aim to produce\ninformative pseudo-labels, store the best pseudo-labels generated, and select\nthe most reliable components to guide the student model, thereby ensuring\nrobust network training. Additionally, to tackle the issue of intrinsic\nsimilarity, we design a hybrid-granularity feature grouping module that groups\nfeatures at different granularities and aggregates these results. By clustering\nsimilar features, this module promotes segmentation coherence, facilitating\nmore complete segmentation for both single-object and multiple-object images.\nWe validate the effectiveness of our approach across multiple ISCOS tasks, and\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancing\nthe performance of existing models.", "authors": ["Chunming He", "Kai Li", "Yachao Zhang", "Ziyun Yang", "Youwei Pang", "Longxiang Tang", "Chengyu Fang", "Yulun Zhang", "Linghe Kong", "Xiu Li", "Sina Farsiu"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:55:36.593228", "title_zh": "使用不完整監督分割隱藏物體", "summary_zh": "本研究提出一種新的方法，用於在只有不完整標註資料的情況下，分割那些與周圍環境無縫融合的隱藏物體。由於標註資料不完整，以及隱藏物體與背景高度相似，這項任務極具挑戰性。我們提出名為SEE的統一平均教師框架，利用視覺基礎模型SAM生成偽標籤，並設計一系列策略來生成、儲存和監督這些偽標籤，以確保穩健的網路訓練。此外，我們設計了一種混合粒度特徵分組模組，通過聚類相似特徵，促進分割一致性，進而更完整地分割單個或多個物體。實驗結果表明，我們的模型在多個隱藏物體分割任務上都達到了最先進的性能，並且可以作為一個隨插即用的解決方案，提升現有模型的效能。", "applications": ["**智慧安防：** 在監視器畫面中，即使嫌犯試圖隱藏身形融入背景，這項技術也能準確識別並追蹤，提高破案率。", "**醫療影像分析：** 醫生可以利用這項技術，在X光片或MRI影像中更清楚地辨識出隱藏在組織中的微小腫瘤，及早發現並治療。", "**自動駕駛：** 在惡劣天氣或光線不足的情況下，車載鏡頭可能難以辨識道路上的行人或障礙物，這項技術能幫助車輛更準確地感知周圍環境，提升行車安全。"], "pitch": "各位投資人，想像一下，一個能看穿偽裝的世界！我們的技術正在開啟這個可能性。現今的AI視覺辨識在隱藏物體的辨識上仍然存在巨大盲點，這不僅是技術挑戰，更是潛在的巨大商機。我們的『不完整監督隱藏物體分割』技術，能讓機器在近乎無監督的情況下，學習辨識隱藏的目標，這代表什麼？\n\n首先，安防產業將迎來革命。想想機場安檢、邊境管制，甚至是商場防盜，我們的技術能有效提升安全等級，減少犯罪發生。\n\n其次，醫療診斷將更加精準。早期癌症的檢測往往仰賴醫生肉眼判斷，我們的技術能輔助醫生發現微小病灶，大幅提升治癒率。\n\n更重要的是，自動駕駛將更加安全可靠。在惡劣環境下，我們的技術能幫助汽車『看』得更清楚，減少事故發生。\n\n這項技術的應用範圍遠不止於此。想像一下，軍事偵察、工業瑕疵檢測、甚至是考古研究，都能因為我們的技術而取得突破性進展。\n\n我們不僅僅是在開發一個演算法，我們正在打造一個全新的視覺感知平台。我們預計在未來五年內，將這項技術推廣到全球市場，與各產業龍頭合作，共同打造一個更安全、更智慧的世界。現在加入我們，您將成為這場視覺革命的先驅！", "audio": "docs/data/audios/2506.08955v1.wav"}
{"query": "Diffusion Model", "id": "2506.08796v1", "url": "http://arxiv.org/abs/2506.08796v1", "title": "Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling", "summary": "Recently, the rectified flow (RF) has emerged as the new state-of-the-art\namong flow-based diffusion models due to its high efficiency advantage in\nstraight path sampling, especially with the amazing images generated by a\nseries of RF models such as Flux 1.0 and SD 3.0. Although a straight-line\nconnection between the noisy and natural data distributions is intuitive, fast,\nand easy to optimize, it still inevitably leads to: 1) Diversity concerns,\nwhich arise since straight-line paths only cover a fairly restricted sampling\nspace. 2) Multi-scale noise modeling concerns, since the straight line flow\nonly needs to optimize the constant velocity field $\\bm v$ between the two\ndistributions $\\bm\\pi_0$ and $\\bm\\pi_1$. In this work, we present\nDiscretized-RF, a new family of rectified flow (also called momentum flow\nmodels since they refer to the previous velocity component and the random\nvelocity component in each diffusion step), which discretizes the straight path\ninto a series of variable velocity field sub-paths (namely ``momentum fields'')\nto expand the search space, especially when close to the distribution\n$p_\\text{noise}$. Different from the previous case where noise is directly\nsuperimposed on $\\bm x$, we introduce noise on the velocity $\\bm v$ of the\nsub-path to change its direction in order to improve the diversity and\nmulti-scale noise modeling abilities. Experimental results on several\nrepresentative datasets demonstrate that learning momentum flow matching by\nsampling random velocity fields will produce trajectories that are both diverse\nand efficient, and can consistently generate high-quality and diverse results.\nCode is available at https://github.com/liuruixun/momentum-fm.", "authors": ["Zhiyuan Ma", "Ruixun Liu", "Sixian Liu", "Jianjun Li", "Bowen Zhou"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:56:45.617158", "title_zh": "流動多樣且高效：透過隨機速度場採樣學習動量流匹配", "summary_zh": "這項研究提出一種新的修正流模型，稱為Discretized-RF，它將傳統的直線路徑分割成一系列變速場子路徑，擴展搜尋空間，尤其是在接近雜訊分佈時。不同於直接在資料上疊加雜訊，此方法在子路徑的速度上引入雜訊，改變其方向，從而提升多樣性和多尺度雜訊建模能力。實驗結果表明，透過採樣隨機速度場學習動量流匹配，可以產生多樣且高效的軌跡，並能持續生成高品質且多樣化的結果。簡單來說，這項技術讓AI生成圖像更豐富、更真實，也更有效率。", "applications": ["遊戲開發：快速生成多樣化的遊戲場景和角色，節省美術設計時間，並提供更豐富的遊戲體驗。", "影視特效：高效創建逼真的視覺特效，例如自然災害、奇幻生物等，降低製作成本，提升視覺衝擊力。", "服裝設計：設計師可以快速生成各種服裝款式和紋理，激發靈感，並根據客戶需求進行個性化定制。"], "pitch": "各位創投先進，想像一下，未來AI不再只是複製現有圖像，而是能創造出前所未見的藝術品、設計出獨一無二的產品！我們的Discretized-RF技術，突破了傳統AI圖像生成的限制，讓AI更像一位真正的藝術家，擁有無限的創意和想像力。這項技術不僅能大幅提升圖像生成效率，降低成本，更能開創全新的商業模式。例如，我們可以打造一個AI設計平台，讓使用者輕鬆生成個性化的產品設計、藝術作品，甚至虛擬世界的場景和角色。未來，每個企業、每個個人都能擁有自己的AI設計師，創造出無限的商業價值。現在投資，您將站在AI圖像革命的最前沿，共同見證AI創造力的無限可能！", "audio": "docs/data/audios/2506.08796v1.wav"}
