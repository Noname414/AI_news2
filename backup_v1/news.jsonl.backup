{"id": "2506.05341v1", "query": "Large Language Model", "url": "http://arxiv.org/abs/2506.05341v1", "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning", "summary": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility.", "authors": ["Xingjian Ran", "Yixuan Li", "Linning Xu", "Mulin Yu", "Bo Dai"], "published_date": "2025-06-05", "title_zh": "基於空間推理的3D室內場景合成之直接數值佈局生成", "summary_zh": "這項技術名為DirectLayout，利用大型語言模型（LLMs）直接從文字描述生成3D室內佈局。它將生成過程分為鳥瞰圖佈局生成、提升至3D空間以及物件放置優化三個階段。透過Chain-of-Thought (CoT)激活和CoT-Grounded Generative Layout Reward，DirectLayout能夠進行更精確的空間推理和規劃。此外，它採用迭代資產-佈局對齊方法，解決物件與佈局不匹配的問題。實驗證明，DirectLayout在語義一致性、泛化能力和物理合理性方面表現出色，能根據使用者指令生成開放詞彙且精準的室內場景。", "applications": ["想像一下：你想要重新裝潢你的客廳，但不知道該如何擺放家具。你可以用文字描述你的想法，例如「一張沙發放在窗戶旁邊，電視櫃放在沙發對面，中間放一張咖啡桌」，DirectLayout就能自動生成一個3D模型，讓你預覽裝潢效果。", "遊戲開發者可以利用DirectLayout快速生成各種風格的室內場景，節省大量手動設計的時間和精力。例如，描述「一個陰森的古堡房間，有破碎的鏡子和蜘蛛網」，就能快速產生符合需求的場景。", "房地產公司可以用DirectLayout為潛在買家展示房屋的各種裝潢方案。買家可以根據自己的喜好，用文字描述想要的風格，例如「現代簡約風格的臥室，有落地窗和一張大床」，就能即時看到效果，提升購買意願。"], "pitch": "各位投資人，我們帶來的是DirectLayout，一個劃時代的3D室內場景生成技術。它不僅解決了現有技術在佈局生成方面的瓶頸，更開啟了無限的商業可能。想像一下，未來的家居設計、遊戲開發、房地產銷售，甚至元宇宙的場景構建，都將因為DirectLayout而變得更加高效和個性化。我們相信，DirectLayout將成為這些產業的基礎設施，帶來巨大的市場需求和回報。現在正是投資DirectLayout的最佳時機，讓我們一起打造一個更具創意和效率的未來！", "audio": "audios\\2506.05341v1.mp3", "timestamp": "2025-06-06T08:48:02.586013"}
{"id": "2506.05334v1", "query": "AI", "url": "http://arxiv.org/abs/2506.05334v1", "title": "Search Arena: Analyzing Search-Augmented LLMs", "summary": "Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.", "authors": ["Mihran Miroyan", "Tsung-Han Wu", "Logan King", "Tianle Li", "Jiayi Pan", "Xinyan Hu", "Wei-Lin Chiang", "Anastasios N. Angelopoulos", "Trevor Darrell", "Narges Norouzi", "Joseph E. Gonzalez"], "published_date": "2025-06-05", "title_zh": "搜尋競技場：分析搜尋增強型大型語言模型", "summary_zh": "本研究推出一個名為「搜尋競技場」的大規模人類偏好資料集，包含超過24,000個多輪使用者與搜尋增強型大型語言模型的互動。分析發現，使用者偏好會受到引用數量的影響，即使引用的內容並未直接支持聲稱的主張，這揭示了感知可信度與實際可信度之間的差距。此外，使用者對不同引用來源的偏好也不同，通常更喜歡社群驅動的平台。研究進一步發現，在非搜尋環境中，網路搜尋不會降低性能，甚至可能提高性能；但在搜尋環境中，如果僅依賴模型的參數知識，搜尋品質會受到顯著影響。此資料集已開源，旨在支持未來相關研究。", "applications": ["**家庭作業輔導：** 小朋友寫作業遇到問題，不再只是問爸媽，直接問AI！AI會自動搜尋網路資訊，整理出最正確、最新的答案，還會列出資料來源，讓家長放心。", "**旅遊行程規劃：** 想去日本玩，但不知道怎麼安排行程？告訴AI你的喜好，它會搜尋最新的旅遊資訊、交通方式、餐廳評價，自動生成一份客製化的旅遊行程表，連訂房連結都幫你找好！", "**醫療資訊查詢：** 感冒症狀持續不舒服，想了解相關資訊？別再自己亂查網路了！AI會搜尋可信賴的醫療網站、研究報告，整理出正確的資訊，讓你更了解自己的身體狀況，並判斷是否需要就醫。"], "pitch": "各位投資人，想像一下，未來每個人都擁有一個超級助理，它不僅能回答你的問題，還能像一位專業研究員一樣，快速搜尋、整理網路上的海量資訊，提供最可靠的答案。這就是搜尋增強型大型語言模型（Search-Augmented LLMs）的潛力！我們的研究「搜尋競技場」正是這個領域的基石。我們創建了一個龐大、真實的資料集，可以幫助開發者打造更聰明、更值得信賴的AI。這個技術不僅能應用在教育、旅遊、醫療等領域，更能顛覆傳統的搜尋引擎，創造全新的商業模式。試想，一個能根據用戶提問，直接提供解決方案的搜尋引擎，其價值將遠遠超過現在的關鍵字搜尋。我們相信，這個技術將會成為AI發展的下一個爆發點，現在投資，您將站在浪潮的最前端！未來，我們將進一步開發更精準的演算法，解決AI的偏見問題，讓AI真正成為人類的助力，而不是阻力。這是一個充滿機會的市場，加入我們，一起開創AI的新紀元！", "audio": "audios\\2506.05334v1.mp3", "timestamp": "2025-06-06T08:47:52.620512"}
{"id": "2506.05333v1", "query": "AI", "url": "http://arxiv.org/abs/2506.05333v1", "title": "Kinetics: Rethinking Test-Time Scaling Laws", "summary": "We rethink test-time scaling laws from a practical efficiency perspective,\nrevealing that the effectiveness of smaller models is significantly\noverestimated. Prior work, grounded in compute-optimality, overlooks critical\nmemory access bottlenecks introduced by inference-time strategies (e.g.,\nBest-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to\n32B parameters, reveals a new Kinetics Scaling Law that better guides resource\nallocation by incorporating both computation and memory access costs. Kinetics\nScaling Law suggests that test-time compute is more effective when used on\nmodels above a threshold than smaller ones. A key reason is that in TTS,\nattention, rather than parameter count, emerges as the dominant cost factor.\nMotivated by this, we propose a new scaling paradigm centered on sparse\nattention, which lowers per-token cost and enables longer generations and more\nparallel samples within the same resource budget. Empirically, we show that\nsparse attention models consistently outperform dense counterparts, achieving\nover 60 points gains in low-cost regimes and over 5 points gains in high-cost\nregimes for problem-solving accuracy on AIME, encompassing evaluations on\nstate-of-the-art MoEs. These results suggest that sparse attention is essential\nfor realizing the full potential of test-time scaling because, unlike training,\nwhere parameter scaling saturates, test-time accuracy continues to improve\nthrough increased generation. The code is available at\nhttps://github.com/Infini-AI-Lab/Kinetics.", "authors": ["Ranajoy Sadhukhan", "Zhuoming Chen", "Haizhong Zheng", "Yang Zhou", "Emma Strubell", "Beidi Chen"], "published_date": "2025-06-05", "title_zh": "動能學：重新思考測試時期的擴展法則", "summary_zh": "本研究從實際效率的角度重新審視了測試時期的擴展法則，發現小型模型的有效性被嚴重高估。以往基於計算最佳化的研究忽略了推論時策略（如Best-of-N、長CoT）引入的關鍵記憶體存取瓶頸。我們全面的分析，涵蓋了從0.6B到32B參數的模型，揭示了一種新的動能學擴展法則，通過結合計算和記憶體存取成本，更好地指導資源分配。動能學擴展法則表明，在測試時期，將計算資源用於高於閾值的模型比小型模型更有效。關鍵原因是，在TTS中，注意力機制，而不是參數數量，成為主要的成本因素。為此，我們提出了一種以稀疏注意力為中心的新擴展範例，降低了每個token的成本，並在相同的資源預算內實現更長的生成和更多的並行樣本。實驗結果表明，稀疏注意力模型始終優於密集模型，在低成本情況下，AIME上的問題解決準確度提高了60多分，在高成本情況下提高了5分以上，包括對最先進的MoE的評估。這些結果表明，稀疏注意力對於充分發揮測試時期擴展的潛力至關重要，因為與參數擴展飽和的訓練不同，測試時期的準確性會通過增加生成而持續提高。", "applications": ["**智慧客服：** 想像一下，未來的客服機器人能更快速、更準確地回答你的問題，甚至能理解你的情緒。這項技術就像是幫客服機器人升級了更強大的大腦，讓它們在有限的資源下，也能處理更複雜的對話，提供更人性化的服務，減少等待時間，提高客戶滿意度。", "**即時翻譯：** 出國旅遊再也不怕語言不通！這項技術能讓翻譯軟體更有效率地處理不同語言的轉換，即使在網路訊號不佳或裝置效能有限的情況下，也能提供流暢、準確的即時翻譯。就像是隨身攜帶一位超強翻譯官，隨時隨地幫你溝通無礙。", "**個人化學習：** 每個人的學習方式都不同。這項技術可以應用於智慧學習平台，根據學生的學習進度和理解程度，更精準地調整教材和練習，提供客製化的學習內容。就像是擁有了一位專屬的AI家教，能針對你的弱點加強輔導，讓你學習更有效率，成績突飛猛進。"], "pitch": "各位創投先進，我們正在重新定義AI的未來！傳統的AI模型追求的是更大的參數規模，但我們發現，更聰明的資源分配，尤其是針對記憶體存取和注意力機制的優化，才是提升效率的關鍵。我們的「動能學擴展法則」和「稀疏注意力」技術，就像是為AI引擎裝上了渦輪增壓，讓它在有限的資源下，爆發出更強大的效能。這意味著更低的運算成本、更快的反應速度，以及更廣泛的應用場景。想像一下，未來的手機、穿戴裝置，甚至物聯網設備，都能搭載強大的AI功能，實現真正的智慧生活。我們相信，這項技術將引領下一代AI革命，帶來巨大的商業價值。現在投資，就是投資AI的未來！我們的目標是打造一個更高效、更普及的AI生態系統，讓每個人都能享受到AI帶來的便利和價值。請加入我們，一起開創AI的新紀元！", "audio": "audios\\2506.05333v1.mp3", "timestamp": "2025-06-06T08:47:41.025503"}
{"id": "2506.05321v1", "query": "Foundation Model", "url": "http://arxiv.org/abs/2506.05321v1", "title": "LSM-2: Learning from Incomplete Wearable Sensor Data", "summary": "Foundation models, a cornerstone of recent advancements in machine learning,\nhave predominantly thrived on complete and well-structured data. Wearable\nsensor data frequently suffers from significant missingness, posing a\nsubstantial challenge for self-supervised learning (SSL) models that typically\nassume complete data inputs. This paper introduces the second generation of\nLarge Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel\nSSL approach that learns robust representations directly from incomplete data\nwithout requiring explicit imputation. AIM's core novelty lies in its use of\nlearnable mask tokens to model both existing (\"inherited\") and artificially\nintroduced missingness, enabling it to robustly handle fragmented real-world\ndata during inference. Pre-trained on an extensive dataset of 40M hours of\nday-long multimodal sensor data, our LSM-2 with AIM achieves the best\nperformance across a diverse range of tasks, including classification,\nregression and generative modeling. Furthermore, LSM-2 with AIM exhibits\nsuperior scaling performance, and critically, maintains high performance even\nunder targeted missingness scenarios, reflecting clinically coherent patterns,\nsuch as the diagnostic value of nighttime biosignals for hypertension\nprediction. This makes AIM a more reliable choice for real-world wearable data\napplications.", "authors": ["Maxwell A. Xu", "Girish Narayanswamy", "Kumar Ayush", "Dimitris Spathis", "Shun Liao", "Shyam A. Tailor", "Ahmed Metwally", "A. Ali Heydari", "Yuwei Zhang", "Jake Garrison", "Samy Abdel-Ghaffar", "Xuhai Xu", "Ken Gu", "Jacob Sunshine", "Ming-Zher Poh", "Yun Liu", "Tim Althoff", "Shrikanth Narayanan", "Pushmeet Kohli", "Mark Malhotra", "Shwetak Patel", "Yuzhe Yang", "James M. Rehg", "Xin Liu", "Daniel McDuff"], "published_date": "2025-06-05", "title_zh": "LSM-2：從不完整的穿戴式感測器資料中學習", "summary_zh": "現今機器學習仰賴大量完整資料，但穿戴式裝置數據常有缺失。本研究推出第二代大型感測模型LSM-2，採用自適應與繼承遮罩(AIM)技術，直接從不完整資料中學習，無需填補。AIM利用可學習的遮罩符號模擬現有和人為缺失，增強模型在真實環境下的穩定性。LSM-2在包含4000萬小時多模態感測器資料集上預訓練，在分類、回歸和生成模型等多項任務中表現最佳。即使面對針對性缺失，LSM-2仍維持高效能，例如夜間生理訊號對高血壓預測的診斷價值。AIM使其成為穿戴式裝置數據應用的可靠選擇。", "applications": ["想像一下，未來你的智慧手錶能更準確地監測你的睡眠品質，即使你睡覺時手錶鬆動了一下，導致數據有些遺失，它仍然可以給你可靠的分析結果，幫助你改善睡眠。", "如果你是一位運動員，穿戴式裝置可以追蹤你的運動表現。即使在比賽過程中，感測器偶爾出現訊號中斷，這項技術也能確保數據的完整性，讓你得到最準確的訓練反饋。", "在醫療照護方面，醫生可以利用穿戴式裝置遠端監測病人的健康狀況。即使病人有時候沒有正確佩戴裝置，導致數據不完整，這項技術也能夠盡可能地還原數據，幫助醫生做出更準確的診斷。"], "pitch": "各位投資人，我們正處於一個穿戴式裝置數據爆炸的時代，但數據不完整的問題嚴重阻礙了AI的發展。LSM-2的AIM技術完美解決了這個痛點，它就像一個數據修復大師，能從殘缺的數據中挖掘出寶藏。想像一下，這項技術可以應用於精準醫療，及早發現疾病風險；可以優化運動訓練，打造更強大的運動員；甚至可以應用於智慧城市，提升公共安全。我們預計，LSM-2將成為穿戴式裝置AI領域的關鍵基礎設施，市場潛力巨大，回報可期！現在加入我們，一起開創穿戴式AI的新紀元！", "audio": "audios\\2506.05321v1.mp3", "timestamp": "2025-06-06T08:48:05.729154"}
{"id": "2506.05263v1", "query": "Foundation Model", "url": "http://arxiv.org/abs/2506.05263v1", "title": "Can Foundation Models Generalise the Presentation Attack Detection Capabilities on ID Cards?", "summary": "Nowadays, one of the main challenges in presentation attack detection (PAD)\non ID cards is obtaining generalisation capabilities for a diversity of\ncountries that are issuing ID cards. Most PAD systems are trained on one, two,\nor three ID documents because of privacy protection concerns. As a result, they\ndo not obtain competitive results for commercial purposes when tested in an\nunknown new ID card country. In this scenario, Foundation Models (FM) trained\non huge datasets can help to improve generalisation capabilities. This work\nintends to improve and benchmark the capabilities of FM and how to use them to\nadapt the generalisation on PAD of ID Documents. Different test protocols were\nused, considering zero-shot and fine-tuning and two different ID card datasets.\nOne private dataset based on Chilean IDs and one open-set based on three ID\ncountries: Finland, Spain, and Slovakia. Our findings indicate that bona fide\nimages are the key to generalisation.", "authors": ["Juan E. Tapia", "Christoph Busch"], "published_date": "2025-06-05", "title_zh": "基礎模型能否泛化身分證件上的呈現攻擊偵測能力？", "summary_zh": "目前身分證件上的呈現攻擊偵測（PAD）主要挑戰在於如何泛化到不同國家發行的身分證件。由於隱私保護考量，大多數PAD系統僅在少數幾個身分證件上訓練，導致在新國家身分證件上的商業應用表現不佳。本研究旨在評估基礎模型（FM）在改善PAD泛化能力方面的潛力。我們使用零樣本和微調等不同測試協議，並採用智利身分證的私有數據集以及包含芬蘭、西班牙和斯洛伐克身分證的開放數據集。研究結果表明，真實（bona fide）圖像對於泛化至關重要。", "applications": ["手機銀行App：透過掃描身分證驗證身份時，能有效辨識偽造的身分證件，防止詐騙集團盜用身份開戶或進行非法交易。", "機場自助通關：自動辨識旅客身分證件真偽，加速通關流程，同時提高安全性，防止不法份子使用假證件入境。", "線上貸款申請：在線上申請貸款時，系統能自動驗證申請人上傳的身分證件是否真實，降低銀行因詐欺造成的損失。"], "pitch": "各位創投先進，我們正站在身份驗證技術的革命性轉折點！想像一下，一個能辨識全球任何國家身分證件真偽的AI系統，其應用範圍涵蓋金融、安保、邊境管制等各個領域。傳統的身份驗證系統需要針對每個國家、甚至每種證件進行獨立訓練，耗時耗力且成本高昂。而我們的技術，基於最先進的基礎模型，只需少量樣本即可快速適應新的證件類型，大幅降低部署成本和時間。未來，我們將進一步開發生物特徵融合技術，結合人臉辨識、指紋辨識等，打造更安全、更便捷的身份驗證解決方案。這不僅僅是一項技術，更是一個價值數十億美元的市場！現在加入我們，共同塑造身份驗證的未來！", "audio": "audios\\2506.05263v1.mp3", "timestamp": "2025-06-06T08:47:49.517191"}
{"id": "2506.05210v1", "query": "Foundation Model", "url": "http://arxiv.org/abs/2506.05210v1", "title": "Towards Vision-Language-Garment Models For Web Knowledge Garment Understanding and Generation", "summary": "Multimodal foundation models have demonstrated strong generalization, yet\ntheir ability to transfer knowledge to specialized domains such as garment\ngeneration remains underexplored. We introduce VLG, a vision-language-garment\nmodel that synthesizes garments from textual descriptions and visual imagery.\nOur experiments assess VLG's zero-shot generalization, investigating its\nability to transfer web-scale reasoning to unseen garment styles and prompts.\nPreliminary results indicate promising transfer capabilities, highlighting the\npotential for multimodal foundation models to adapt effectively to specialized\ndomains like fashion design.", "authors": ["Jan Ackermann", "Kiyohiro Nakayama", "Guandao Yang", "Tong Wu", "Gordon Wetzstein"], "published_date": "2025-06-05", "title_zh": "邁向視覺-語言-服裝模型：用於網路知識服裝理解與生成", "summary_zh": "本研究提出一個名為VLG的視覺-語言-服裝模型，旨在將多模態基礎模型的強大泛化能力應用於服裝生成領域。VLG模型能夠根據文字描述和視覺圖像合成服裝。實驗結果顯示，VLG在零樣本泛化方面表現出良好的潛力，能夠將網路規模的推理能力轉移到未見過的服裝款式和提示詞上。這項研究突顯了多模態基礎模型在時尚設計等專業領域的適應能力，為服裝設計的自動化和個性化提供了新的可能性。", "applications": ["想像一下，你只要用手機拍一張喜歡的衣服照片，然後用文字描述你想要的顏色、材質或細節，App就能自動生成一件完全符合你要求的設計圖，甚至直接下單訂製。", "服裝設計師可以利用這個技術快速生成各種設計草圖，省去大量手繪和建模的時間，大幅提升工作效率，並激發更多創意靈感。", "電商平台可以利用這項技術，讓消費者上傳自己的照片，然後試穿各種款式的衣服，模擬出逼真的穿搭效果，提升購物體驗和購買意願。"], "pitch": "各位投資人，我們正在打造一個革命性的服裝設計引擎，它結合了視覺、語言和服裝知識，能夠讓機器像設計師一樣思考和創造。想像一下，一個AI驅動的服裝設計師，24小時不間斷地產生新的設計，滿足全球消費者對個性化服裝的需求。這不僅僅是一個技術突破，更是一個巨大的市場機會。我們的VLG模型將顛覆傳統的服裝設計和生產模式，降低設計成本，縮短生產週期，實現真正的按需生產。我們預計在未來五年內，VLG將成為服裝設計領域的行業標準，為我們帶來數十億美元的收入。現在加入我們，一起引領服裝設計的未來！", "audio": "audios\\2506.05210v1.mp3", "timestamp": "2025-06-06T08:47:46.458858"}
{"id": "2506.05350v1", "query": "Computer Vision", "url": "http://arxiv.org/abs/2506.05350v1", "title": "Contrastive Flow Matching", "summary": "Unconditional flow-matching trains diffusion models to transport samples from\na source distribution to a target distribution by enforcing that the flows\nbetween sample pairs are unique. However, in conditional settings (e.g.,\nclass-conditioned models), this uniqueness is no longer guaranteed--flows from\ndifferent conditions may overlap, leading to more ambiguous generations. We\nintroduce Contrastive Flow Matching, an extension to the flow matching\nobjective that explicitly enforces uniqueness across all conditional flows,\nenhancing condition separation. Our approach adds a contrastive objective that\nmaximizes dissimilarities between predicted flows from arbitrary sample pairs.\nWe validate Contrastive Flow Matching by conducting extensive experiments\nacross varying model architectures on both class-conditioned (ImageNet-1k) and\ntext-to-image (CC3M) benchmarks. Notably, we find that training models with\nContrastive Flow Matching (1) improves training speed by a factor of up to 9x,\n(2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9\ncompared to training the same models with flow matching. We release our code\nat: https://github.com/gstoica27/DeltaFM.git.", "authors": ["George Stoica", "Vivek Ramanujan", "Xiang Fan", "Ali Farhadi", "Ranjay Krishna", "Judy Hoffman"], "published_date": "2025-06-05", "title_zh": "對比式流匹配", "summary_zh": "本研究提出「對比式流匹配」，旨在解決條件式擴散模型中，不同條件下的流可能重疊，導致生成結果模糊的問題。傳統流匹配訓練擴散模型時，要求樣本對之間的流是唯一的。然而，在像是類別條件或文字生成圖片等條件式模型中，這個唯一性難以保證。對比式流匹配通過增加一個對比目標，最大化任意樣本對預測流之間的不相似性，從而強化條件分離。實驗結果顯示，相較於傳統流匹配，使用對比式流匹配能顯著提升訓練速度，最多可達9倍；所需去噪步驟最多減少5倍；並降低FID指標，最高可達8.9。", "applications": ["AI繪圖App：使用者輸入特定風格或內容描述，App能更快、更精準地生成高品質圖像，例如，輸入「梵谷風格的貓」，就能立即獲得符合要求的畫作。", "醫療影像分析：醫生可以利用AI快速分析X光片或MRI圖像，更精確地識別病灶，例如，輸入「肺部結節」，AI能迅速標記出可疑區域，輔助診斷。", "產品設計自動化：設計師可以輸入產品的功能需求和外觀偏好，AI自動生成多個設計方案，例如，輸入「符合人體工學的辦公椅」，AI能生成多種不同風格的設計圖。"], "pitch": "想像一下，我們正站在AI生成內容的下一個浪潮前沿！傳統的AI圖像生成技術，像是DALL-E或Stable Diffusion，雖然令人驚豔，但在特定條件下的生成效果仍然不夠精準，訓練成本也居高不下。我們的「對比式流匹配」技術，就像是為AI圖像生成引擎裝上了渦輪增壓，讓它在速度、效率和精準度上都實現了飛躍。這意味著更低的算力需求，更快的訓練速度，以及更高品質的生成結果。試想一下，未來遊戲開發者可以利用這項技術，快速生成各種風格的遊戲場景和角色；廣告公司可以根據客戶需求，即時生成客製化的廣告素材；甚至是個人使用者，也能輕鬆創作獨一無二的藝術作品。更重要的是，這項技術不僅僅局限於圖像生成，它還可以應用於語音合成、影片生成等更廣泛的領域。我們相信，「對比式流匹配」將成為AI生成內容領域的基礎設施，徹底改變內容創作的方式，並為各行各業帶來巨大的商業價值。現在投資，您將成為這場革命的領跑者！", "audio": "audios\\2506.05350v1.mp3", "timestamp": "2025-06-06T08:47:51.552197"}
{"id": "2506.05340v1", "query": "Diffusion Model", "url": "http://arxiv.org/abs/2506.05340v1", "title": "Exploring Diffusion Transformer Designs via Grafting", "summary": "Designing model architectures requires decisions such as selecting operators\n(e.g., attention, convolution) and configurations (e.g., depth, width).\nHowever, evaluating the impact of these decisions on model quality requires\ncostly pretraining, limiting architectural investigation. Inspired by how new\nsoftware is built on existing code, we ask: can new architecture designs be\nstudied using pretrained models? To this end, we present grafting, a simple\napproach for editing pretrained diffusion transformers (DiTs) to materialize\nnew architectures under small compute budgets. Informed by our analysis of\nactivation behavior and attention locality, we construct a testbed based on the\nDiT-XL/2 design to study the impact of grafting on model quality. Using this\ntestbed, we develop a family of hybrid designs via grafting: replacing softmax\nattention with gated convolution, local attention, and linear attention, and\nreplacing MLPs with variable expansion ratio and convolutional variants.\nNotably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for\nDiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model\n(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval\nscore. Finally, we present a case study that restructures DiT-XL/2 by\nconverting every pair of sequential transformer blocks into parallel blocks via\ngrafting. This reduces model depth by 2x and yields better quality (FID: 2.77)\nthan other models of comparable depth. Together, we show that new diffusion\nmodel designs can be explored by grafting pretrained DiTs, with edits ranging\nfrom operator replacement to architecture restructuring. Code and grafted\nmodels: https://grafting.stanford.edu", "authors": ["Keshigeyan Chandrasegaran", "Michael Poli", "Daniel Y. Fu", "Dongjun Kim", "Lea M. Hadzic", "Manling Li", "Agrim Gupta", "Stefano Massaroli", "Azalia Mirhoseini", "Juan Carlos Niebles", "Stefano Ermon", "Li Fei-Fei"], "published_date": "2025-06-05", "title_zh": "透過嫁接探索擴散轉換器設計", "summary_zh": "本研究提出一種名為「嫁接」的創新方法，能在預訓練的擴散轉換器（DiT）模型上，以極低的運算成本探索新的架構設計。透過分析激活行為和注意力局部性，研究團隊建立了一個測試平台，並利用嫁接技術開發了一系列混合設計，例如將softmax注意力替換為門控卷積、局部注意力或線性注意力，以及替換具有可變擴展比率和卷積變體的MLP。實驗結果顯示，許多混合設計在僅使用不到2%的預訓練運算資源下，便能達到良好的品質。此技術還能加速文本到圖像模型的生成速度，並實現架構重組，在降低模型深度的同時提升模型品質。嫁接技術為探索新型擴散模型設計提供了一種高效且經濟的方法。", "applications": ["**個人化圖像生成：**想像一下，你可以根據自己的喜好，快速客製化生成獨特的圖像。例如，設計獨一無二的生日賀卡、個人頭像，甚至創作專屬的藝術作品，而無需耗費大量時間和金錢學習專業繪圖軟體。", "**AI藝術風格轉換：**將你喜歡的照片轉換成各種藝術風格，例如梵谷的星夜、莫內的印象派風格等。過去需要耗費大量算力，現在透過嫁接技術，可以更快速、更有效地實現，讓每個人都能輕鬆成為藝術家。", "**加速產品設計迭代：**設計師可以利用這項技術，快速測試不同的設計方案。例如，修改汽車外觀、室內裝潢風格，或者生成不同材質的服裝效果圖，加速設計流程，縮短產品上市時間。"], "pitch": "各位投資人，我們正在革新AI圖像生成領域！傳統模型訓練耗時耗力，動輒數百萬美元。我們的「嫁接」技術，就像軟體工程的模組化設計，能讓AI模型像樂高積木一樣快速組裝、迭代。想像一下，我們可以為客戶提供客製化的AI模型，滿足他們在廣告、遊戲、電商等領域的特定需求，而成本僅為傳統方法的零頭！這不僅大幅降低了開發門檻，更開創了全新的商業模式。我們預計，未來AI模型將不再是昂貴的奢侈品，而是像App一樣普及。我們的技術將成為AI圖像生成領域的「App Store」，讓每個人都能輕鬆打造屬於自己的AI創意應用。現在加入我們，共同開創AI圖像生成的新紀元！", "audio": "audios\\2506.05340v1.mp3", "timestamp": "2025-06-06T08:48:14.508438"}
{"id": "2506.05231v1", "query": "Diffusion Model", "url": "http://arxiv.org/abs/2506.05231v1", "title": "Progressive Tempering Sampler with Diffusion", "summary": "Recent research has focused on designing neural samplers that amortize the\nprocess of sampling from unnormalized densities. However, despite significant\nadvancements, they still fall short of the state-of-the-art MCMC approach,\nParallel Tempering (PT), when it comes to the efficiency of target evaluations.\nOn the other hand, unlike a well-trained neural sampler, PT yields only\ndependent samples and needs to be rerun -- at considerable computational cost\n-- whenever new samples are required. To address these weaknesses, we propose\nthe Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion\nmodels sequentially across temperatures, leveraging the advantages of PT to\nimprove the training of neural samplers. We also introduce a novel method to\ncombine high-temperature diffusion models to generate approximate\nlower-temperature samples, which are minimally refined using MCMC and used to\ntrain the next diffusion model. PTSD enables efficient reuse of sample\ninformation across temperature levels while generating well-mixed, uncorrelated\nsamples. Our method significantly improves target evaluation efficiency,\noutperforming diffusion-based neural samplers.", "authors": ["Severi Rissanen", "RuiKang OuYang", "Jiajun He", "Wenlin Chen", "Markus Heinonen", "Arno Solin", "José Miguel Hernández-Lobato"], "published_date": "2025-06-05", "title_zh": "基於擴散的漸進退火採樣器", "summary_zh": "近年研究集中於設計神經採樣器，以加速從未正規化密度中採樣的過程。儘管有顯著進展，但在目標評估效率方面，它們仍不如最先進的MCMC方法——並行退火(PT)。與訓練良好的神經採樣器不同，PT僅產生相關樣本，每次需要新樣本時都需要重新運行，計算成本高昂。為了解決這些弱點，我們提出了基於擴散的漸進退火採樣器(PTSD)，它在不同溫度下依序訓練擴散模型，利用PT的優勢來改進神經採樣器的訓練。PTSD能有效重複利用跨溫度層級的樣本資訊，同時產生混合良好、不相關的樣本，顯著提高了目標評估效率，優於基於擴散的神經採樣器。", "applications": ["藥物研發：加速新藥分子的篩選和設計，大幅縮短藥物開發週期，降低成本。", "金融建模：更精準地模擬市場波動，幫助投資者做出更明智的決策，降低投資風險。", "材料科學：協助科學家發現具有特定性質的新材料，例如更耐高溫的合金或更高效的太陽能電池。"], "pitch": "想像一下，我們正在打造一個AI煉金術士！傳統的藥物研發、金融建模、材料設計，就像中世紀的煉金術，耗時費力，成功率極低。我們的PTSD技術，就像給了煉金術士一個超級電腦，能快速模擬各種可能性，找到最佳配方。這意味著，新藥上市時間可以縮短數年，投資風險可以大幅降低，新材料的發現也將不再是漫長的等待。這不僅僅是一項技術突破，更是一場產業革命！我們正在打造一個價值數十億美元的平台，讓AI賦能各行各業，加速創新，引領未來。投資我們，就是投資未來！", "audio": "audios\\2506.05231v1.mp3", "timestamp": "2025-06-06T08:47:31.952478"}
{"id": "2506.05344v1", "query": "Large Language Model", "url": "http://arxiv.org/abs/2506.05344v1", "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs", "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.", "authors": ["Jiahui Wang", "Zuyan Liu", "Yongming Rao", "Jiwen Lu"], "published_date": "2025-06-05", "title_zh": "SparseMM：多模態大型語言模型中視覺概念反應所產生的頭部稀疏性", "summary_zh": "本研究揭示多模態大型語言模型(MLLM)中，僅有極少數的注意力頭部(約5%)負責視覺理解，稱之為視覺頭部。我們設計了一種免訓練框架，透過分析目標反應來量化頭部層級的視覺相關性，有效識別這些視覺頭部。基於此，我們提出SparseMM，一種KV-Cache優化策略，根據視覺分數為LLM中的頭部分配非對稱的計算預算，利用視覺頭部的稀疏性加速MLLM的推論。SparseMM在解碼過程中優先考慮壓力並保留視覺語義，在主流多模態基準測試中實現了卓越的準確性-效率權衡，並在效率測試中保持性能均等的同時，實現了1.38倍的實時加速和52%的內存減少。", "applications": ["**智慧家庭監控：** 想像一下，你的智慧攝影機能更快速、更精準地辨識畫面中的異常狀況，例如小偷闖入或家人跌倒。這項技術讓攝影機只專注於重要的視覺資訊，減少誤判，並即時發出警報。", "**自動駕駛輔助：** 開車時，汽車需要迅速判斷路況。這項技術可以幫助自動駕駛系統更快地分析影像，例如辨識行人、交通號誌和障礙物，提升行車安全。", "**醫療影像診斷：** 醫生可以利用這項技術，讓AI系統更有效率地分析X光片、CT掃描等影像，加速疾病診斷，找出潛在病灶，及早治療。"], "pitch": "各位投資人，我們帶來了一項革命性的技術——SparseMM，它將徹底改變多模態AI的效率和應用。目前，多模態大型語言模型雖然強大，但運算成本極高，限制了其廣泛應用。SparseMM透過精準定位並優化模型中負責視覺理解的關鍵部分，大幅降低了運算需求，同時保持甚至提升了模型性能。這意味著，我們可以將複雜的視覺AI應用部署到資源有限的設備上，例如手機、無人機和物聯網裝置。想像一下，未來每個家庭監控攝影機都能具備更強大的AI能力，自動駕駛系統能更安全可靠，醫療診斷能更快速精準。SparseMM不僅僅是一項技術，更是一個開啟AI應用新時代的鑰匙。我們相信，透過SparseMM，我們能解鎖數十億美元的市場，並在多模態AI領域佔據領先地位。現在加入我們，一起塑造AI的未來！", "audio": "audios\\2506.05344v1.mp3", "timestamp": "2025-06-06T08:48:11.368841"}
{"id": "2506.05346v1", "query": "Large Language Model", "url": "http://arxiv.org/abs/2506.05346v1", "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets", "summary": "Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers.", "authors": ["Lei Hsiung", "Tianyu Pang", "Yung-Chen Tang", "Linyue Song", "Tsung-Yi Ho", "Pin-Yu Chen", "Yaoqing Yang"], "published_date": "2025-06-05", "title_zh": "為何大型語言模型（LLM）的安全防護在微調後崩潰：對齊與微調數據集之間的相似性分析", "summary_zh": "大型語言模型雖然強大，但微調後容易出現安全漏洞。本研究發現，關鍵在於用於安全對齊的原始數據集。當對齊數據集與微調任務的數據集非常相似時，模型的安全防護會顯著減弱，更容易受到攻擊。相反，如果兩者差異大，模型安全性會更好，有害程度最多可降低10.33%。因此，設計安全防護時，數據集的選擇至關重要。這項研究為微調服務提供商提供了實用建議，能有效提升模型安全性，減少現實世界中遭受攻擊的風險。", "applications": ["**兒童教育APP：** 確保AI導師不會教導不適當或危險的知識。透過控制訓練數據的相似度，防止APP產生對兒童有害的內容。", "**金融交易系統：** 防止AI交易系統被惡意利用，進行非法或高風險操作。確保AI不會根據相似的詐欺案例數據，學習到詐欺行為模式。", "**醫療診斷輔助：** 避免AI診斷系統在遇到罕見病例時，因為與常見病例數據過於相似，而產生錯誤判斷，造成醫療事故。"], "pitch": "各位創投先進，我們發現了大型語言模型安全領域的重大漏洞，並提出了革命性的解決方案！現今LLM廣泛應用，但微調後的安全性卻難以保證，這如同在高樓大廈中埋藏了不定時炸彈。我們的技術能精準分析並控制訓練數據的相似性，從源頭加強安全防護，讓LLM真正『安全可控』。想像一下，未來AI醫療診斷系統不會誤診，AI金融交易系統不會被詐欺利用，AI教育APP能百分百保護兒童。這不僅僅是技術升級，更是對整個AI產業的信任升級！我們將建立一套AI安全評估與優化平台，為各行各業提供客製化的安全解決方案，打造AI安全的新標準。預期在AI安全需求爆發式增長的未來，我們的技術將成為市場剛需，潛在商業價值不可估量！現在加入我們，共同引領AI安全革命，搶佔未來市場先機！", "audio": "audios\\2506.05346v1.mp3", "timestamp": "2025-06-06T08:47:57.037234"}
{"id": "2506.05349v1", "query": "Computer Vision", "url": "http://arxiv.org/abs/2506.05349v1", "title": "VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos", "summary": "Mathematical reasoning in real-world video settings presents a fundamentally\ndifferent challenge than in static images or text. It requires interpreting\nfine-grained visual information, accurately reading handwritten or digital\ntext, and integrating spoken cues, often dispersed non-linearly over time. In\nsuch multimodal contexts, success hinges not just on perception, but on\nselectively identifying and integrating the right contextual details from a\nrich and noisy stream of content. To this end, we introduce VideoMathQA, a\nbenchmark designed to evaluate whether models can perform such temporally\nextended cross-modal reasoning on videos. The benchmark spans 10 diverse\nmathematical domains, covering videos ranging from 10 seconds to over 1 hour.\nIt requires models to interpret structured visual content, understand\ninstructional narratives, and jointly ground concepts across visual, audio, and\ntextual modalities. We employ graduate-level experts to ensure high quality,\ntotaling over $920$ man-hours of annotation. To reflect real-world scenarios,\nquestions are designed around three core reasoning challenges: direct problem\nsolving, where answers are grounded in the presented question; conceptual\ntransfer, which requires applying learned methods to new problems; and deep\ninstructional comprehension, involving multi-step reasoning over extended\nexplanations and partially worked-out solutions. Each question includes\nmulti-step reasoning annotations, enabling fine-grained diagnosis of model\ncapabilities. Through this benchmark, we highlight the limitations of existing\napproaches and establish a systematic evaluation framework for models that must\nreason, rather than merely perceive, across temporally extended and\nmodality-rich mathematical problem settings. Our benchmark and evaluation code\nare available at: https://mbzuai-oryx.github.io/VideoMathQA", "authors": ["Hanoona Rasheed", "Abdelrahman Shaker", "Anqi Tang", "Muhammad Maaz", "Ming-Hsuan Yang", "Salman Khan", "Fahad Khan"], "published_date": "2025-06-05", "title_zh": "VideoMathQA：透過影片中的多模態理解來評估數學推理能力", "summary_zh": "VideoMathQA是一個評估模型在影片中進行跨模態數學推理能力的基準。它涵蓋10個不同的數學領域，影片長度從10秒到1小時不等。模型需要解讀視覺內容、理解教學敘事，並整合視覺、聽覺和文字模態的概念。問題設計圍繞三個核心推理挑戰：直接問題解決、概念轉移和深度教學理解。此基準測試旨在突顯現有方法的局限性，並為必須在時間跨度和模態豐富的數學問題設定中進行推理的模型建立系統的評估框架。簡單來說，VideoMathQA就像一個數學影片理解考試，測試AI是否能像人類一樣，從影片中學習和解決數學問題。", "applications": ["想像一下，你正在學習微積分，但課本太難懂。透過VideoMathQA技術，AI可以分析教學影片，自動標記重點、整理筆記，甚至在你卡住時提供客製化的解說，讓學習變得更輕鬆。", "廚師在網路上分享獨家食譜，但步驟複雜難懂。VideoMathQA可以分析食譜影片，將步驟拆解成易於理解的視覺化流程圖，並根據你的廚藝程度提供調整建議，讓你也能輕鬆做出大師級料理。", "工廠技師正在維修複雜的機器，但說明書遺失了。透過VideoMathQA，AI可以分析維修影片，即時提供零件名稱、組裝步驟和潛在問題的警示，協助技師快速排除故障。"], "pitch": "各位創投先進，我們正站在AI教育革命的風口浪尖！VideoMathQA不僅僅是一個基準測試，它更是通往未來智慧教育和工業應用的鑰匙。想像一下，一個能夠真正理解影片內容並進行數學推理的AI，它將顛覆傳統的學習模式，讓知識獲取變得更高效、更個性化。在教育領域，我們可以打造出千人千面的AI家教，根據學生的學習風格和進度，提供客製化的輔導。在工業領域，我們可以利用VideoMathQA開發出智慧維修系統，降低生產成本，提高效率。這項技術的潛力是無限的，從線上教育平台到工業自動化，市場規模將達到數十億美元。現在投資VideoMathQA，就是投資未來！我們有信心在三年內成為AI教育領域的獨角獸，為投資者帶來豐厚的回報！", "audio": "audios\\2506.05349v1.mp3", "timestamp": "2025-06-06T08:48:09.845177"}
{"id": "2506.05348v1", "query": "Computer Vision", "url": "http://arxiv.org/abs/2506.05348v1", "title": "FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene Reconstruction", "summary": "This paper addresses the challenge of reconstructing dynamic 3D scenes with\ncomplex motions. Some recent works define 3D Gaussian primitives in the\ncanonical space and use deformation fields to map canonical primitives to\nobservation spaces, achieving real-time dynamic view synthesis. However, these\nmethods often struggle to handle scenes with complex motions due to the\ndifficulty of optimizing deformation fields. To overcome this problem, we\npropose FreeTimeGS, a novel 4D representation that allows Gaussian primitives\nto appear at arbitrary time and locations. In contrast to canonical Gaussian\nprimitives, our representation possesses the strong flexibility, thus improving\nthe ability to model dynamic 3D scenes. In addition, we endow each Gaussian\nprimitive with an motion function, allowing it to move to neighboring regions\nover time, which reduces the temporal redundancy. Experiments results on\nseveral datasets show that the rendering quality of our method outperforms\nrecent methods by a large margin.", "authors": ["Yifan Wang", "Peishan Yang", "Zhen Xu", "Jiaming Sun", "Zhanhua Zhang", "Yong Chen", "Hujun Bao", "Sida Peng", "Xiaowei Zhou"], "published_date": "2025-06-05", "title_zh": "FreeTimeGS：隨時隨地自由高斯函數，用於動態場景重建", "summary_zh": "本研究提出FreeTimeGS，一種創新的4D表示法，讓高斯基元能自由地在任何時間和地點出現，解決重建複雜動態3D場景的挑戰。不同於傳統方法，FreeTimeGS具有極高的靈活性，更擅長模擬動態3D場景。我們還賦予每個高斯基元運動函數，使其能隨時間移動到鄰近區域，減少時間上的冗餘。實驗結果顯示，我們的渲染品質大幅超越現有技術，為動態場景重建帶來突破。", "applications": ["虛擬實境(VR)遊戲：玩家在VR世界中與動態物件互動，例如揮舞球拍擊打飛來的網球，FreeTimeGS能更真實地呈現網球的飛行軌跡與受力變形。", "自動駕駛：精準重建道路上車輛、行人等動態物體的3D模型，提升自動駕駛系統對環境的感知能力，預測其行為，從而做出更安全的決策。", "電影特效：製作更逼真的電影特效，例如爆炸、火焰等動態場景，讓特效看起來更自然，減少穿模或不真實感。"], "pitch": "想像一下，我們正在打造一個能完美捕捉真實世界動態的數位雙胞胎技術！FreeTimeGS是動態3D重建領域的革命性突破，它能以前所未有的精度和效率，重現複雜的動態場景。這意味著什麼？更逼真的VR/AR體驗、更安全的自動駕駛，以及更震撼的電影特效。但這僅僅是開始！未來，我們能將這項技術應用於遠程手術，讓醫生能精準操作機械手臂，即時應對複雜的生理變化；也能應用於智慧城市，即時監控交通流量、人群密度，優化城市管理。FreeTimeGS不僅僅是一項技術，它是一個平台，一個通往無限可能的入口。我們正在尋找有遠見的投資者，一起開創這個動態3D世界的未來！", "audio": "audios\\2506.05348v1.mp3", "timestamp": "2025-06-06T08:47:32.101732"}
